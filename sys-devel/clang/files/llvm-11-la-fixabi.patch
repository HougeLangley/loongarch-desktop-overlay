diff --git a/clang/include/clang/Basic/DiagnosticDriverKinds.td b/clang/include/clang/Basic/DiagnosticDriverKinds.td
index 558639ecad6aa..e5ec07f0d9331 100644
--- a/clang/include/clang/Basic/DiagnosticDriverKinds.td
+++ b/clang/include/clang/Basic/DiagnosticDriverKinds.td
@@ -148,6 +148,8 @@ def err_drv_force_crash : Error<
   "failing because %select{environment variable 'FORCE_CLANG_DIAGNOSTICS_CRASH' is set|'-gen-reproducer' is used}0">;
 def err_drv_invalid_mfloat_abi : Error<
   "invalid float ABI '%0'">;
+def err_drv_invalid_loongarch_mfpu : Error<
+  "invalid loongarch FPU value '%0'. Please specify FPU = 64,32 or none">;
 def err_drv_invalid_mtp : Error<
   "invalid thread pointer reading mode '%0'">;
 def err_drv_missing_arg_mtp : Error<
diff --git a/clang/lib/Basic/Targets/LoongArch.cpp b/clang/lib/Basic/Targets/LoongArch.cpp
index 10b05bf982821..28a82e065044e 100644
--- a/clang/lib/Basic/Targets/LoongArch.cpp
+++ b/clang/lib/Basic/Targets/LoongArch.cpp
@@ -52,22 +52,9 @@ void LoongArchTargetInfo::getTargetDefines(const LangOptions &Opts,
                                       MacroBuilder &Builder) const {
   Builder.defineMacro("__loongarch__");
 
-  if (ABI == "lp32") {
-    Builder.defineMacro("__loongarch32");
-  } else {
-    Builder.defineMacro("__loongarch64");
-  }
-
-  //TODO: support others ABIs
-  if (ABI == "lp64d") {
+  if (ABI == "lp64d" || ABI == "lp64s" || ABI == "lp64f") {
     Builder.defineMacro("__loongarch_lp64");
-  }
-
-  if (ABI == "lp32") {
-    Builder.defineMacro("_ABILP32", "1");
-  } else if (ABI == "lpx32") {
-    Builder.defineMacro("_ABILPX32", "2");
-  } else if (ABI == "lp64d") {
+    Builder.defineMacro("__loongarch64");
     Builder.defineMacro("_ABILP64", "3");
     Builder.defineMacro("_LOONGARCH_SIM", "_ABILP64");
   } else
@@ -75,49 +62,56 @@ void LoongArchTargetInfo::getTargetDefines(const LangOptions &Opts,
 
   Builder.defineMacro("__REGISTER_PREFIX__", "");
 
-  switch (FloatABI) {
-  case HardFloat:
-    Builder.defineMacro("__loongarch_hard_float", Twine(1));
-    break;
-  case SoftFloat:
-    Builder.defineMacro("__loongarch_soft_float", Twine(1));
-    break;
-  }
-
-  if (IsSingleFloat)
-    Builder.defineMacro("__loongarch_single_float", Twine(1));
-
-  switch (FPMode) {
-  case FP32:
-    Builder.defineMacro("__loongarch_fpr", Twine(32));
-    break;
-  case FP64:
-    Builder.defineMacro("__loongarch_fpr", Twine(64));
-    break;
-  }
-
   Builder.defineMacro("_LOONGARCH_SZPTR", Twine(getPointerWidth(0)));
   Builder.defineMacro("_LOONGARCH_SZINT", Twine(getIntWidth()));
   Builder.defineMacro("_LOONGARCH_SZLONG", Twine(getLongWidth()));
 
-  Builder.defineMacro("_LOONGARCH_ARCH", "\"" + CPU + "\"");
-  Builder.defineMacro("_LOONGARCH_ARCH_" + StringRef(CPU).upper());
+  Builder.defineMacro("_LOONGARCH_TUNE", "\"" + CPU + "\"");
+  Builder.defineMacro("_LOONGARCH_TUNE_" + StringRef(CPU).upper());
+
+  Builder.defineMacro("_LOONGARCH_ARCH", "\"" + getTriple().getArchName() + "\"");
+  Builder.defineMacro("_LOONGARCH_ARCH_" + StringRef(getTriple().getArchName()).upper());
 
   Builder.defineMacro("__GCC_HAVE_SYNC_COMPARE_AND_SWAP_1");
   Builder.defineMacro("__GCC_HAVE_SYNC_COMPARE_AND_SWAP_2");
   Builder.defineMacro("__GCC_HAVE_SYNC_COMPARE_AND_SWAP_4");
 
-  // 32-bit loongarch processors don't have the necessary lld/scd instructions
-  // found in 64-bit processors. In the case of lp32 on a 64-bit processor,
-  // the instructions exist but using them violates the ABI since they
-  // require 64-bit GPRs and LP32 only supports 32-bit GPRs.
-  if (ABI == "lpx32" || ABI == "lp64d")
+  // 32-bit loongarch processors don't have the necessary ll.d/sc.d instructions
+  // found in 64-bit processors.
+  if (ABI == "lp64d" || ABI == "lp64s" || ABI == "lp64f")
     Builder.defineMacro("__GCC_HAVE_SYNC_COMPARE_AND_SWAP_8");
+
+  // Bit-width of general purpose registers.
+  Builder.defineMacro("__loongarch_grlen", Twine(getRegisterWidth()));
+
+  // Bit-width of floating-point registers. The possible values for
+  // this macro are 0, 32 and 64. 0 if there is no FPU.
+  if (HasBasicD || HasBasicF)
+    Builder.defineMacro("__loongarch_frlen", HasBasicD ? "64" : "32");
+  else
+    Builder.defineMacro("__loongarch_frlen", "0");
+
+  // FIXME: Defined if floating-point/extended ABI type is single or double.
+  if (ABI == "lp64d" || ABI == "lp64f")
+    Builder.defineMacro("__loongarch_hard_float");
+
+  // FIXME: Defined if floating-point/extended ABI type is double.
+  if (ABI == "lp64d")
+    Builder.defineMacro("__loongarch_double_float");
+
+  // FIXME: Defined if floating-point/extended ABI type is single.
+  if (ABI == "lp64f")
+    Builder.defineMacro("__loongarch_single_float");
+
+  // FIXME: Defined if floating-point/extended ABI type is soft.
+  if (ABI == "lp64s")
+    Builder.defineMacro("__loongarch_soft_float");
 }
 
 bool LoongArchTargetInfo::hasFeature(StringRef Feature) const {
   return llvm::StringSwitch<bool>(Feature)
-      .Case("fp64", FPMode == FP64)
+      .Case("d", HasBasicD)
+      .Case("f", HasBasicF)
       .Default(false);
 }
 
@@ -127,48 +121,22 @@ ArrayRef<Builtin::Info> LoongArchTargetInfo::getTargetBuiltins() const {
 }
 
 bool LoongArchTargetInfo::validateTarget(DiagnosticsEngine &Diags) const {
-  // FIXME: It's valid to use LP32 on a 64-bit CPU but the backend can't handle
-  //        this yet. It's better to fail here than on the backend assertion.
-  if (processorSupportsGPR64() && ABI == "lp32") {
-    Diags.Report(diag::err_target_unsupported_abi) << ABI << CPU;
-    return false;
-  }
-
   // 64-bit ABI's require 64-bit CPU's.
-  if (!processorSupportsGPR64() && (ABI == "lpx32" || ABI == "lp64d")) {
+  if (!processorSupportsGPR64() &&
+      (ABI == "lp64d" || ABI == "lp64s" || ABI == "lp64f")) {
     Diags.Report(diag::err_target_unsupported_abi) << ABI << CPU;
     return false;
   }
 
-  // FIXME: It's valid to use lp32 on a loongarch64 triple but the backend
-  //        can't handle this yet. It's better to fail here than on the
-  //        backend assertion.
-  if (getTriple().isLoongArch64() && ABI == "lp32") {
-    Diags.Report(diag::err_target_unsupported_abi_for_triple)
-        << ABI << getTriple().str();
-    return false;
-  }
-
-  // FIXME: It's valid to use lpx32/lp64d on a loongarch32 triple but the backend
-  //        can't handle this yet. It's better to fail here than on the
-  //        backend assertion.
-  if (getTriple().isLoongArch32() && (ABI == "lpx32" || ABI == "lp64d")) {
+  // FIXME: It's valid to use lp64d/lp64s/lp64f on a loongarch32 triple
+  // but the backend can't handle this yet. It's better to fail here than on the
+  // backend assertion.
+  if (getTriple().isLoongArch32() &&
+      (ABI == "lp64d" || ABI == "lp64s" || ABI == "lp64f")) {
     Diags.Report(diag::err_target_unsupported_abi_for_triple)
         << ABI << getTriple().str();
     return false;
   }
 
-  // -mfp32 and lpx32/lp64d ABIs are incompatible
-  if (FPMode != FP64 && !IsSingleFloat &&
-      (ABI == "lpx32"  || ABI == "lp64d")) {
-    Diags.Report(diag::err_opt_not_valid_with_opt) << "-mfp32" << ABI;
-    return false;
-  }
-
-  if (FPMode != FP64 && (CPU == "la464")) {
-    Diags.Report(diag::err_opt_not_valid_with_opt) << "-mfp32" << CPU;
-    return false;
-  }
-
   return true;
 }
diff --git a/clang/lib/Basic/Targets/LoongArch.h b/clang/lib/Basic/Targets/LoongArch.h
index cba15694fb1e5..b959840b1c1ca 100644
--- a/clang/lib/Basic/Targets/LoongArch.h
+++ b/clang/lib/Basic/Targets/LoongArch.h
@@ -25,11 +25,10 @@ class LLVM_LIBRARY_VISIBILITY LoongArchTargetInfo : public TargetInfo {
   void setDataLayout() {
     StringRef Layout;
 
-    if (ABI == "lp32")
-      Layout = "m:m-p:32:32-i8:8:32-i16:16:32-i64:64-n32-S64";
-    else if (ABI == "lpx32")
-      Layout = "m:e-p:32:32-i8:8:32-i16:16:32-i64:64-n32:64-S128";
-    else if (ABI == "lp64d")
+    if (ABI == "ilp32d" || ABI == "ilp32f" || ABI == "ilp32s")
+      // TODO
+      llvm_unreachable("Unimplemented ABI");
+    else if (ABI == "lp64d" || ABI == "lp64s" || ABI == "lp64f")
       Layout = "m:e-i8:8:32-i16:16:32-i64:64-n32:64-S128";
     else
       llvm_unreachable("Invalid ABI");
@@ -39,23 +38,20 @@ class LLVM_LIBRARY_VISIBILITY LoongArchTargetInfo : public TargetInfo {
 
   static const Builtin::Info BuiltinInfo[];
   std::string CPU;
-  bool IsSingleFloat;
-  enum LoongArchFloatABI { HardFloat, SoftFloat } FloatABI;
+  bool HasBasicF;
+  bool HasBasicD;
 
 protected:
-  enum FPModeEnum { FP32, FP64 } FPMode;
   std::string ABI;
 
 public:
   LoongArchTargetInfo(const llvm::Triple &Triple, const TargetOptions &)
-      : TargetInfo(Triple),
-        IsSingleFloat(false),
-        FloatABI(HardFloat),
-        FPMode(FP64) {
+      : TargetInfo(Triple), HasBasicF(false), HasBasicD(false) {
     TheCXXABI.set(TargetCXXABI::GenericLoongArch);
 
     if (Triple.isLoongArch32())
-      setABI("lp32");
+      // TODO
+      llvm_unreachable("Unimplemented triple");
     else
       setABI("lp64d");
 
@@ -69,20 +65,12 @@ class LLVM_LIBRARY_VISIBILITY LoongArchTargetInfo : public TargetInfo {
   StringRef getABI() const override { return ABI; }
 
   bool setABI(const std::string &Name) override {
-    if (Name == "lp32") {
-      setLP32ABITypes();
-      ABI = Name;
-      return true;
+    if (Name == "ilp32d" || Name == "ilp32f" || Name == "ilp32s") {
+      // TODO
+      llvm_unreachable("Unimplemented ABI");
     }
 
-    if (Name == "lpx32") {
-      //setLPX32ABITypes();
-      //ABI = Name;
-      //return true;
-      //TODO: implement
-      return false;
-    }
-    if (Name == "lp64d") {
+    if (Name == "lp64d" || Name == "lp64s" || Name == "lp64f") {
       setLP64ABITypes();
       ABI = Name;
       return true;
@@ -90,37 +78,12 @@ class LLVM_LIBRARY_VISIBILITY LoongArchTargetInfo : public TargetInfo {
     return false;
   }
 
-  void setLP32ABITypes() {
-    Int64Type = SignedLongLong;
-    IntMaxType = Int64Type;
-    LongDoubleFormat = &llvm::APFloat::IEEEdouble();
-    LongDoubleWidth = LongDoubleAlign = 64;
-    LongWidth = LongAlign = 32;
-    MaxAtomicPromoteWidth = MaxAtomicInlineWidth = 32;
-    PointerWidth = PointerAlign = 32;
-    PtrDiffType = SignedInt;
-    SizeType = UnsignedInt;
-    SuitableAlign = 64;
-  }
-
-  void setLPX32LP64ABITypes() {
+  void setLP64ABITypes() {
     LongDoubleWidth = LongDoubleAlign = 128;
     LongDoubleFormat = &llvm::APFloat::IEEEquad();
-    if (getTriple().isOSFreeBSD()) {
-      LongDoubleWidth = LongDoubleAlign = 64;
-      LongDoubleFormat = &llvm::APFloat::IEEEdouble();
-    }
     MaxAtomicPromoteWidth = MaxAtomicInlineWidth = 64;
     SuitableAlign = 128;
-  }
-
-  void setLP64ABITypes() {
-    setLPX32LP64ABITypes();
-    if (getTriple().isOSOpenBSD()) {
-      Int64Type = SignedLongLong;
-    } else {
-      Int64Type = SignedLong;
-    }
+    Int64Type = SignedLong;
     IntMaxType = Int64Type;
     LongWidth = LongAlign = 64;
     PointerWidth = PointerAlign = 64;
@@ -128,16 +91,6 @@ class LLVM_LIBRARY_VISIBILITY LoongArchTargetInfo : public TargetInfo {
     SizeType = UnsignedLong;
   }
 
-  void setLPX32ABITypes() {
-    setLPX32LP64ABITypes();
-    Int64Type = SignedLongLong;
-    IntMaxType = Int64Type;
-    LongWidth = LongAlign = 32;
-    PointerWidth = PointerAlign = 32;
-    PtrDiffType = SignedInt;
-    SizeType = UnsignedInt;
-  }
-
   bool isValidCPUName(StringRef Name) const override;
   void fillValidCPUList(SmallVectorImpl<StringRef> &Values) const override;
 
@@ -270,19 +223,14 @@ class LLVM_LIBRARY_VISIBILITY LoongArchTargetInfo : public TargetInfo {
 
   bool handleTargetFeatures(std::vector<std::string> &Features,
                             DiagnosticsEngine &Diags) override {
-    IsSingleFloat = false;
-    FloatABI = HardFloat;
-    FPMode = FP64;
+    HasBasicF = false;
+    HasBasicD = false;
 
     for (const auto &Feature : Features) {
-      if (Feature == "+single-float")
-        IsSingleFloat = true;
-      else if (Feature == "+soft-float")
-        FloatABI = SoftFloat;
-      else if (Feature == "+fp64")
-        FPMode = FP64;
-      else if (Feature == "-fp64")
-        FPMode = FP32;
+      if (Feature == "+f")
+        HasBasicF = true;
+      else if (Feature == "+d")
+        HasBasicD = true;
     }
 
     setDataLayout();
@@ -371,7 +319,8 @@ class LLVM_LIBRARY_VISIBILITY LoongArchTargetInfo : public TargetInfo {
   }
 
   bool hasInt128Type() const override {
-    return (ABI == "lpx32" || ABI == "lp64d") || getTargetOpts().ForceEnableInt128;
+    return (ABI == "lp64d" || ABI == "lp64s" || ABI == "lp64f") ||
+           getTargetOpts().ForceEnableInt128;
   }
 
   bool validateTarget(DiagnosticsEngine &Diags) const override;
diff --git a/clang/lib/CodeGen/TargetInfo.cpp b/clang/lib/CodeGen/TargetInfo.cpp
index 3ae2366ba5d82..19d7df5998394 100644
--- a/clang/lib/CodeGen/TargetInfo.cpp
+++ b/clang/lib/CodeGen/TargetInfo.cpp
@@ -7959,6 +7959,8 @@ ABIArgInfo LoongArchABIInfo::classifyArgumentType(QualType Ty, bool IsFixed,
     bool IsCandidate =
         detectFPCCEligibleStruct(Ty, Field1Ty, Field1Off, Field2Ty, Field2Off,
                                  NeededArgGPRs, NeededArgFPRs);
+    if (Ty->isStructureOrClassType() && isAggregateTypeForABI(Ty))
+      IsCandidate = false;
     if (IsCandidate && NeededArgGPRs <= ArgGPRsLeft &&
         NeededArgFPRs <= ArgFPRsLeft) {
       ArgGPRsLeft -= NeededArgGPRs;
@@ -7966,7 +7968,8 @@ ABIArgInfo LoongArchABIInfo::classifyArgumentType(QualType Ty, bool IsFixed,
       return coerceAndExpandFPCCEligibleStruct(Field1Ty, Field1Off, Field2Ty,
                                                Field2Off);
     }
-  } else if (Ty->isStructureOrClassType() && Size == 128 && isAggregateTypeForABI(Ty)) {
+  } else if (Ty->isStructureOrClassType() && Size == 128 &&
+             isAggregateTypeForABI(Ty)) {
     uint64_t Offset = 8;
     uint64_t OrigOffset = Offset;
     uint64_t TySize = getContext().getTypeSize(Ty);
diff --git a/clang/lib/Driver/Driver.cpp b/clang/lib/Driver/Driver.cpp
index eb86256a6aa2b..0b74b7529de1d 100644
--- a/clang/lib/Driver/Driver.cpp
+++ b/clang/lib/Driver/Driver.cpp
@@ -547,10 +547,9 @@ static llvm::Triple computeTargetTriple(const Driver &D,
   A = Args.getLastArg(options::OPT_mabi_EQ);
   if (A && Target.isLoongArch()) {
     StringRef ABIName = A->getValue();
-    if (ABIName == "lp32") {
-      Target = Target.get32BitArchVariant();
-      if (Target.getEnvironment() == llvm::Triple::GNUABI64)
-        Target.setEnvironment(llvm::Triple::GNU);
+    if (ABIName == "ilp32d" || ABIName == "ilp32f" || ABIName == "ilp32s") {
+      // TODO
+      llvm_unreachable("Unimplemented ABI");
     } else if (ABIName == "lp64d") {
       Target = Target.get64BitArchVariant();
       if (Target.getEnvironment() == llvm::Triple::GNU)
diff --git a/clang/lib/Driver/ToolChains/Arch/LoongArch.cpp b/clang/lib/Driver/ToolChains/Arch/LoongArch.cpp
index 8105b6fac740c..e0823f2ce76aa 100644
--- a/clang/lib/Driver/ToolChains/Arch/LoongArch.cpp
+++ b/clang/lib/Driver/ToolChains/Arch/LoongArch.cpp
@@ -30,15 +30,8 @@ void loongarch::getLoongArchCPUAndABI(const ArgList &Args, const llvm::Triple &T
                                options::OPT_mcpu_EQ))
     CPUName = A->getValue();
 
-  if (Arg *A = Args.getLastArg(options::OPT_mabi_EQ)) {
+  if (Arg *A = Args.getLastArg(options::OPT_mabi_EQ))
     ABIName = A->getValue();
-    // Convert a GNU style LoongArch ABI name to the name
-    // accepted by LLVM LoongArch backend.
-    ABIName = llvm::StringSwitch<llvm::StringRef>(ABIName)
-                  .Case("32", "lp32")
-                  .Case("64", "lp64d")
-                  .Default(ABIName);
-  }
 
   // Setup default CPU and ABI names.
   if (CPUName.empty() && ABIName.empty()) {
@@ -56,24 +49,29 @@ void loongarch::getLoongArchCPUAndABI(const ArgList &Args, const llvm::Triple &T
 
   if (ABIName.empty()) {
     ABIName = llvm::StringSwitch<const char *>(CPUName)
-                  .Case("loongarch32", "lp32")
+                  .Case("loongarch32", "ilp32d")
                   .Case("la464", "lp64d")
-                  .Default("");
-  }
-
-  if (ABIName.empty()) {
-    // Deduce ABI name from the target triple.
-    ABIName = Triple.isLoongArch32() ? "lp32" : "lp64d";
+                  .Default(Triple.isLoongArch32() ? "ilp32d" : "lp64d");
   }
 
   if (CPUName.empty()) {
     // Deduce CPU name from ABI name.
     CPUName = llvm::StringSwitch<const char *>(ABIName)
-                  .Case("lp32", DefLoongArch32CPU)
-                  .Cases("lpx32", "lp64d", DefLoongArch64CPU)
+                  .Cases("lp64d", "lp64f", "lp64s", DefLoongArch64CPU)
                   .Default("");
   }
 
+  if (Arg *A = Args.getLastArg(options::OPT_msingle_float,
+                               options::OPT_mdouble_float,
+                               options::OPT_msoft_float)) {
+    if (A->getOption().matches(options::OPT_msingle_float))
+      ABIName = "lp64f";
+    else if (A->getOption().matches(options::OPT_mdouble_float))
+      ABIName = "lp64d";
+    else
+      ABIName = "lp64s";
+  }
+
   // FIXME: Warn on inconsistent use of -march and -mabi.
 }
 
@@ -82,52 +80,8 @@ std::string loongarch::getLoongArchABILibSuffix(const ArgList &Args,
   StringRef CPUName, ABIName;
   tools::loongarch::getLoongArchCPUAndABI(Args, Triple, CPUName, ABIName);
   return llvm::StringSwitch<std::string>(ABIName)
-      .Case("lp32", "")
-      .Case("lpx32", "32")
-      .Case("lp64d", "64");
-}
-
-// Convert ABI name to the GNU tools acceptable variant.
-StringRef loongarch::getGnuCompatibleLoongArchABIName(StringRef ABI) {
-  return llvm::StringSwitch<llvm::StringRef>(ABI)
-      .Case("lp32", "32")
-      .Case("lp64d", "64")
-      .Default(ABI);
-}
-
-// Select the LoongArch float ABI as determined by -msoft-float, -mhard-float,
-// and -mfloat-abi=.
-loongarch::FloatABI loongarch::getLoongArchFloatABI(const Driver &D, const ArgList &Args) {
-  loongarch::FloatABI ABI = loongarch::FloatABI::Invalid;
-  if (Arg *A =
-          Args.getLastArg(options::OPT_msoft_float, options::OPT_mhard_float,
-                          options::OPT_mfloat_abi_EQ)) {
-    if (A->getOption().matches(options::OPT_msoft_float))
-      ABI = loongarch::FloatABI::Soft;
-    else if (A->getOption().matches(options::OPT_mhard_float))
-      ABI = loongarch::FloatABI::Hard;
-    else {
-      ABI = llvm::StringSwitch<loongarch::FloatABI>(A->getValue())
-                .Case("soft", loongarch::FloatABI::Soft)
-                .Case("hard", loongarch::FloatABI::Hard)
-                .Default(loongarch::FloatABI::Invalid);
-      if (ABI == loongarch::FloatABI::Invalid && !StringRef(A->getValue()).empty()) {
-        D.Diag(clang::diag::err_drv_invalid_mfloat_abi) << A->getAsString(Args);
-        ABI = loongarch::FloatABI::Hard;
-      }
-    }
-  }
-
-  // If unspecified, choose the default based on the platform.
-  if (ABI == loongarch::FloatABI::Invalid) {
-    // Assume "hard", because it's a default value used by gcc.
-    // When we start to recognize specific target LoongArch processors,
-    // we will be able to select the default more correctly.
-    ABI = loongarch::FloatABI::Hard;
-  }
-
-  assert(ABI != loongarch::FloatABI::Invalid && "must select an ABI");
-  return ABI;
+      .Cases("ilp32d", "ilp32f", "ilp32s", "32")
+      .Cases("lp64d", "lp64f", "lp64s", "64");
 }
 
 void loongarch::getLoongArchTargetFeatures(const Driver &D, const llvm::Triple &Triple,
@@ -135,14 +89,9 @@ void loongarch::getLoongArchTargetFeatures(const Driver &D, const llvm::Triple &
                                  std::vector<StringRef> &Features) {
   StringRef CPUName;
   StringRef ABIName;
+  StringRef FPUValue;
   getLoongArchCPUAndABI(Args, Triple, CPUName, ABIName);
-  ABIName = getGnuCompatibleLoongArchABIName(ABIName);
-
-  // At final link time, LP32 and LPX32 with CPIC will have another section
-  // added to the binary which contains the stub functions to perform
-  // any fixups required for PIC code.
 
-  bool IsLP64D = ABIName == "64";
   bool NonPIC = false;
 
   Arg *LastPICArg = Args.getLastArg(options::OPT_fPIC, options::OPT_fno_PIC,
@@ -156,33 +105,52 @@ void loongarch::getLoongArchTargetFeatures(const Driver &D, const llvm::Triple &
          O.matches(options::OPT_fno_PIE) || O.matches(options::OPT_fno_pie));
   }
 
-  if (IsLP64D && NonPIC) {
+  if (NonPIC) {
     NonPIC = false;
   }
 
-  loongarch::FloatABI FloatABI = loongarch::getLoongArchFloatABI(D, Args);
-  if (FloatABI == loongarch::FloatABI::Soft) {
-    // FIXME: Note, this is a hack. We need to pass the selected float
-    // mode to the LoongArchTargetInfoBase to define appropriate macros there.
-    // Now it is the only method.
-    Features.push_back("+soft-float");
+  if (Arg *A = Args.getLastArg(options::OPT_mfpu_EQ))
+    FPUValue = A->getValue();
+
+  if (Arg *A = Args.getLastArg(options::OPT_msingle_float,
+                               options::OPT_mdouble_float,
+                               options::OPT_msoft_float)) {
+    if (A->getOption().matches(options::OPT_msingle_float))
+      FPUValue = "32";
+    else if (A->getOption().matches(options::OPT_mdouble_float))
+      FPUValue = "64";
+    else
+      FPUValue = "none";
   }
 
-  AddTargetFeature(Args, Features, options::OPT_msingle_float,
-                   options::OPT_mdouble_float, "single-float");
+  // Setup feature.
+  if (FPUValue.empty())
+    Features.push_back("+d");
+  else {
+    if (FPUValue == "64")
+      Features.push_back("+d");
+    else if (FPUValue == "32")
+      Features.push_back("+f");
+    else if (FPUValue == "none") {
+      Features.push_back("-f");
+      Features.push_back("-d");
+    } else
+      D.Diag(clang::diag::err_drv_invalid_loongarch_mfpu)
+          << FPUValue;
+  }
 
-  // Add the last -mfp32/-mfp64, if none are given and fp64 is default,
-  // pass fp64.
-  if (Arg *A = Args.getLastArg(options::OPT_mfp32,
-                               options::OPT_mfp64)) {
-    if (A->getOption().matches(options::OPT_mfp32))
-      Features.push_back("-fp64");
-    else
-      Features.push_back("+fp64");
-  } else if (loongarch::isFP64Default(Args)) {
-    Features.push_back("+fp64");
+  // lp64f ABI and -mfpu=none are incompatible.
+  if (hasLoongArchAbiArg(Args, "lp64f") && hasLoongArchFpuArg(Args, "none")) {
+    D.Diag(clang::diag::err_opt_not_valid_with_opt) << "lp64f"
+                                                    << "-mfpu=none";
   }
 
+  // Also lp64d ABI is only compatible with -mfpu=64.
+  if ((hasLoongArchAbiArg(Args, "lp64d") || ABIName == "lp64d") &&
+      (hasLoongArchFpuArg(Args, "none") || hasLoongArchFpuArg(Args, "32"))) {
+    D.Diag(clang::diag::err_opt_not_valid_without_opt) << "lp64d"
+                                                       << "-mfpu=64";
+  }
 }
 
 bool loongarch::hasLoongArchAbiArg(const ArgList &Args, const char *Value) {
@@ -195,6 +163,7 @@ bool loongarch::isUCLibc(const ArgList &Args) {
   return A && A->getOption().matches(options::OPT_muclibc);
 }
 
-bool loongarch::isFP64Default(const ArgList &Args) {
-  return Args.getLastArg(options::OPT_msingle_float) ? false : true;
+bool loongarch::hasLoongArchFpuArg(const ArgList &Args, const char *Value) {
+  Arg *A = Args.getLastArg(options::OPT_mfpu_EQ);
+  return A && (A->getValue() == StringRef(Value));
 }
diff --git a/clang/lib/Driver/ToolChains/Arch/LoongArch.h b/clang/lib/Driver/ToolChains/Arch/LoongArch.h
index 53664346f8f8a..5c581ff69c981 100644
--- a/clang/lib/Driver/ToolChains/Arch/LoongArch.h
+++ b/clang/lib/Driver/ToolChains/Arch/LoongArch.h
@@ -21,25 +21,17 @@ namespace driver {
 namespace tools {
 
 namespace loongarch {
-enum class FloatABI {
-  Invalid,
-  Soft,
-  Hard,
-};
-
 void getLoongArchCPUAndABI(const llvm::opt::ArgList &Args,
                       const llvm::Triple &Triple, StringRef &CPUName,
                       StringRef &ABIName);
 void getLoongArchTargetFeatures(const Driver &D, const llvm::Triple &Triple,
                            const llvm::opt::ArgList &Args,
                            std::vector<StringRef> &Features);
-StringRef getGnuCompatibleLoongArchABIName(StringRef ABI);
-loongarch::FloatABI getLoongArchFloatABI(const Driver &D, const llvm::opt::ArgList &Args);
 std::string getLoongArchABILibSuffix(const llvm::opt::ArgList &Args,
                                 const llvm::Triple &Triple);
 bool hasLoongArchAbiArg(const llvm::opt::ArgList &Args, const char *Value);
+bool hasLoongArchFpuArg(const llvm::opt::ArgList &Args, const char *Value);
 bool isUCLibc(const llvm::opt::ArgList &Args);
-bool isFP64Default(const llvm::opt::ArgList &Args);
 
 } // end namespace loongarch
 } // end namespace target
diff --git a/clang/lib/Driver/ToolChains/Clang.cpp b/clang/lib/Driver/ToolChains/Clang.cpp
index 120adfc9b7444..14e2dea98f85c 100644
--- a/clang/lib/Driver/ToolChains/Clang.cpp
+++ b/clang/lib/Driver/ToolChains/Clang.cpp
@@ -1745,19 +1745,6 @@ void Clang::AddLoongArchTargetArgs(const ArgList &Args,
   CmdArgs.push_back("-target-abi");
   CmdArgs.push_back(ABIName.data());
 
-  loongarch::FloatABI ABI = loongarch::getLoongArchFloatABI(D, Args);
-  if (ABI == loongarch::FloatABI::Soft) {
-    // Floating point operations and argument passing are soft.
-    CmdArgs.push_back("-msoft-float");
-    CmdArgs.push_back("-mfloat-abi");
-    CmdArgs.push_back("soft");
-  } else {
-    // Floating point operations and argument passing are hard.
-    assert(ABI == loongarch::FloatABI::Hard && "Invalid float abi!");
-    CmdArgs.push_back("-mfloat-abi");
-    CmdArgs.push_back("hard");
-  }
-
   if (Arg *A = Args.getLastArg(options::OPT_mcheck_zero_division,
                                options::OPT_mno_check_zero_division)) {
     if (A->getOption().matches(options::OPT_mno_check_zero_division)) {
diff --git a/clang/lib/Driver/ToolChains/Gnu.cpp b/clang/lib/Driver/ToolChains/Gnu.cpp
index aae9c93069d4f..72990d7f96d5b 100644
--- a/clang/lib/Driver/ToolChains/Gnu.cpp
+++ b/clang/lib/Driver/ToolChains/Gnu.cpp
@@ -833,26 +833,18 @@ void tools::gnutools::Assembler::ConstructJob(Compilation &C,
     StringRef CPUName;
     StringRef ABIName;
     loongarch::getLoongArchCPUAndABI(Args, getToolChain().getTriple(), CPUName, ABIName);
-    ABIName = loongarch::getGnuCompatibleLoongArchABIName(ABIName);
 
     //FIXME: Currently gnu as doesn't support -march
     //CmdArgs.push_back("-march=loongarch");
     //CmdArgs.push_back(CPUName.data());
 
-    //FIXME: modify loongarch::getGnuCompatibleLoongArchABIName()
     CmdArgs.push_back("-mabi=lp64d");
-    //CmdArgs.push_back(ABIName.data());
 
     // -mno-shared should be emitted unless -fpic, -fpie, -fPIC, -fPIE,
     // or -mshared (not implemented) is in effect.
     if (RelocationModel == llvm::Reloc::Static)
       CmdArgs.push_back("-mno-shared");
 
-    // LLVM doesn't support -mplt yet and acts as if it is always given.
-    // However, -mplt has no effect with the LP64D ABI.
-    if (ABIName != "64")
-      CmdArgs.push_back("-call_nonpic");
-
     break;
 
     // Add the last -mfp32/-mfp64.
diff --git a/clang/lib/Driver/ToolChains/Linux.cpp b/clang/lib/Driver/ToolChains/Linux.cpp
index fec429b7a5191..6a92d2bc35a73 100644
--- a/clang/lib/Driver/ToolChains/Linux.cpp
+++ b/clang/lib/Driver/ToolChains/Linux.cpp
@@ -487,9 +487,10 @@ std::string Linux::getDynamicLinker(const ArgList &Args) const {
   }
   case llvm::Triple::loongarch32:
   case llvm::Triple::loongarch64: {
+    StringRef CPUName, ABIName;
+    tools::loongarch::getLoongArchCPUAndABI(Args, Triple, CPUName, ABIName);
     LibDir = "lib" + tools::loongarch::getLoongArchABILibSuffix(Args, Triple);
-    //TODO: support other ABIs
-    Loader = "ld-linux-loongarch-lp64d.so.1";
+    Loader = ("ld-linux-loongarch-" + ABIName + ".so.1").str();
     break;
   }
   case llvm::Triple::mips:
diff --git a/clang/test/CodeGen/struct-128.cpp b/clang/test/CodeGen/struct-128.cpp
new file mode 100644
index 0000000000000..573a16eb18241
--- /dev/null
+++ b/clang/test/CodeGen/struct-128.cpp
@@ -0,0 +1,23 @@
+// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
+// RUN: %clang_cc1  -triple loongarch64-linux-gnu -emit-llvm %s -o - | FileCheck %s
+
+//Check that a struct return value with a certain condition on loongarch64 is returned by address
+
+typedef struct _B {
+    int *ptr;
+    int index;
+} B;
+
+typedef struct _A : B {
+    float dd;
+} A;
+
+// CHECK-LABEL: void @_Z3foov(%struct._A*
+// CHECK-NEXT:  entry:
+// CHECK-NEXT:    ret void
+//
+A foo()
+{
+    A a;
+    return a;
+}
diff --git a/clang/test/CodeGen/struct-packed-128.cpp b/clang/test/CodeGen/struct-packed-128.cpp
new file mode 100644
index 0000000000000..d31ccb08d985a
--- /dev/null
+++ b/clang/test/CodeGen/struct-packed-128.cpp
@@ -0,0 +1,23 @@
+// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
+// RUN: %clang_cc1  -triple loongarch64-linux-gnu -emit-llvm %s -o - | FileCheck %s
+typedef struct _B {
+    int *ptr;
+    int index;
+} __attribute__((packed)) B;
+
+typedef struct _A : B {
+    float dd;
+} A;
+
+// CHECK-LABEL: [2 x i64] @_Z3foov(
+// CHECK-NEXT:  entry:
+// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__A:%.*]], align 4
+// CHECK-NEXT:    [[TMP0:%.*]] = bitcast %struct._A* [[RETVAL]] to [2 x i64]*
+// CHECK-NEXT:    [[TMP1:%.*]] = load [2 x i64], [2 x i64]* [[TMP0]], align 4
+// CHECK-NEXT:    ret [2 x i64] [[TMP1]]
+//
+A foo()
+{
+    A a;
+    return a;
+}
diff --git a/clang/test/CodeGen/struct-private-128.cpp b/clang/test/CodeGen/struct-private-128.cpp
new file mode 100644
index 0000000000000..336fd817a830c
--- /dev/null
+++ b/clang/test/CodeGen/struct-private-128.cpp
@@ -0,0 +1,24 @@
+// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
+// RUN: %clang_cc1  -triple loongarch64-linux-gnu -emit-llvm %s -o - | FileCheck %s
+typedef struct _B {
+    int *ptr;
+private:
+    int index;
+} B;
+
+typedef struct _A : B {
+    float dd;
+} A;
+
+// CHECK-LABEL: @_Z3foov(
+// CHECK-NEXT:  entry:
+// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__A:%.*]], align 8
+// CHECK-NEXT:    [[TMP0:%.*]] = bitcast %struct._A* [[RETVAL]] to [2 x i64]*
+// CHECK-NEXT:    [[TMP1:%.*]] = load [2 x i64], [2 x i64]* [[TMP0]], align 8
+// CHECK-NEXT:    ret [2 x i64] [[TMP1]]
+//
+A foo()
+{
+    A a;
+    return a;
+}
diff --git a/clang/test/CodeGen/struct-static.cpp b/clang/test/CodeGen/struct-static.cpp
new file mode 100644
index 0000000000000..4253a80d08156
--- /dev/null
+++ b/clang/test/CodeGen/struct-static.cpp
@@ -0,0 +1,25 @@
+// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
+// RUN: %clang_cc1  -triple loongarch64-linux-gnu -emit-llvm %s -o - | FileCheck %s
+
+typedef struct _B {
+ static long ptr;
+ int index;
+} B;
+
+typedef struct _A : B {
+ float dd;
+} A;
+
+// CHECK-LABEL: @_Z3foov(
+// CHECK-NEXT:  entry:
+// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT__A:%.*]], align 4
+// CHECK-NEXT:    [[TMP0:%.*]] = bitcast %struct._A* [[RETVAL]] to i64*
+// CHECK-NEXT:    [[TMP1:%.*]] = load i64, i64* [[TMP0]], align 4
+// CHECK-NEXT:    ret i64 [[TMP1]]
+//
+A foo()
+{
+ A a;
+ return a;
+}
+
diff --git a/clang/test/Driver/loongarch-abi-fpu.c b/clang/test/Driver/loongarch-abi-fpu.c
new file mode 100644
index 0000000000000..180d440c9981c
--- /dev/null
+++ b/clang/test/Driver/loongarch-abi-fpu.c
@@ -0,0 +1,26 @@
+/// Check passing -mabi=<ABIName> and -mfpu=<FPU> options to the backend.
+
+// RUN: %clang -target loongarch64 %s -mabi=lp64s -mfpu=none -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=FEATURE-NF-ND %s
+// RUN: %clang -target loongarch64 %s -mabi=lp64s -mfpu=32 -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=FEATURE-F %s
+// RUN: %clang -target loongarch64 %s -mabi=lp64s -mfpu=64 -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=FEATURE-D %s
+// RUN: %clang -target loongarch64 %s -mabi=lp64f -mfpu=none -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=ERRLP64F-WITH-FPUNONE %s
+// RUN: %clang -target loongarch64 %s -mabi=lp64f -mfpu=32 -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=FEATURE-F %s
+// RUN: %clang -target loongarch64 %s -mabi=lp64f -mfpu=64 -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=FEATURE-D %s
+// RUN: %clang -target loongarch64 %s -mabi=lp64d -mfpu=none -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=ERRLP64D-ONLY-FPU64 %s
+// RUN: %clang -target loongarch64 %s -mabi=lp64d -mfpu=32 -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=ERRLP64D-ONLY-FPU64 %s
+// RUN: %clang -target loongarch64 %s -mabi=lp64d -mfpu=64 -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=FEATURE-D %s
+
+// FEATURE-D: "-target-feature" "+d"
+// FEATURE-F: "-target-feature" "+f"
+// FEATURE-NF-ND: "-target-feature" "-f" "-target-feature" "-d"
+// ERRLP64D-ONLY-FPU64: error: option 'lp64d' cannot be specified without '-mfpu=64'
+// ERRLP64F-WITH-FPUNONE: error: option 'lp64f' cannot be specified with '-mfpu=none'
diff --git a/clang/test/Driver/loongarch-double-single-soft.c b/clang/test/Driver/loongarch-double-single-soft.c
new file mode 100644
index 0000000000000..4b25f876575d5
--- /dev/null
+++ b/clang/test/Driver/loongarch-double-single-soft.c
@@ -0,0 +1,12 @@
+// Check passing -m*-float options to the backend.
+
+// RUN: %clang -target loongarch64 %s -mdouble-float -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=CHECK-DOUBLE %s
+// RUN: %clang -target loongarch64 %s -msingle-float -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=CHECK-SINGLE %s
+// RUN: %clang -target loongarch64 %s -msoft-float -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=CHECK-SOFT %s
+
+// CHECK-DOUBLE: "-target-feature" "+d" "-target-abi" "lp64d"
+// CHECK-SINGLE: "-target-feature" "+f" "-target-abi" "lp64f"
+// CHECK-SOFT: "-target-feature" "-f" "-target-feature" "-d" "-target-abi" "lp64s"
diff --git a/clang/test/Driver/loongarch-mabi.c b/clang/test/Driver/loongarch-mabi.c
new file mode 100644
index 0000000000000..88a90408debd3
--- /dev/null
+++ b/clang/test/Driver/loongarch-mabi.c
@@ -0,0 +1,22 @@
+// Check passing -mabi=<ABIName> options to the backend.
+
+// check default ABI for loongarch64
+// RUN: %clang -target loongarch64 %s -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=CHECK-LP64D %s
+// check -mabi=lp64d option for loongarch64
+// RUN: %clang -target loongarch64 %s -mabi=lp64d -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=CHECK-LP64D %s
+// check -mabi=lp64f option for loongarch64
+// RUN: %clang -target loongarch64 %s -mabi=lp64f -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=CHECK-LP64F %s
+// check -mabi=lp64s option for loongarch64
+// RUN: %clang -target loongarch64 %s -mabi=lp64s -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=CHECK-LP64S %s
+// check invalid -mabi=x option for loongarch64
+// RUN: not %clang -target loongarch64 %s -mabi=x 2>&1 \
+// RUN:   | FileCheck --check-prefix=CHECK-X %s
+
+// CHECK-LP64D: "-target-abi" "lp64d"
+// CHECK-LP64F: "-target-abi" "lp64f"
+// CHECK-LP64S: "-target-abi" "lp64s"
+// CHECK-X: error: unknown target ABI 'x'
diff --git a/clang/test/Driver/loongarch-mfpu.c b/clang/test/Driver/loongarch-mfpu.c
new file mode 100644
index 0000000000000..0cf05fd3eccf0
--- /dev/null
+++ b/clang/test/Driver/loongarch-mfpu.c
@@ -0,0 +1,21 @@
+// Check passing -mfpu=<FPU> options to the backend.
+
+// check default feature for loongarch64
+// RUN: %clang -target loongarch64 %s -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=FEATURE-D %s
+// check -mfpu=64 option for loongarch64
+// RUN: %clang -target loongarch64 %s -mfpu=64 -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=FEATURE-D %s
+// check -mfpu=32 option for loongarch64
+// RUN: %clang -target loongarch64 %s -mfpu=32 -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=ERRLP64D-ONLY-FPU64 %s
+// check -mfpu=none option for loongarch64
+// RUN: %clang -target loongarch64 %s -mfpu=none -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=ERRLP64D-ONLY-FPU64 %s
+// check -mfpu=x option for loongarch64
+// RUN: %clang -target loongarch64 %s -mfpu=x -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=INVALID-FPU %s
+
+// FEATURE-D: "-target-feature" "+d"
+// INVALID-FPU: error: invalid loongarch FPU value 'x'. Please specify FPU = 64,32 or none
+// ERRLP64D-ONLY-FPU64: error: option 'lp64d' cannot be specified without '-mfpu=64'
diff --git a/clang/test/Preprocessor/init.c b/clang/test/Preprocessor/init.c
index 3cc36302aa0e4..89c720673d7c4 100644
--- a/clang/test/Preprocessor/init.c
+++ b/clang/test/Preprocessor/init.c
@@ -8433,3 +8433,33 @@
 // RISCV64-LINUX: #define __unix__ 1
 // RISCV64-LINUX: #define linux 1
 // RISCV64-LINUX: #define unix 1
+
+// RUN: %clang_cc1 -x c -E -dM -ffreestanding -fgnuc-version=4.2.1 -triple=loongarch64 -target-feature +d /dev/null \
+// RUN:   | FileCheck -match-full-lines -check-prefixes=LOONGARCH64,LOONGARCH64-HASBASICD %s
+// RUN: %clang_cc1 -x c -E -dM -ffreestanding -fgnuc-version=4.2.1 -triple=loongarch64 -target-feature +f /dev/null \
+// RUN:   | FileCheck -match-full-lines -check-prefixes=LOONGARCH64,LOONGARCH64-HASBASICF %s
+// RUN: %clang_cc1 -x c -E -dM -ffreestanding -fgnuc-version=4.2.1 -triple=loongarch64 -target-feature -d  -target-feature -f /dev/null \
+// RUN:   | FileCheck -match-full-lines -check-prefixes=LOONGARCH64,LOONGARCH64-SOFT %s
+// RUN: %clang_cc1 -x c -E -dM -ffreestanding -fgnuc-version=4.2.1 -triple=loongarch64 -target-abi lp64s /dev/null \
+// RUN:   | FileCheck -match-full-lines -check-prefixes=LOONGARCH64,LOONGARCH64-LP64S %s
+// RUN: %clang_cc1 -x c -E -dM -ffreestanding -fgnuc-version=4.2.1 -triple=loongarch64 -target-abi lp64f /dev/null \
+// RUN:   | FileCheck -match-full-lines -check-prefixes=LOONGARCH64,LOONGARCH64-LP64F,LOONGARCH64-HARD %s
+// RUN: %clang_cc1 -x c -E -dM -ffreestanding -fgnuc-version=4.2.1 -triple=loongarch64 -target-abi lp64d /dev/null \
+// RUN:   | FileCheck -match-full-lines -check-prefixes=LOONGARCH64,LOONGARCH64-LP64D,LOONGARCH64-HARD %s
+// RUN: %clang_cc1 -x c -E -dM -ffreestanding -fgnuc-version=4.2.1 -triple=loongarch64 /dev/null \
+// RUN:   | FileCheck -match-full-lines -check-prefix=LOONGARCH64 %s
+// LOONGARCH64: #define _LOONGARCH_ARCH "loongarch64"
+// LOONGARCH64: #define _LOONGARCH_SZINT 32
+// LOONGARCH64: #define _LOONGARCH_SZLONG 64
+// LOONGARCH64: #define _LOONGARCH_SZPTR 64
+// LOONGARCH64: #define _LOONGARCH_TUNE "la464"
+// LOONGARCH64: #define __loongarch__ 1
+// LOONGARCH64-LP64D: #define __loongarch_double_float 1
+// LOONGARCH64-HASBASICD: #define __loongarch_frlen 64
+// LOONGARCH64-HASBASICF: #define __loongarch_frlen 32
+// LOONGARCH64-SOFT: #define __loongarch_frlen 0
+// LOONGARCH64: #define __loongarch_grlen 64
+// LOONGARCH64-HARD: #define __loongarch_hard_float 1
+// LOONGARCH64: #define __loongarch_lp64 1
+// LOONGARCH64-LP64F: #define __loongarch_single_float 1
+// LOONGARCH64-LP64S: #define __loongarch_soft_float 1
diff --git a/llvm/include/llvm/BinaryFormat/ELF.h b/llvm/include/llvm/BinaryFormat/ELF.h
index df7fb25b2d7db..ff7508b424e66 100644
--- a/llvm/include/llvm/BinaryFormat/ELF.h
+++ b/llvm/include/llvm/BinaryFormat/ELF.h
@@ -636,10 +636,13 @@ enum : unsigned {
   // FIXME: Change these when all ABIs definition were finalized.
   // See current definitions:
   // https://loongson.github.io/LoongArch-Documentation/LoongArch-ELF-ABI-EN.html#_e_flags_identifies_abi_type_and_version
-  EF_LARCH_ABI = 0x0003,
-  EF_LARCH_ABI_LP32 = 0x0001,
-  EF_LARCH_ABI_XLP32 = 0x0002,
-  EF_LARCH_ABI_LP64D = 0x0003,
+  EF_LARCH_BASE_ABI = 0x3,
+  EF_LARCH_BASE_ABI_ILP32S = 0x5,
+  EF_LARCH_BASE_ABI_ILP32F = 0x6,
+  EF_LARCH_BASE_ABI_ILP32D = 0x7,
+  EF_LARCH_BASE_ABI_LP64S = 0x1,
+  EF_LARCH_BASE_ABI_LP64F = 0x2,
+  EF_LARCH_BASE_ABI_LP64D = 0x3
 };
 
 // ELF Relocation types for LoongArch
diff --git a/llvm/lib/Target/LoongArch/AsmParser/LoongArchAsmParser.cpp b/llvm/lib/Target/LoongArch/AsmParser/LoongArchAsmParser.cpp
index b036936e87b34..a012f8af1c825 100644
--- a/llvm/lib/Target/LoongArch/AsmParser/LoongArchAsmParser.cpp
+++ b/llvm/lib/Target/LoongArch/AsmParser/LoongArchAsmParser.cpp
@@ -259,23 +259,18 @@ class LoongArchAsmParser : public MCTargetAsmParser {
     return getSTI().getFeatureBits()[LoongArch::Feature64Bit];
   }
 
-  bool isFP64bit() const {
-    return getSTI().getFeatureBits()[LoongArch::FeatureFP64Bit];
-  }
-
   const LoongArchABIInfo &getABI() const { return ABI; }
-  bool isABI_LPX32() const { return ABI.IsLPX32(); }
   bool isABI_LP64D() const { return ABI.IsLP64D(); }
-  bool isABI_LP32() const { return ABI.IsLP32(); }
+  bool isABI_LP64S() const { return ABI.IsLP64S(); }
+  bool isABI_LP64F() const { return ABI.IsLP64F(); }
+  bool isABI_ILP32D() const { return ABI.IsILP32D(); }
+  bool isABI_ILP32F() const { return ABI.IsILP32F(); }
+  bool isABI_ILP32S() const { return ABI.IsILP32S(); }
 
   bool inPicMode() {
     return IsPicEnabled;
   }
 
-  bool useSoftFloat() const {
-    return getSTI().getFeatureBits()[LoongArch::FeatureSoftFloat];
-  }
-
   const MCExpr *createTargetUnaryExpr(const MCExpr *E,
                                       AsmToken::TokenKind OperatorToken,
                                       MCContext &Ctx) override {
@@ -299,7 +294,7 @@ class LoongArchOperand : public MCParsedAsmOperand {
   /// The exact class is finalized by the render method.
   enum RegKind {
     RegKind_GPR = 1,      /// GPR32 and GPR64 (depending on is64Bit())
-    RegKind_FGR = 2,      /// FGR32, FGR64 (depending on isFP64bit())
+    RegKind_FGR = 2,      /// FGR32, FGR64 (depending on hasBasicD())
     RegKind_FCFR = 4,     /// FCFR
     RegKind_FCSR = 8,     /// FCSR
     RegKind_Numeric = RegKind_GPR | RegKind_FGR | RegKind_FCFR | RegKind_FCSR
@@ -1071,25 +1066,49 @@ bool LoongArchAsmParser::processInstruction(MCInst &Inst, SMLoc IDLoc,
 
   Inst.setLoc(IDLoc);
 
+  // Check branch instructions.
   if (MCID.isBranch() || MCID.isCall()) {
     const unsigned Opcode = Inst.getOpcode();
     MCOperand Offset;
-
+    bool check = true;
+    unsigned OffsetOpndIdx, OffsetOpndWidth;
     switch (Opcode) {
     default:
+      check = false;
       break;
     case LoongArch::BEQ:
     case LoongArch::BNE:
-      assert(MCID.getNumOperands() == 3 && "unexpected number of operands");
-      Offset = Inst.getOperand(2);
-      if (!Offset.isImm())
-        break; // We'll deal with this situation later on when applying fixups.
-      if (!isIntN(17, Offset.getImm()))
-        return Error(IDLoc, "branch target out of range");
-      if (offsetToAlignment(Offset.getImm(),
-                            Align(1LL << 2)))
-        return Error(IDLoc, "branch to misaligned address");
+    case LoongArch::BLT:
+    case LoongArch::BGE:
+    case LoongArch::BLTU:
+    case LoongArch::BGEU:
+      OffsetOpndIdx = 2;
+      OffsetOpndWidth = 16;
       break;
+    case LoongArch::BEQZ:
+    case LoongArch::BNEZ:
+    case LoongArch::BCEQZ:
+    case LoongArch::BCNEZ:
+      OffsetOpndIdx = 1;
+      OffsetOpndWidth = 21;
+      break;
+    case LoongArch::B:
+    case LoongArch::BL:
+      OffsetOpndIdx = 0;
+      OffsetOpndWidth = 26;
+      break;
+    }
+    if (check) {
+      assert(MCID.getNumOperands() == OffsetOpndIdx + 1 &&
+             "unexpected number of operands");
+      Offset = Inst.getOperand(OffsetOpndIdx);
+      // Non-Imm situation will be dealed with later on when applying fixups.
+      if (Offset.isImm()) {
+        if (!isIntN(OffsetOpndWidth + 2, Offset.getImm()))
+          return Error(IDLoc, "branch target out of range");
+        if (offsetToAlignment(Offset.getImm(), Align(1LL << 2)))
+          return Error(IDLoc, "branch to misaligned address");
+      }
     }
   }
 
diff --git a/llvm/lib/Target/LoongArch/Disassembler/LoongArchDisassembler.cpp b/llvm/lib/Target/LoongArch/Disassembler/LoongArchDisassembler.cpp
index 42031bca92b88..935dce7a3ac5e 100644
--- a/llvm/lib/Target/LoongArch/Disassembler/LoongArchDisassembler.cpp
+++ b/llvm/lib/Target/LoongArch/Disassembler/LoongArchDisassembler.cpp
@@ -42,8 +42,6 @@ class LoongArchDisassembler : public MCDisassembler {
   LoongArchDisassembler(const MCSubtargetInfo &STI, MCContext &Ctx)
       : MCDisassembler(STI, Ctx) {}
 
-  bool isFP64() const { return STI.getFeatureBits()[LoongArch::FeatureFP64Bit]; }
-
   bool is64Bit() const { return STI.getFeatureBits()[LoongArch::Feature64Bit]; }
 
   DecodeStatus getInstruction(MCInst &Instr, uint64_t &Size,
diff --git a/llvm/lib/Target/LoongArch/LoongArch.td b/llvm/lib/Target/LoongArch/LoongArch.td
index f839264559f50..d24b12eb6d35a 100644
--- a/llvm/lib/Target/LoongArch/LoongArch.td
+++ b/llvm/lib/Target/LoongArch/LoongArch.td
@@ -23,13 +23,13 @@ class PredicateControl {
   list<Predicate> EncodingPredicates = [];
   // Predicates for the GPR size such as is64Bit
   list<Predicate> GPRPredicates = [];
-  // Predicates for the FGR size and layout such as IsFP64bit
+  // Predicates for the FGR size and layout such as HasBasicD
   list<Predicate> FGRPredicates = [];
-  // Predicates for the instruction group membership such as ISA's.
+  // Predicates for the instruction group membership such as ISA's
   list<Predicate> InsnPredicates = [];
-  // Predicate for the ISA extension that an instruction belongs to.
+  // Predicate for the ISA extension that an instruction belongs to
   list<Predicate> ExtPredicate = [];
-  // Predicate for marking the instruction as usable in hard-float mode only.
+  // Predicate for marking the instruction as usable in hard-float mode only
   list<Predicate> HardFloatPredicate = [];
   // Predicates for anything else
   list<Predicate> AdditionalPredicates = [];
@@ -48,18 +48,17 @@ class AdditionalRequires<list<Predicate> preds> {
 }
 
 //===----------------------------------------------------------------------===//
-// LoongArch Subtarget features                                                    //
+// LoongArch Subtarget features                                               //
 //===----------------------------------------------------------------------===//
 
-def FeatureFP64Bit     : SubtargetFeature<"fp64", "IsFP64bit", "true",
-                                "Support 64-bit FP registers">;
-def FeatureSingleFloat : SubtargetFeature<"single-float", "IsSingleFloat",
-                                "true", "Only supports single precision float">;
-def FeatureSoftFloat   : SubtargetFeature<"soft-float", "IsSoftFloat", "true",
-                                "Does not support floating point instructions">;
-def Feature64Bit       : SubtargetFeature<"64bit", "HasLA64", "true",
-                                "Support LA64 ISA",
-                                [FeatureFP64Bit]>;
+def Feature64Bit
+    : SubtargetFeature<"64bit", "HasLA64", "true",
+                       "LA64 Basic Integer and Privilege Instruction Set">;
+def FeatureBasicF : SubtargetFeature<"f", "HasBasicF", "true",
+                                     "'F' (Single-Precision Floating-Point)">;
+def FeatureBasicD : SubtargetFeature<"d", "HasBasicD", "true",
+                                     "'D' (Double-Precision Floating-Point)",
+                                     [FeatureBasicF]>;
 
 //===----------------------------------------------------------------------===//
 // Register File, Calling Conv, Instruction Descriptions
diff --git a/llvm/lib/Target/LoongArch/LoongArch32InstrInfo.td b/llvm/lib/Target/LoongArch/LoongArch32InstrInfo.td
index 231aca00f9186..ae30e7ea1d60c 100644
--- a/llvm/lib/Target/LoongArch/LoongArch32InstrInfo.td
+++ b/llvm/lib/Target/LoongArch/LoongArch32InstrInfo.td
@@ -54,6 +54,8 @@ let DecoderNamespace = "LoongArch32" in {
     def AND32   : Int_Reg3<"and", GPR32Opnd, and>, R3I<0b0101001>;
     def OR32    : Int_Reg3<"or", GPR32Opnd, or>, R3I<0b0101010>;
     def XOR32   : Int_Reg3<"xor", GPR32Opnd, xor>, R3I<0b0101011>;
+    def ANDN32  : Int_Reg3<"andn", GPR32Opnd>, R3I<0b0101101>;
+    def ORN32   : Int_Reg3<"orn", GPR32Opnd>, R3I<0b0101100>;
   }
 
   def SLL_W : Shift_Var<"sll.w", GPR32Opnd, shl>, R3I<0b0101110>;
@@ -301,8 +303,6 @@ def : LoongArchPat<(select i32:$cond, immz, i32:$f),
                    (MASKNEZ32 i32:$f, i32:$cond)>;
 
 // truncate
-def : LoongArchPat<(i32 (trunc (i64 (assertsext GPR64:$src)))),
-                   (EXTRACT_SUBREG GPR64:$src, sub_32)>, GPR_64;
 def : LoongArchPat<(i32 (trunc (assertzext_lt_i32 GPR64:$src))),
                    (EXTRACT_SUBREG GPR64:$src, sub_32)>, GPR_64;
 def : LoongArchPat<(i32 (trunc GPR64:$src)),
@@ -539,3 +539,185 @@ defm : SetlePats<GPR32, XORI32, SLT32, SLTU32>;
 defm : SetgtPats<GPR32, SLT32, SLTU32>;
 defm : SetgePats<GPR32, XORI32, SLT32, SLTU32>;
 defm : SetgeImmPats<GPR32, XORI32, SLTI32, SLTUI32>;
+
+def : LoongArchPat<(i64 (sext (i32 (xor (i32 (trunc (i64 (assertsext GPR64:$rj)))), (immZExt12:$imm12))))),
+                   (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+                   (XORI32 (EXTRACT_SUBREG GPR64:$rj, sub_32), (immZExt12:$imm12)), sub_32)>, GPR_64;
+
+def : LoongArchPat<(i64 (sext (i32 (add (i32 (trunc (i64 (assertsext GPR64:$rj)))), (i32 (trunc (i64 (assertsext GPR64:$rk)))))))),
+                   (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+                   (ADD_W (EXTRACT_SUBREG GPR64:$rj, sub_32), (EXTRACT_SUBREG GPR64:$rk, sub_32)), sub_32)>, GPR_64;
+
+def : LoongArchPat<(i64 (sext (i32 (add (i32 (trunc (i64 (assertsext GPR64:$rj)))), (immSExt12:$imm12))))),
+                   (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+                   (ADDI_W (EXTRACT_SUBREG GPR64:$rj, sub_32), (immSExt12:$imm12)), sub_32)>, GPR_64;
+
+def : LoongArchPat<(i64 (sext (i32 (sra (i32 (trunc (i64 (assertsext GPR64:$rj)))), (i32 (trunc (i64 (assertsext GPR64:$rk)))))))),
+                   (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+                   (SRA_W (EXTRACT_SUBREG GPR64:$rj, sub_32), (EXTRACT_SUBREG GPR64:$rk, sub_32)), sub_32)>, GPR_64;
+
+def : LoongArchPat<(i64 (sext (i32 (srl (i32 (trunc (i64 (assertsext GPR64:$rj)))), (i32 (trunc (i64 (assertsext GPR64:$rk)))))))),
+                   (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+                   (SRL_W (EXTRACT_SUBREG GPR64:$rj, sub_32), (EXTRACT_SUBREG GPR64:$rk, sub_32)), sub_32)>, GPR_64;
+
+def : LoongArchPat<(i64 (sext (i32 (mul (i32 (trunc (i64 (assertsext GPR64:$rj)))), (i32 (trunc (i64 (assertsext GPR64:$rk)))))))),
+                   (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+                   (MUL_W (EXTRACT_SUBREG GPR64:$rj, sub_32), (EXTRACT_SUBREG GPR64:$rk, sub_32)), sub_32)>, GPR_64;
+
+def : LoongArchPat<(i64 (sext (i32 (xor (i32 (trunc (i64 (assertsext GPR64:$rj)))), (i32 (trunc (i64 (assertsext GPR64:$rk)))))))),
+                   (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+                   (XOR32 (EXTRACT_SUBREG GPR64:$rj, sub_32), (EXTRACT_SUBREG GPR64:$rk, sub_32)), sub_32)>, GPR_64;
+
+def : LoongArchPat<(i64 (sext (i32 (xor (i32 (trunc (i64 (assertsext GPR64:$rj)))), (i32 GPR32:$rk))))),
+                   (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+                   (XOR32 (EXTRACT_SUBREG GPR64:$rj, sub_32), GPR32:$rk), sub_32)>, GPR_64;
+
+def : LoongArchPat<(i64 (sext (i32 (or (i32 (trunc (i64 (assertsext GPR64:$rj)))), (uimm12_32:$imm12))))),
+                   (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+                   (ORI32 (EXTRACT_SUBREG GPR64:$rj, sub_32), (uimm12_32:$imm12)), sub_32)>, GPR_64;
+
+def : LoongArchPat<(i64 (sext (i32 (or (i32 (trunc (i64 (assertsext GPR64:$rj)))), (i32 GPR32:$rk))))),
+                   (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+                   (OR32 (EXTRACT_SUBREG GPR64:$rj, sub_32), GPR32:$rk), sub_32)>, GPR_64;
+
+def : LoongArchPat<(i64 (sext (select i32:$cond, (i32 (trunc (i64 (assertsext GPR64:$t)))), (i32 (trunc (i64 (assertsext GPR64:$f))))))),
+                   (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+                   (OR32 (MASKEQZ32 (EXTRACT_SUBREG GPR64:$t, sub_32), i32:$cond),
+                         (MASKNEZ32 (EXTRACT_SUBREG GPR64:$f, sub_32), i32:$cond)), sub_32)>;
+
+def : LoongArchPat<(i64 (sext (i32 (shl (i32 (trunc (i64 (assertsext GPR64:$rj)))), (i32 (trunc (i64 (assertsext GPR64:$rk)))))))),
+                   (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+                   (SLL_W (EXTRACT_SUBREG GPR64:$rj, sub_32), (EXTRACT_SUBREG GPR64:$rk, sub_32)), sub_32)>, GPR_64;
+
+def : LoongArchPat<(i64 (sext (i32 (srem (i32 (trunc (i64 (assertsext GPR64:$rj)))), (i32 (trunc (i64 (assertsext GPR64:$rk)))))))),
+                   (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+                   (MOD_W (EXTRACT_SUBREG GPR64:$rj, sub_32), (EXTRACT_SUBREG GPR64:$rk, sub_32)), sub_32)>, GPR_64;
+
+def : LoongArchPat<(atomic_store_32 addr:$a, (i32 (trunc (i64 (assertsext GPR64:$rj))))),
+                   (ST_W32 (EXTRACT_SUBREG GPR64:$rj, sub_32), addr:$a)>, GPR_64;
+
+def : LoongArchPat<(i64 (sext (i32 (sub (i32 (trunc (i64 (assertsext GPR64:$rj)))), (i32 (trunc (i64 (assertsext GPR64:$rk)))))))),
+                   (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+                   (SUB_W (EXTRACT_SUBREG GPR64:$rj, sub_32), (EXTRACT_SUBREG GPR64:$rk, sub_32)), sub_32)>, GPR_64;
+
+def : LoongArchPat<(i64 (sext (i32 (udiv (i32 (trunc (i64 (assertsext GPR64:$rj)))), (i32 (trunc (i64 (assertsext GPR64:$rk)))))))),
+                   (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+                   (DIV_WU (EXTRACT_SUBREG GPR64:$rj, sub_32), (EXTRACT_SUBREG GPR64:$rk, sub_32)), sub_32)>, GPR_64;
+
+def : LoongArchPat<(i64 (sext (i32 (urem (i32 (trunc (i64 (assertsext GPR64:$rj)))), (i32 (trunc (i64 (assertsext GPR64:$rk)))))))),
+                   (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+                   (MOD_WU (EXTRACT_SUBREG GPR64:$rj, sub_32), (EXTRACT_SUBREG GPR64:$rk, sub_32)), sub_32)>, GPR_64;
+
+def : LoongArchPat<(brcond (i32 (seteq (i32 (trunc (i64 (assertsext GPR64:$rj)))), 0)), bb:$offs21),
+                   (BEQZ32 (EXTRACT_SUBREG GPR64:$rj, sub_32), brtarget:$offs21)>;
+
+def : LoongArchPat<(setne (i32 (trunc (i64 (assertsext GPR64:$rj)))), 0),
+                   (SLTU32 ZERO, (EXTRACT_SUBREG GPR64:$rj, sub_32))>;
+
+def : LoongArchPat<(select i32:$cond, (i32 (trunc (i64 (assertsext GPR64:$t)))), (i32 (trunc (i64 (assertsext GPR64:$f))))),
+                   (OR32 (MASKEQZ32 (EXTRACT_SUBREG GPR64:$t, sub_32), i32:$cond),
+                         (MASKNEZ32 (EXTRACT_SUBREG GPR64:$f, sub_32), i32:$cond))>;
+
+def : LoongArchPat<(select (i32 (setne (i32 (trunc (i64 (assertsext GPR64:$cond)))), immz)), immz, i32:$f),
+                   (MASKNEZ32 i32:$f, (EXTRACT_SUBREG GPR64:$cond, sub_32))>;
+
+def : LoongArchPat<(select (i32 (seteq (i32 (trunc (i64 (assertsext GPR64:$cond)))), immz)), immz, i32:$f),
+                   (MASKEQZ32 i32:$f, (EXTRACT_SUBREG GPR64:$cond, sub_32))>;
+
+  def : LoongArchPat<(store (i32 (trunc (i64 (assertsext GPR64:$v)))), addr:$a),
+                     (ST_W32 (EXTRACT_SUBREG GPR64:$v, sub_32), addr:$a)>;
+
+
+def : LoongArchPat<(i32 (xor GPR32:$rj, (i32 -1))),
+                   (NOR32 ZERO, GPR32:$rj)>;
+
+def : LoongArchPat<(and GPR32:$rj, (i32 (xor GPR32:$rk, (i32 -1)))),
+                   (ANDN32 GPR32:$rj, GPR32:$rk)>;
+
+def : LoongArchPat<
+                   (i64
+                     (sext
+                       (i32 (and (i32 (trunc (i64 (assertsext GPR64:$rj)))),
+                                 (i32 (xor (i32 (trunc (i64 (assertsext GPR64:$rk)))),
+                                           (i32 -1))))
+                       )
+                      )
+                     ),
+                   (INSERT_SUBREG
+                     (i64 (IMPLICIT_DEF)),
+                     (ANDN32 (EXTRACT_SUBREG GPR64:$rj, sub_32),
+                             (EXTRACT_SUBREG GPR64:$rk, sub_32)),
+                      sub_32
+                  )>;
+
+def : LoongArchPat<
+                  (i64
+                    (sext
+                      (i32 (or (i32 (trunc (i64 (assertsext GPR64:$rj)))),
+                               (i32 (xor (i32 (trunc (i64 (assertsext GPR64:$rk)))),
+                                         (i32 -1))))
+                      )
+                    )
+                  ),
+                   (INSERT_SUBREG
+                     (i64 (IMPLICIT_DEF)),
+                     (ORN32 (EXTRACT_SUBREG GPR64:$rj, sub_32),
+                            (EXTRACT_SUBREG GPR64:$rk, sub_32)),
+                    sub_32
+                  )>;
+
+def : LoongArchPat<(i64
+                     (sext
+                       (i32 (xor (i32 (or (i32 (trunc (i64 (assertsext GPR64:$rj)))),
+                                          (i32 (trunc (i64 (assertsext GPR64:$rk)))))),
+                                 (i32 -1))
+                       )
+                      )
+                    ),
+                   (INSERT_SUBREG
+                     (i64 (IMPLICIT_DEF)),
+                     (NOR32 (EXTRACT_SUBREG GPR64:$rj, sub_32),
+                            (EXTRACT_SUBREG GPR64:$rk, sub_32)),
+                    sub_32
+                   )>;
+
+def : LoongArchPat<(i64
+                     (sext
+                       (i32 (xor (i32 (trunc (i64 (or (i64 (assertsext GPR64:$rj)),
+                                                      (i64 (assertsext GPR64:$rk)))))),
+                                 (i32 -1))
+                       )
+                      )
+                    ),
+                   (INSERT_SUBREG
+                     (i64 (IMPLICIT_DEF)),
+                     (NOR32 (EXTRACT_SUBREG GPR64:$rk, sub_32),
+                            (EXTRACT_SUBREG GPR64:$rj, sub_32)),
+                    sub_32
+                   )>;
+
+def : LoongArchPat<(i64
+                      (sext
+                        (i32 (xor (i32 (trunc (i64 (assertsext GPR64:$rj)))),
+                                  (i32 -1))
+                        )
+                      )
+                    ),
+                   (INSERT_SUBREG
+                     (i64 (IMPLICIT_DEF)),
+                     (NOR32 ZERO, (EXTRACT_SUBREG GPR64:$rj, sub_32)),
+                   sub_32
+                  )>;
+
+def : LoongArchPat<(i64
+                     (zext
+                       (i32 (seteq (i32 (trunc (i64 (assertsext GPR64:$rj)))),
+                                   (i32 0))
+                       )
+                      )
+                    ),
+                   (INSERT_SUBREG
+                     (i64 (IMPLICIT_DEF)),
+                     (SLTUI32 (EXTRACT_SUBREG GPR64:$rj, sub_32), (i32 1)),
+                    sub_32
+                  )>;
diff --git a/llvm/lib/Target/LoongArch/LoongArchAsmPrinter.cpp b/llvm/lib/Target/LoongArch/LoongArchAsmPrinter.cpp
index ccda6013f2255..c010b72ff8a07 100644
--- a/llvm/lib/Target/LoongArch/LoongArchAsmPrinter.cpp
+++ b/llvm/lib/Target/LoongArch/LoongArchAsmPrinter.cpp
@@ -114,6 +114,19 @@ void LoongArchAsmPrinter::emitPseudoIndirectBranch(MCStreamer &OutStreamer,
   EmitToStreamer(OutStreamer, TmpInst0);
 }
 
+void LoongArchAsmPrinter::emitPseudoTailBranch(MCStreamer &OutStreamer,
+                                               const MachineInstr *MI) {
+  MCInst TmpInst;
+  TmpInst.setOpcode(LoongArch::B);
+
+  MCOperand MCOp;
+
+  lowerOperand(MI->getOperand(0), MCOp);
+  TmpInst.addOperand(MCOp);
+
+  EmitToStreamer(OutStreamer, TmpInst);
+}
+
 void LoongArchAsmPrinter::emitInstruction(const MachineInstr *MI) {
   LoongArchTargetStreamer &TS = getTargetStreamer();
   unsigned Opc = MI->getOpcode();
@@ -155,6 +168,10 @@ void LoongArchAsmPrinter::emitInstruction(const MachineInstr *MI) {
       emitPseudoIndirectBranch(*OutStreamer, &*I);
       continue;
     }
+    if (I->getOpcode() == LoongArch::PseudoTailReturn){
+      emitPseudoTailBranch(*OutStreamer, &*I);
+      continue;
+    }
 
     // Some instructions are marked as pseudo right now which
     // would make the test fail for the wrong reason but
@@ -186,9 +203,18 @@ void LoongArchAsmPrinter::emitInstruction(const MachineInstr *MI) {
 /// Emit Set directives.
 const char *LoongArchAsmPrinter::getCurrentABIString() const {
   switch (static_cast<LoongArchTargetMachine &>(TM).getABI().GetEnumValue()) {
-  case LoongArchABIInfo::ABI::LP32:  return "abilp32";
-  case LoongArchABIInfo::ABI::LPX32:  return "abilpx32";
-  case LoongArchABIInfo::ABI::LP64D:  return "abilp64d";
+  case LoongArchABIInfo::ABI::ILP32D:
+    return "abiilp32d";
+  case LoongArchABIInfo::ABI::ILP32F:
+    return "abiilp32f";
+  case LoongArchABIInfo::ABI::ILP32S:
+    return "abiilp32s";
+  case LoongArchABIInfo::ABI::LP64D:
+    return "abilp64d";
+  case LoongArchABIInfo::ABI::LP64S:
+    return "abilp64s";
+  case LoongArchABIInfo::ABI::LP64F:
+    return "abilp64f";
   default: llvm_unreachable("Unknown LoongArch ABI");
   }
 }
@@ -459,15 +485,6 @@ void LoongArchAsmPrinter::emitStartOfAsmFile(Module &M) {
   const LoongArchTargetMachine &MTM = static_cast<const LoongArchTargetMachine &>(TM);
   const LoongArchSubtarget STI(TT, CPU, FS, MTM, None);
 
-  const LoongArchABIInfo &ABI = MTM.getABI();
-
-  // Tell the assembler which ABI we are using
-  std::string SectionName = std::string(".mdebug.") + getCurrentABIString();
-  OutStreamer->SwitchSection(
-      OutContext.getELFSection(SectionName, ELF::SHT_PROGBITS, 0));
-
-  // TODO: handle O64 ABI
-
   TS.updateABIInfo(STI);
 }
 
diff --git a/llvm/lib/Target/LoongArch/LoongArchAsmPrinter.h b/llvm/lib/Target/LoongArch/LoongArchAsmPrinter.h
index 49d98895fdfab..0facaa294721e 100644
--- a/llvm/lib/Target/LoongArch/LoongArchAsmPrinter.h
+++ b/llvm/lib/Target/LoongArch/LoongArchAsmPrinter.h
@@ -74,6 +74,9 @@ class LLVM_LIBRARY_VISIBILITY LoongArchAsmPrinter : public AsmPrinter {
   void emitPseudoIndirectBranch(MCStreamer &OutStreamer,
                                 const MachineInstr *MI);
 
+  void emitPseudoTailBranch(MCStreamer &OutStreamer,
+                            const MachineInstr *MI);
+
   // lowerOperand - Convert a MachineOperand into the equivalent MCOperand.
   bool lowerOperand(const MachineOperand &MO, MCOperand &MCOp);
 
diff --git a/llvm/lib/Target/LoongArch/LoongArchCCState.cpp b/llvm/lib/Target/LoongArch/LoongArchCCState.cpp
index 6630ca7598aca..18996f1e2307b 100644
--- a/llvm/lib/Target/LoongArch/LoongArchCCState.cpp
+++ b/llvm/lib/Target/LoongArch/LoongArchCCState.cpp
@@ -84,7 +84,7 @@ void LoongArchCCState::PreAnalyzeCallResultForF128(
 }
 
 /// Identify lowered values that originated from f128 or float arguments and
-/// record this for use by RetCC_LoongArchLP64LPX32.
+/// record this for use by RetCC_LoongArchLP64.
 void LoongArchCCState::PreAnalyzeReturnForF128(
     const SmallVectorImpl<ISD::OutputArg> &Outs) {
   const MachineFunction &MF = getMachineFunction();
diff --git a/llvm/lib/Target/LoongArch/LoongArchCCState.h b/llvm/lib/Target/LoongArch/LoongArchCCState.h
index 1c1a1446efba6..56d5b89b1ca98 100644
--- a/llvm/lib/Target/LoongArch/LoongArchCCState.h
+++ b/llvm/lib/Target/LoongArch/LoongArchCCState.h
@@ -28,12 +28,12 @@ class LoongArchCCState : public CCState {
 
 private:
   /// Identify lowered values that originated from f128 arguments and record
-  /// this for use by RetCC_LoongArchLP64LPX32.
+  /// this for use by RetCC_LoongArchLP64.
   void PreAnalyzeCallResultForF128(const SmallVectorImpl<ISD::InputArg> &Ins,
                                    const Type *RetTy, const char * Func);
 
   /// Identify lowered values that originated from f128 arguments and record
-  /// this for use by RetCC_LoongArchLP64LPX32.
+  /// this for use by RetCC_LoongArchLP64.
   void PreAnalyzeReturnForF128(const SmallVectorImpl<ISD::OutputArg> &Outs);
 
   /// Identify lowered values that originated from f128 arguments and record
@@ -44,7 +44,7 @@ class LoongArchCCState : public CCState {
                          const char *Func);
 
   /// Identify lowered values that originated from f128 arguments and record
-  /// this for use by RetCC_LoongArchLP64LPX32.
+  /// this for use by RetCC_LoongArchLP64.
   void
   PreAnalyzeFormalArgumentsForF128(const SmallVectorImpl<ISD::InputArg> &Ins);
 
diff --git a/llvm/lib/Target/LoongArch/LoongArchCallingConv.td b/llvm/lib/Target/LoongArch/LoongArchCallingConv.td
index 13170f3f0ac33..f5e902a87acec 100644
--- a/llvm/lib/Target/LoongArch/LoongArchCallingConv.td
+++ b/llvm/lib/Target/LoongArch/LoongArchCallingConv.td
@@ -75,10 +75,10 @@ def RetCC_F128 : CallingConv<[
 ]>;
 
 //===----------------------------------------------------------------------===//
-// LoongArch LP32 Calling Convention
+// LoongArch ILP32 Calling Convention
 //===----------------------------------------------------------------------===//
 
-def CC_LoongArchLP32 : CallingConv<[
+def CC_LoongArchILP32 : CallingConv<[
   // Promote i8/i16 arguments to i32.
   CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
 
@@ -91,51 +91,50 @@ def CC_LoongArchLP32 : CallingConv<[
   CCIfType<[f64], CCAssignToStack<8, 8>>
 ]>;
 
-// Only the return rules are defined here for LP32. The rules for argument
+// Only the return rules are defined here for 32-bit ABI. The rules for argument
 // passing are defined in LoongArchISelLowering.cpp.
-def RetCC_LoongArchLP32 : CallingConv<[
+def RetCC_LoongArchILP32 : CallingConv<[
   // Promote i1/i8/i16 return values to i32.
   CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
 
-  // i32 are returned in registers V0, V1, A0, A1, unless the original return
+  // i32 are returned in registers A0, A1, unless the original return
   // type was a vector of floats.
   CCIfOrigArgWasNotVectorFloat<CCIfType<[i32],
                                         CCAssignToReg<[A0, A1]>>>,
 
-  // f32 are returned in registers F0, F2
+  // f32 are returned in registers F0, F1
   CCIfType<[f32], CCAssignToReg<[F0, F1]>>,
 
-  // f64 arguments are returned in F0_64 and F2_64 in FP64bit mode or
-  // in F0 and F1 in FP32bit mode.
-  CCIfType<[f64], CCIfSubtarget<"isFP64bit()", CCAssignToReg<[F0_64, F1_64]>>>
+  // f64 arguments are returned in F0_64 and F1_64 in hasBasicD mode.
+  CCIfType<[f64], CCIfSubtarget<"hasBasicD()", CCAssignToReg<[F0_64, F1_64]>>>
 ]>;
 
-def CC_LoongArchLP32_FP32 : CustomCallingConv;
-def CC_LoongArchLP32_FP64 : CustomCallingConv;
+def CC_LoongArchILP32_FP32 : CustomCallingConv;
+def CC_LoongArchILP32_FP64 : CustomCallingConv;
 def CC_LoongArch_F128 : CustomCallingConv;
 
-def CC_LoongArchLP32_FP : CallingConv<[
-  CCIfSubtargetNot<"isFP64bit()", CCDelegateTo<CC_LoongArchLP32_FP32>>,
-  CCIfSubtarget<"isFP64bit()", CCDelegateTo<CC_LoongArchLP32_FP64>>
+def CC_LoongArchILP32_FP : CallingConv<[
+  CCIfSubtargetNot<"hasBasicD()", CCDelegateTo<CC_LoongArchILP32_FP32>>,
+  CCIfSubtarget<"hasBasicD()", CCDelegateTo<CC_LoongArchILP32_FP64>>
 ]>;
 
 //===----------------------------------------------------------------------===//
-// LoongArch LPX32/LP64 Calling Convention
+// LoongArch LP64 Calling Convention
 //===----------------------------------------------------------------------===//
 
-def CC_LoongArchLP64LPX32_SoftFloat : CallingConv<[
+def CC_LoongArchLP64_SoftFloat : CallingConv<[
   CCAssignToReg<[A0, A1, A2, A3,
                  A4, A5, A6, A7]>,
   CCAssignToStack<4, 8>
 ]>;
 
-def CC_LoongArchLP64LPX32 : CallingConv<[
+def CC_LoongArchLP64 : CallingConv<[
 
   // All integers (except soft-float integers) are promoted to 64-bit.
   CCIfType<[i8, i16, i32], CCIfOrigArgWasNotFloat<CCPromoteToType<i64>>>,
 
   // The only i32's we have left are soft-float arguments.
-  CCIfSubtarget<"useSoftFloat()", CCIfType<[i32], CCDelegateTo<CC_LoongArchLP64LPX32_SoftFloat>>>,
+  CCIfSubtarget<"useSoftFloat()", CCIfType<[i32], CCDelegateTo<CC_LoongArchLP64_SoftFloat>>>,
 
   // Integer arguments are passed in integer registers.
   //CCIfType<[i64], CCAssignToRegWithShadow<[A0_64, A1_64, A2_64, A3_64,
@@ -169,9 +168,9 @@ def CC_LoongArchLP64LPX32 : CallingConv<[
          CCAssignToStack<32, 32>>
 ]>;
 
-// LPX32/LP64 variable arguments.
+// LP64 variable arguments.
 // All arguments are passed in integer registers.
-def CC_LoongArchLP64LPX32_VarArg : CallingConv<[
+def CC_LoongArchLP64_VarArg : CallingConv<[
   // All integers are promoted to 64-bit.
   CCIfType<[i8, i16, i32], CCPromoteToType<i64>>,
 
@@ -187,7 +186,7 @@ def CC_LoongArchLP64LPX32_VarArg : CallingConv<[
   CCIfType<[i64, f64], CCAssignToStack<8, 8>>
 ]>;
 
-def RetCC_LoongArchLP64LPX32 : CallingConv<[
+def RetCC_LoongArchLP64 : CallingConv<[
   // f128 needs to be handled similarly to f32 and f64. However, f128 is not
   // legal and is lowered to i128 which is further lowered to a pair of i64's.
   // This presents us with a problem for the calling convention since hard-float
@@ -196,8 +195,7 @@ def RetCC_LoongArchLP64LPX32 : CallingConv<[
   // pre-analyze (see PreAnalyzeReturnForF128()) step to pass information on
   // whether the result was originally an f128 into the tablegen-erated code.
   //
-  // f128 should only occur for the LP64D ABI where long double is 128-bit. On
-  // LPX32, long double is equivalent to double.
+  // f128 should only occur for the 64-bit ABI where long double is 128-bit.
   CCIfType<[i64], CCIfOrigArgWasF128<CCDelegateTo<RetCC_F128>>>,
 
   CCIfType<[i8, i16, i32, i64], CCIfInReg<CCPromoteToType<i64>>>,
@@ -215,9 +213,8 @@ def RetCC_LoongArchLP64LPX32 : CallingConv<[
 //===----------------------------------------------------------------------===//
 // LoongArch FastCC Calling Convention
 //===----------------------------------------------------------------------===//
-//LP32 has been removed because of not support
 
-def CC_LoongArchLP64LPX32_FastCC : CallingConv<[
+def CC_LoongArchLP64_FastCC : CallingConv<[
   // Integer arguments are passed in integer registers.
   CCIfType<[i64], CCAssignToReg<[A0_64, A1_64, A2_64, A3_64, T0_64, T1_64,
                                  T2_64, T3_64, T4_64, T5_64, T6_64, T7_64,
@@ -248,8 +245,7 @@ def CC_LoongArch_FastCC : CallingConv<[
   // Stack parameter slots for i32 and f32 are 32-bit words and 4-byte aligned.
   CCIfType<[i32, f32], CCAssignToStack<4, 4>>,
 
-//  CCIfSubtarget<"isABI_LP32()", CCDelegateTo<CC_LoongArchLP32_FastCC>>,
-  CCDelegateTo<CC_LoongArchLP64LPX32_FastCC>
+  CCDelegateTo<CC_LoongArchLP64_FastCC>
 ]>;
 
 //===----------------------------------------------------------------------===//
@@ -257,13 +253,12 @@ def CC_LoongArch_FastCC : CallingConv<[
 //===----------------------------------------------------------------------===//
 
 def RetCC_LoongArch : CallingConv<[
-  CCIfSubtarget<"isABI_LPX32()", CCDelegateTo<RetCC_LoongArchLP64LPX32>>,
-  CCIfSubtarget<"isABI_LP64D()", CCDelegateTo<RetCC_LoongArchLP64LPX32>>,
-  CCDelegateTo<RetCC_LoongArchLP32>
+  CCIfSubtarget<"isABI_LP64()", CCDelegateTo<RetCC_LoongArchLP64>>,
+  CCDelegateTo<RetCC_LoongArchILP32>
 ]>;
 
 def CC_LoongArch_ByVal : CallingConv<[
-  CCIfSubtarget<"isABI_LP32()", CCIfByVal<CCPassByVal<4, 4>>>,
+  CCIfSubtarget<"isABI_ILP32()", CCIfByVal<CCPassByVal<4, 4>>>,
   CCIfByVal<CCPassByVal<8, 8>>
 ]>;
 
@@ -280,23 +275,22 @@ def CC_LoongArch_FixedArg : CallingConv<[
   // pre-analyze (see PreAnalyzeFormalArgsForF128()) step to pass information on
   // whether the argument was originally an f128 into the tablegen-erated code.
   //
-  // f128 should only occur for the LP64D ABI where long double is 128-bit. On
-  // LPX32, long double is equivalent to double.
+  // f128 should only occur for the 64-bit ABI where long double is 128-bit.
   CCIfType<[i64],
       CCIfSubtargetNot<"useSoftFloat()",
           CCIfOrigArgWasF128<CCBitConvertToType<i64>>>>,
 
   CCIfCC<"CallingConv::Fast", CCDelegateTo<CC_LoongArch_FastCC>>,
 
-  CCIfSubtarget<"isABI_LP32()", CCDelegateTo<CC_LoongArchLP32_FP>>,
-  CCDelegateTo<CC_LoongArchLP64LPX32>
+  CCIfSubtarget<"isABI_ILP32()", CCDelegateTo<CC_LoongArchILP32_FP>>,
+  CCDelegateTo<CC_LoongArchLP64>
 ]>;
 
 def CC_LoongArch_VarArg : CallingConv<[
   CCIfByVal<CCDelegateTo<CC_LoongArch_ByVal>>,
 
-  CCIfSubtarget<"isABI_LP32()", CCDelegateTo<CC_LoongArchLP32_FP>>,
-  CCDelegateTo<CC_LoongArchLP64LPX32_VarArg>
+  CCIfSubtarget<"isABI_ILP32()", CCDelegateTo<CC_LoongArchILP32_FP>>,
+  CCDelegateTo<CC_LoongArchLP64_VarArg>
 ]>;
 
 def CC_LoongArch : CallingConv<[
@@ -311,22 +305,8 @@ def CC_LoongArch : CallingConv<[
 def CSR_SingleFloatOnly : CalleeSavedRegs<(add (sequence "F%u", 31, 24), RA, FP,
                                                (sequence "S%u", 8, 0))>;
 
-//def CSR_LP32_FPXX : CalleeSavedRegs<(add (sequence "D%u", 15, 10), RA, FP,
-//                                        (sequence "S%u", 8, 0))> {
-//  let OtherPreserved = (add (decimate (sequence "F%u", 30, 20), 2));
-//}
-
-def CSR_LP32 : CalleeSavedRegs<(add (sequence "F%u_64", 31, 24), RA, FP,
+def CSR_ILP32 : CalleeSavedRegs<(add (sequence "F%u_64", 31, 24), RA, FP,
                                    (sequence "S%u", 8, 0))>;
 
-//def CSR_LP32_FP64 :
-//  CalleeSavedRegs<(add (decimate (sequence "D%u_64", 30, 20), 2), RA, FP,
-//                       (sequence "S%u", 8, 0))>;
-
-def CSR_LPX32 : CalleeSavedRegs<(add F20_64, F22_64, F24_64, F26_64, F28_64,
-                                   F30_64, RA_64, FP_64,
-                                   (sequence "S%u_64", 8, 0))>;
-
-//def CSR_LP64 : CalleeSavedRegs<(add (sequence "D%u_64", 31, 24), RA_64, SP_64, FP_64,
 def CSR_LP64 : CalleeSavedRegs<(add (sequence "F%u_64", 31, 24), RA_64, FP_64,
                                    (sequence "S%u_64", 8, 0))>;
diff --git a/llvm/lib/Target/LoongArch/LoongArchExpandPseudo.cpp b/llvm/lib/Target/LoongArch/LoongArchExpandPseudo.cpp
index fc1b501965c34..d0f656ac4d865 100644
--- a/llvm/lib/Target/LoongArch/LoongArchExpandPseudo.cpp
+++ b/llvm/lib/Target/LoongArch/LoongArchExpandPseudo.cpp
@@ -73,6 +73,9 @@ namespace {
     bool expandPseudoCall(MachineBasicBlock &BB,
                           MachineBasicBlock::iterator I,
                           MachineBasicBlock::iterator &NMBBI);
+    bool expandPseudoTailCall(MachineBasicBlock &BB,
+                              MachineBasicBlock::iterator I);
+
     bool expandPseudoTEQ(MachineBasicBlock &BB,
                          MachineBasicBlock::iterator I,
                          MachineBasicBlock::iterator &NMBBI);
@@ -328,6 +331,9 @@ bool LoongArchExpandPseudo::expandAtomicBinOpSubword(
 
   bool IsSwap = false;
   bool IsNand = false;
+  bool IsMAX = false;
+  bool IsMIN = false;
+  bool IsUnsigned = false;
 
   unsigned Opcode = 0;
   switch (I->getOpcode()) {
@@ -354,24 +360,30 @@ bool LoongArchExpandPseudo::expandAtomicBinOpSubword(
     LLVM_FALLTHROUGH;
   case LoongArch::ATOMIC_LOAD_MAX_I16_POSTRA:
     Opcode = LoongArch::AMMAX_DB_W;
+    IsMAX = true;
     break;
   case LoongArch::ATOMIC_LOAD_MIN_I8_POSTRA:
     SEOp = LoongArch::EXT_W_B32;
     LLVM_FALLTHROUGH;
   case LoongArch::ATOMIC_LOAD_MIN_I16_POSTRA:
     Opcode = LoongArch::AMMIN_DB_W;
+    IsMIN = true;
     break;
   case LoongArch::ATOMIC_LOAD_UMAX_I8_POSTRA:
     SEOp = LoongArch::EXT_W_B32;
     LLVM_FALLTHROUGH;
   case LoongArch::ATOMIC_LOAD_UMAX_I16_POSTRA:
     Opcode = LoongArch::AMMAX_DB_WU;
+    IsMAX = true;
+    IsUnsigned = true;
     break;
   case LoongArch::ATOMIC_LOAD_UMIN_I8_POSTRA:
     SEOp = LoongArch::EXT_W_B32;
     LLVM_FALLTHROUGH;
   case LoongArch::ATOMIC_LOAD_UMIN_I16_POSTRA:
     Opcode = LoongArch::AMMIN_DB_WU;
+    IsMIN = true;
+    IsUnsigned = true;
     break;
   case LoongArch::ATOMIC_LOAD_SUB_I8_POSTRA:
     SEOp = LoongArch::EXT_W_B32;
@@ -442,6 +454,34 @@ bool LoongArchExpandPseudo::expandAtomicBinOpSubword(
     BuildMI(loopMBB, DL, TII->get(LoongArch::AND32), BinOpRes)
         .addReg(BinOpRes)
         .addReg(Mask);
+  } else if (IsMAX || IsMIN) {
+
+    unsigned SLTScratch4 = IsUnsigned ? LoongArch::SLTU32 : LoongArch::SLT32;
+    unsigned CMPIncr = IsMAX ? LoongArch::MASKEQZ32 : LoongArch::MASKNEZ32;
+    unsigned CMPOldVal = IsMAX ? LoongArch::MASKNEZ32 : LoongArch::MASKEQZ32;
+
+    unsigned Scratch4 = I->getOperand(9).getReg();
+    unsigned Scratch5 = I->getOperand(10).getReg();
+
+    BuildMI(loopMBB, DL, TII->get(LoongArch::AND32), Scratch5)
+        .addReg(OldVal)
+        .addReg(Mask);
+    BuildMI(loopMBB, DL, TII->get(LoongArch::AND32), Incr)
+        .addReg(Incr)
+        .addReg(Mask);
+    BuildMI(loopMBB, DL, TII->get(SLTScratch4), Scratch4)
+        .addReg(Scratch5)
+        .addReg(Incr);
+    BuildMI(loopMBB, DL, TII->get(CMPOldVal), BinOpRes)
+        .addReg(Scratch5)
+        .addReg(Scratch4);
+    BuildMI(loopMBB, DL, TII->get(CMPIncr), Scratch4)
+        .addReg(Incr)
+        .addReg(Scratch4);
+    BuildMI(loopMBB, DL, TII->get(LoongArch::OR32), BinOpRes)
+        .addReg(BinOpRes)
+        .addReg(Scratch4);
+
   } else if (!IsSwap) {
     //  <binop> binopres, oldval, incr2
     //  and newval, binopres, mask
@@ -463,13 +503,19 @@ bool LoongArchExpandPseudo::expandAtomicBinOpSubword(
   // StoreVal<tied1> = sc StoreVal, 0(Ptr)
   // beq StoreVal, zero, loopMBB
   BuildMI(loopMBB, DL, TII->get(LoongArch::AND32), StoreVal)
-    .addReg(OldVal).addReg(Mask2);
+      .addReg(OldVal)
+      .addReg(Mask2);
   BuildMI(loopMBB, DL, TII->get(LoongArch::OR32), StoreVal)
-    .addReg(StoreVal).addReg(BinOpRes);
+      .addReg(StoreVal)
+      .addReg(BinOpRes);
   BuildMI(loopMBB, DL, TII->get(SC), StoreVal)
-    .addReg(StoreVal).addReg(Ptr).addImm(0);
+      .addReg(StoreVal)
+      .addReg(Ptr)
+      .addImm(0);
   BuildMI(loopMBB, DL, TII->get(BEQ))
-    .addReg(StoreVal).addReg(LoongArch::ZERO).addMBB(loopMBB);
+      .addReg(StoreVal)
+      .addReg(LoongArch::ZERO)
+      .addMBB(loopMBB);
 
   //  sinkMBB:
   //    and     maskedoldval1,oldval,mask
@@ -479,9 +525,11 @@ bool LoongArchExpandPseudo::expandAtomicBinOpSubword(
   sinkMBB->addSuccessor(exitMBB, BranchProbability::getOne());
 
   BuildMI(sinkMBB, DL, TII->get(LoongArch::AND32), Dest)
-    .addReg(OldVal).addReg(Mask);
+      .addReg(OldVal)
+      .addReg(Mask);
   BuildMI(sinkMBB, DL, TII->get(LoongArch::SRL_W), Dest)
-      .addReg(Dest).addReg(ShiftAmnt);
+      .addReg(Dest)
+      .addReg(ShiftAmnt);
 
   BuildMI(sinkMBB, DL, TII->get(SEOp), Dest).addReg(Dest);
 
@@ -866,6 +914,31 @@ bool LoongArchExpandPseudo::expandLoadAddr(MachineBasicBlock &BB,
   return true;
 }
 
+bool LoongArchExpandPseudo::expandPseudoTailCall(
+    MachineBasicBlock &BB, MachineBasicBlock::iterator I) {
+
+  MachineFunction *MF = BB.getParent();
+  MachineInstr &MI = *I;
+  DebugLoc DL = MI.getDebugLoc();
+
+  const MachineOperand &MO = MI.getOperand(0);
+
+  unsigned NoFlag = LoongArchII::MO_NO_FLAG;
+
+  MachineInstrBuilder MIB =
+      BuildMI(BB, I, DL, TII->get(LoongArch::PseudoTailReturn));
+
+  if (MO.isSymbol()) {
+    MIB.addExternalSymbol(MO.getSymbolName(), NoFlag);
+  } else {
+    MIB.addDisp(MO, 0, NoFlag);
+  }
+
+  MI.eraseFromParent();
+
+  return true;
+}
+
 bool LoongArchExpandPseudo::expandPseudoCall(MachineBasicBlock &BB,
                                            MachineBasicBlock::iterator I,
                                            MachineBasicBlock::iterator &NMBBI) {
@@ -976,6 +1049,8 @@ bool LoongArchExpandPseudo::expandMI(MachineBasicBlock &MBB,
     return expandPseudoTEQ(MBB, MBBI, NMBB);
   case LoongArch::PseudoCall:
     return expandPseudoCall(MBB, MBBI, NMBB);
+  case LoongArch::PseudoTailCall:
+    return expandPseudoTailCall(MBB, MBBI);
   case LoongArch::LoadAddrLocal:
   case LoongArch::LoadAddrLocalRR:
   case LoongArch::LoadAddrGlobal:
diff --git a/llvm/lib/Target/LoongArch/LoongArchISelLowering.cpp b/llvm/lib/Target/LoongArch/LoongArchISelLowering.cpp
index 11e5817d47083..fb3c578ff99cf 100644
--- a/llvm/lib/Target/LoongArch/LoongArchISelLowering.cpp
+++ b/llvm/lib/Target/LoongArch/LoongArchISelLowering.cpp
@@ -95,10 +95,6 @@ NoZeroDivCheck("mnocheck-zero-division", cl::Hidden,
                cl::desc("LoongArch: Don't trap on integer division by zero."),
                cl::init(false));
 
-static cl::opt<bool>
-UseLoongArchTailCalls("loongarch-tail-calls", cl::Hidden,
-                      cl::desc("LoongArch: permit tail calls."), cl::init(false));
-
 static const MCPhysReg LoongArch64DPRegs[8] = {
   LoongArch::F0_64, LoongArch::F1_64, LoongArch::F2_64, LoongArch::F3_64,
   LoongArch::F4_64, LoongArch::F5_64, LoongArch::F6_64, LoongArch::F7_64
@@ -227,10 +223,6 @@ LoongArchTargetLowering::LoongArchTargetLowering(const LoongArchTargetMachine &T
   setOperationAction(ISD::SETCC,              MVT::f32,   Custom);
   setOperationAction(ISD::SETCC,              MVT::f64,   Custom);
   setOperationAction(ISD::BRCOND,             MVT::Other, Custom);
-  // fcopysign does not use 'custom' in the instruction legalization phase
-  // when using llvm's Intrinsic-'copysign'.
-  //setOperationAction(ISD::FCOPYSIGN,          MVT::f32,   Custom);
-  //setOperationAction(ISD::FCOPYSIGN,          MVT::f64,   Custom);
   setOperationAction(ISD::FP_TO_SINT,         MVT::i32,   Custom);
 
   if (Subtarget.is64Bit()) {
@@ -351,22 +343,16 @@ LoongArchTargetLowering::LoongArchTargetLowering(const LoongArchTargetMachine &T
   setTargetDAGCombine(ISD::AssertZext);
   setTargetDAGCombine(ISD::SHL);
 
-  if (ABI.IsLP32()) {
-    // These libcalls are not available in 32-bit.
-    setLibcallName(RTLIB::SHL_I128, nullptr);
-    setLibcallName(RTLIB::SRL_I128, nullptr);
-    setLibcallName(RTLIB::SRA_I128, nullptr);
+  if (ABI.IsILP32D() || ABI.IsILP32F() || ABI.IsILP32S()) {
+    // TODO
+    llvm_unreachable("Unimplemented ABI");
   }
 
-  if (!Subtarget.useSoftFloat()) {
+  if (Subtarget.hasBasicF())
     addRegisterClass(MVT::f32, &LoongArch::FGR32RegClass);
 
-    // When dealing with single precision only, use libcalls
-    if (!Subtarget.isSingleFloat()) {
-      if (Subtarget.isFP64bit())
-        addRegisterClass(MVT::f64, &LoongArch::FGR64RegClass);
-    }
-  }
+  if (Subtarget.hasBasicD())
+    addRegisterClass(MVT::f64, &LoongArch::FGR64RegClass);
 
   setOperationAction(ISD::SMUL_LOHI,          MVT::i32, Custom);
   setOperationAction(ISD::UMUL_LOHI,          MVT::i32, Custom);
@@ -434,12 +420,12 @@ LoongArchTargetLowering::LoongArchTargetLowering(const LoongArchTargetMachine &T
 
   setMinFunctionAlignment(Subtarget.is64Bit() ? Align(8) : Align(4));
 
-  // The arguments on the stack are defined in terms of 4-byte slots on LP32
-  // and 8-byte slots on LPX32/LP64D.
-  setMinStackArgumentAlignment((ABI.IsLPX32() || ABI.IsLP64D()) ? Align(8)
-                                                               : Align(4));
+  // The arguments on the stack are defined in terms of 4-byte slots on 32bit
+  // target and 8-byte slots on 64bit target.
+  setMinStackArgumentAlignment(Subtarget.is64Bit() ? Align(8) : Align(4));
 
-  setStackPointerRegisterToSaveRestore(ABI.IsLP64D() ? LoongArch::SP_64 : LoongArch::SP);
+  setStackPointerRegisterToSaveRestore(Subtarget.is64Bit() ? LoongArch::SP_64
+                                                           : LoongArch::SP);
 
   MaxStoresPerMemcpy = 16;
 
@@ -800,7 +786,7 @@ shouldTransformMulToShiftsAddsSubs(APInt C, EVT VT,
   //   That allows to remove a workaround for types not supported natively.
   // - Take in account `-Os, -Oz` flags because this optimization
   //   increases code size.
-  unsigned MaxSteps = Subtarget.isABI_LP32() ? 8 : 17;
+  unsigned MaxSteps = Subtarget.is64Bit() ? 17 : 8;
 
   SmallVector<APInt, 16> WorkStack(1, C);
   unsigned Steps = 0;
@@ -1006,7 +992,6 @@ LowerOperation(SDValue Op, SelectionDAG &DAG) const
   case ISD::SETCC:              return lowerSETCC(Op, DAG);
   case ISD::VASTART:            return lowerVASTART(Op, DAG);
   case ISD::VAARG:              return lowerVAARG(Op, DAG);
-  case ISD::FCOPYSIGN:          return lowerFCOPYSIGN(Op, DAG);
   case ISD::FRAMEADDR:          return lowerFRAMEADDR(Op, DAG);
   case ISD::RETURNADDR:         return lowerRETURNADDR(Op, DAG);
   case ISD::EH_RETURN:          return lowerEH_RETURN(Op, DAG);
@@ -1447,6 +1432,8 @@ MachineBasicBlock *LoongArchTargetLowering::emitAtomicBinaryPartword(
   unsigned Scratch = RegInfo.createVirtualRegister(RC);
   unsigned Scratch2 = RegInfo.createVirtualRegister(RC);
   unsigned Scratch3 = RegInfo.createVirtualRegister(RC);
+  unsigned Scratch4 = RegInfo.createVirtualRegister(RC);
+  unsigned Scratch5 = RegInfo.createVirtualRegister(RC);
 
   unsigned AtomicOp = 0;
   switch (MI.getOpcode()) {
@@ -1585,6 +1572,10 @@ MachineBasicBlock *LoongArchTargetLowering::emitAtomicBinaryPartword(
       .addReg(Scratch2, RegState::EarlyClobber | RegState::Define |
                             RegState::Dead | RegState::Implicit)
       .addReg(Scratch3, RegState::EarlyClobber | RegState::Define |
+                            RegState::Dead | RegState::Implicit)
+      .addReg(Scratch4, RegState::EarlyClobber | RegState::Define |
+                            RegState::Dead | RegState::Implicit)
+      .addReg(Scratch5, RegState::EarlyClobber | RegState::Define |
                             RegState::Dead | RegState::Implicit);
 
   BuildMI(BB, DL, TII->get(LoongArch::DBAR)).addImm(0);
@@ -1971,19 +1962,20 @@ SDValue LoongArchTargetLowering::lowerVAARG(SDValue Op, SelectionDAG &DAG) const
       llvm::MaybeAlign(Node->getConstantOperandVal(3)).valueOrOne();
   const Value *SV = cast<SrcValueSDNode>(Node->getOperand(2))->getValue();
   SDLoc DL(Node);
-  unsigned ArgSlotSizeInBytes = (ABI.IsLPX32() || ABI.IsLP64D()) ? 8 : 4;
+  unsigned ArgSlotSizeInBytes = Subtarget.is64Bit() ? 8 : 4;
 
   SDValue VAListLoad = DAG.getLoad(getPointerTy(DAG.getDataLayout()), DL, Chain,
                                    VAListPtr, MachinePointerInfo(SV));
   SDValue VAList = VAListLoad;
 
   // Re-align the pointer if necessary.
-  // It should only ever be necessary for 64-bit types on LP32 since the minimum
-  // argument alignment is the same as the maximum type alignment for LPX32/LP64D.
+  // It should only ever be necessary for 64-bit types on ILP32D/ILP32F/ILP32S
+  // since the minimum argument alignment is the same as the maximum type
+  // alignment for LP64D/LP64S/LP64F.
   //
   // FIXME: We currently align too often. The code generator doesn't notice
   //        when the pointer is still aligned from the last va_arg (or pair of
-  //        va_args for the i64 on LP32 case).
+  //        va_args for the i64 on ILP32D/ILP32F/ILP32S case).
   if (Align > getMinStackArgumentAlignment()) {
     VAList = DAG.getNode(
         ISD::ADD, DL, VAList.getValueType(), VAList,
@@ -2010,45 +2002,6 @@ SDValue LoongArchTargetLowering::lowerVAARG(SDValue Op, SelectionDAG &DAG) const
   return DAG.getLoad(VT, DL, Chain, VAList, MachinePointerInfo());
 }
 
-static SDValue lowerFCOPYSIGLPX32(SDValue Op, SelectionDAG &DAG) {
-  // TODO:
-  return SDValue();
-}
-
-static SDValue lowerFCOPYSIGLP64(SDValue Op, SelectionDAG &DAG) {
-  unsigned WidthX = Op.getOperand(0).getValueSizeInBits();
-  unsigned WidthY = Op.getOperand(1).getValueSizeInBits();
-  EVT TyX = MVT::getIntegerVT(WidthX), TyY = MVT::getIntegerVT(WidthY);
-  SDLoc DL(Op);
-
-  // Bitcast to integer nodes.
-  SDValue X = DAG.getNode(ISD::BITCAST, DL, TyX, Op.getOperand(0));
-  SDValue Y = DAG.getNode(ISD::BITCAST, DL, TyY, Op.getOperand(1));
-
-  // bstrpick  E, Y, width(Y) - 1, width(Y) - 1  ; extract bit width(Y)-1 of Y
-  // bstrins  X, E, width(X) - 1, width(X) - 1  ; insert extracted bit at bit width(X)-1 of X
-  SDValue E = DAG.getNode(LoongArchISD::BSTRPICK, DL, TyY, Y,
-                          DAG.getConstant(WidthY - 1, DL, MVT::i32), DAG.getConstant(WidthY - 1, DL, MVT::i32));
-
-  if (WidthX > WidthY)
-    E = DAG.getNode(ISD::ZERO_EXTEND, DL, TyX, E);
-  else if (WidthY > WidthX)
-    E = DAG.getNode(ISD::TRUNCATE, DL, TyX, E);
-
-  SDValue I = DAG.getNode(LoongArchISD::BSTRINS, DL, TyX, E,
-                          DAG.getConstant(WidthX - 1, DL, MVT::i32), DAG.getConstant(WidthX - 1, DL, MVT::i32),
-                          X);
-  return DAG.getNode(ISD::BITCAST, DL, Op.getOperand(0).getValueType(), I);
-}
-
-SDValue
-LoongArchTargetLowering::lowerFCOPYSIGN(SDValue Op, SelectionDAG &DAG) const {
-  if (Subtarget.is64Bit())
-    return lowerFCOPYSIGLP64(Op, DAG);
-
-  return lowerFCOPYSIGLPX32(Op, DAG);
-}
-
 SDValue LoongArchTargetLowering::
 lowerFRAMEADDR(SDValue Op, SelectionDAG &DAG) const {
   // check the depth
@@ -2060,7 +2013,8 @@ lowerFRAMEADDR(SDValue Op, SelectionDAG &DAG) const {
   EVT VT = Op.getValueType();
   SDLoc DL(Op);
   SDValue FrameAddr = DAG.getCopyFromReg(
-      DAG.getEntryNode(), DL, ABI.IsLP64D() ? LoongArch::FP_64 : LoongArch::FP, VT);
+      DAG.getEntryNode(), DL,
+      Subtarget.is64Bit() ? LoongArch::FP_64 : LoongArch::FP, VT);
   return FrameAddr;
 }
 
@@ -2076,7 +2030,7 @@ SDValue LoongArchTargetLowering::lowerRETURNADDR(SDValue Op,
   MachineFunction &MF = DAG.getMachineFunction();
   MachineFrameInfo &MFI = MF.getFrameInfo();
   MVT VT = Op.getSimpleValueType();
-  unsigned RA = ABI.IsLP64D() ? LoongArch::RA_64 : LoongArch::RA;
+  unsigned RA = Subtarget.is64Bit() ? LoongArch::RA_64 : LoongArch::RA;
   MFI.setReturnAddressIsTaken(true);
 
   // Return RA, which contains the return address. Mark it an implicit live-in.
@@ -2098,12 +2052,12 @@ SDValue LoongArchTargetLowering::lowerEH_RETURN(SDValue Op, SelectionDAG &DAG)
   SDValue Offset    = Op.getOperand(1);
   SDValue Handler   = Op.getOperand(2);
   SDLoc DL(Op);
-  EVT Ty = ABI.IsLP64D() ? MVT::i64 : MVT::i32;
+  EVT Ty = Subtarget.is64Bit() ? MVT::i64 : MVT::i32;
 
   // Store stack offset in A1, store jump target in A0. Glue CopyToReg and
   // EH_RETURN nodes, so that instructions are emitted back-to-back.
-  unsigned OffsetReg = ABI.IsLP64D() ? LoongArch::A1_64 : LoongArch::A1;
-  unsigned AddrReg = ABI.IsLP64D() ? LoongArch::A0_64 : LoongArch::A0;
+  unsigned OffsetReg = Subtarget.is64Bit() ? LoongArch::A1_64 : LoongArch::A1;
+  unsigned AddrReg = Subtarget.is64Bit() ? LoongArch::A0_64 : LoongArch::A0;
   Chain = DAG.getCopyToReg(Chain, DL, OffsetReg, Offset, SDValue());
   Chain = DAG.getCopyToReg(Chain, DL, AddrReg, Handler, Chain.getValue(1));
   return DAG.getNode(LoongArchISD::EH_RETURN, DL, MVT::Other, Chain,
@@ -2213,7 +2167,8 @@ static SDValue lowerFP_TO_SINT_STORE(StoreSDNode *SD, SelectionDAG &DAG,
 
 SDValue LoongArchTargetLowering::lowerSTORE(SDValue Op, SelectionDAG &DAG) const {
   StoreSDNode *SD = cast<StoreSDNode>(Op);
-  return lowerFP_TO_SINT_STORE(SD, DAG, Subtarget.isSingleFloat());
+  return lowerFP_TO_SINT_STORE(
+      SD, DAG, (Subtarget.hasBasicF() && !Subtarget.hasBasicD()));
 }
 
 SDValue LoongArchTargetLowering::lowerINTRINSIC_WO_CHAIN(SDValue Op,
@@ -2269,7 +2224,8 @@ SDValue LoongArchTargetLowering::lowerFP_TO_UINT(SDValue Op, SelectionDAG &DAG)
 }
 
 SDValue LoongArchTargetLowering::lowerFP_TO_SINT(SDValue Op, SelectionDAG &DAG) const {
-  if (Op.getValueSizeInBits() > 32 && Subtarget.isSingleFloat())
+  if (Op.getValueSizeInBits() > 32 &&
+      (Subtarget.hasBasicF() && !Subtarget.hasBasicD()))
     return SDValue();
 
   EVT FPTy = EVT::getFloatingPointVT(Op.getValueSizeInBits());
@@ -2289,12 +2245,58 @@ SDValue LoongArchTargetLowering::lowerEH_DWARF_CFA(SDValue Op,
   return DAG.getFrameIndex(FI, ValTy);
 }
 
+// Check whether the tail call optimization conditions are met
 bool LoongArchTargetLowering::isEligibleForTailCallOptimization(
-    const CCState &CCInfo, unsigned NextStackOffset,
-    const LoongArchFunctionInfo &FI) const {
-  if (!UseLoongArchTailCalls)
+    const CCState &CCInfo, CallLoweringInfo &CLI, MachineFunction &MF,
+    unsigned NextStackOffset, const LoongArchFunctionInfo &FI) const {
+
+  auto &Callee = CLI.Callee;
+  auto CalleeCC = CLI.CallConv;
+  auto IsVarArg = CLI.IsVarArg;
+  auto &Outs = CLI.Outs;
+  auto &Caller = MF.getFunction();
+  auto CallerCC = Caller.getCallingConv();
+
+  if (Caller.getFnAttribute("disable-tail-calls").getValueAsString() == "true")
+    return false;
+
+  if (Caller.hasFnAttribute("interrupt"))
+    return false;
+
+  if (IsVarArg)
     return false;
 
+  if (getTargetMachine().getCodeModel() == CodeModel::Large)
+    return false;
+
+  if (getTargetMachine().getRelocationModel() == Reloc::Static)
+    return false;
+
+  // Do not tail call optimize if the stack is used to pass parameters.
+  if (CCInfo.getNextStackOffset() != 0)
+    return false;
+
+  // Do not tail call optimize functions with byval parameters.
+  for (auto &Arg : Outs)
+    if (Arg.Flags.isByVal())
+      return false;
+
+  // Do not tail call optimize if either caller or callee uses structret
+  // semantics.
+  auto IsCallerStructRet = Caller.hasStructRetAttr();
+  auto IsCalleeStructRet = Outs.empty() ? false : Outs[0].Flags.isSRet();
+  if (IsCallerStructRet || IsCalleeStructRet)
+    return false;
+
+  // The callee has to preserve all registers the caller needs to preserve.
+  const LoongArchRegisterInfo *TRI = Subtarget.getRegisterInfo();
+  const uint32_t *CallerPreserved = TRI->getCallPreservedMask(MF, CallerCC);
+  if (CalleeCC != CallerCC) {
+    const uint32_t *CalleePreserved = TRI->getCallPreservedMask(MF, CalleeCC);
+    if (!TRI->regmaskSubsetEqual(CallerPreserved, CalleePreserved))
+      return false;
+  }
+
   // Return false if either the callee or caller has a byval argument.
   if (CCInfo.getInRegsParamsCount() > 0 || FI.hasByvalArg())
     return false;
@@ -2310,7 +2312,7 @@ bool LoongArchTargetLowering::isEligibleForTailCallOptimization(
 
 //===----------------------------------------------------------------------===//
 // TODO: Implement a generic logic using tblgen that can support this.
-// LoongArch LP32 ABI rules:
+// LoongArch 32-bit ABI rules:
 // ---
 // i32 - Passed in A0, A1, A2, A3 and stack
 // f32 - Only passed in f32 registers if no int reg has been used yet to hold
@@ -2328,7 +2330,7 @@ bool LoongArchTargetLowering::isEligibleForTailCallOptimization(
 //  For vararg functions, all arguments are passed in A0, A1, A2, A3 and stack.
 //===----------------------------------------------------------------------===//
 
-static bool CC_LoongArchLP32(unsigned ValNo, MVT ValVT, MVT LocVT,
+static bool CC_LoongArchILP32(unsigned ValNo, MVT ValVT, MVT LocVT,
                        CCValAssign::LocInfo LocInfo, ISD::ArgFlagsTy ArgFlags,
                        CCState &State, ArrayRef<MCPhysReg> F64Regs) {
   static const MCPhysReg IntRegs[] = { LoongArch::A0, LoongArch::A1, LoongArch::A2, LoongArch::A3 };
@@ -2424,24 +2426,24 @@ static bool CC_LoongArchLP32(unsigned ValNo, MVT ValVT, MVT LocVT,
   return false;
 }
 
-static bool CC_LoongArchLP32_FP32(unsigned ValNo, MVT ValVT,
+static bool CC_LoongArchILP32_FP32(unsigned ValNo, MVT ValVT,
                             MVT LocVT, CCValAssign::LocInfo LocInfo,
                             ISD::ArgFlagsTy ArgFlags, CCState &State) {
   static const MCPhysReg F64Regs[] = {LoongArch::F0_64, LoongArch::F1_64, LoongArch::F2_64, \
                                       LoongArch::F3_64, LoongArch::F4_64, LoongArch::F5_64, \
                                       LoongArch::F6_64, LoongArch::F7_64 };
 
-  return CC_LoongArchLP32(ValNo, ValVT, LocVT, LocInfo, ArgFlags, State, F64Regs);
+  return CC_LoongArchILP32(ValNo, ValVT, LocVT, LocInfo, ArgFlags, State, F64Regs);
 }
 
-static bool CC_LoongArchLP32_FP64(unsigned ValNo, MVT ValVT,
+static bool CC_LoongArchILP32_FP64(unsigned ValNo, MVT ValVT,
                             MVT LocVT, CCValAssign::LocInfo LocInfo,
                             ISD::ArgFlagsTy ArgFlags, CCState &State) {
   static const MCPhysReg F64Regs[] = {LoongArch::F0_64, LoongArch::F1_64, LoongArch::F2_64, \
                                       LoongArch::F3_64, LoongArch::F4_64, LoongArch::F5_64, \
                                       LoongArch::F6_64, LoongArch::F7_64 };
 
-  return CC_LoongArchLP32(ValNo, ValVT, LocVT, LocInfo, ArgFlags, State, F64Regs);
+  return CC_LoongArchILP32(ValNo, ValVT, LocVT, LocInfo, ArgFlags, State, F64Regs);
 }
 
 static bool CC_LoongArch_F128(unsigned ValNo, MVT ValVT,
@@ -2463,7 +2465,7 @@ static bool CC_LoongArch_F128(unsigned ValNo, MVT ValVT,
   return true;
 }
 
-static bool CC_LoongArchLP32(unsigned ValNo, MVT ValVT, MVT LocVT,
+static bool CC_LoongArchILP32(unsigned ValNo, MVT ValVT, MVT LocVT,
                        CCValAssign::LocInfo LocInfo, ISD::ArgFlagsTy ArgFlags,
                        CCState &State) LLVM_ATTRIBUTE_UNUSED;
 
@@ -2498,12 +2500,11 @@ SDValue LoongArchTargetLowering::passArgOnStack(SDValue StackPtr, unsigned Offse
                       /* Alignment = */ 0, MachineMemOperand::MOVolatile);
 }
 
-void LoongArchTargetLowering::
-getOpndList(SmallVectorImpl<SDValue> &Ops,
-            std::deque<std::pair<unsigned, SDValue>> &RegsToPass,
-            bool IsPICCall, bool GlobalOrExternal, bool InternalLinkage,
-            bool IsCallReloc, CallLoweringInfo &CLI, SDValue Callee,
-            SDValue Chain) const {
+void LoongArchTargetLowering::getOpndList(
+    SmallVectorImpl<SDValue> &Ops,
+    std::deque<std::pair<unsigned, SDValue>> &RegsToPass, bool IsPICCall,
+    bool GlobalOrExternal, bool IsCallReloc, CallLoweringInfo &CLI,
+    SDValue Callee, SDValue Chain, bool IsTailCall) const {
   // Build a sequence of copy-to-reg nodes chained together with token
   // chain and flag operands which copy the outgoing args into registers.
   // The InFlag in necessary since all emitted instructions must be
@@ -2524,12 +2525,14 @@ getOpndList(SmallVectorImpl<SDValue> &Ops,
     Ops.push_back(CLI.DAG.getRegister(RegsToPass[i].first,
                                       RegsToPass[i].second.getValueType()));
 
-  // Add a register mask operand representing the call-preserved registers.
-  const TargetRegisterInfo *TRI = Subtarget.getRegisterInfo();
-  const uint32_t *Mask =
-      TRI->getCallPreservedMask(CLI.DAG.getMachineFunction(), CLI.CallConv);
-  assert(Mask && "Missing call preserved mask for calling convention");
-  Ops.push_back(CLI.DAG.getRegisterMask(Mask));
+  if (!IsTailCall) {
+    // Add a register mask operand representing the call-preserved registers.
+    const TargetRegisterInfo *TRI = Subtarget.getRegisterInfo();
+    const uint32_t *Mask =
+        TRI->getCallPreservedMask(CLI.DAG.getMachineFunction(), CLI.CallConv);
+    assert(Mask && "Missing call preserved mask for calling convention");
+    Ops.push_back(CLI.DAG.getRegisterMask(Mask));
+  }
 
   if (InFlag.getNode())
     Ops.push_back(InFlag);
@@ -2575,15 +2578,17 @@ LoongArchTargetLowering::LowerCall(TargetLowering::CallLoweringInfo &CLI,
 
   // There is one case where CALLSEQ_START..CALLSEQ_END can be nested, which
   // is during the lowering of a call with a byval argument which produces
-  // a call to memcpy. For the LP32 case, this causes the caller to allocate
-  // stack space for the reserved argument area for the callee, then recursively
-  // again for the memcpy call. In the NEWABI case, this doesn't occur as those
-  // ABIs mandate that the callee allocates the reserved argument area. We do
-  // still produce nested CALLSEQ_START..CALLSEQ_END with zero space though.
+  // a call to memcpy. For the ILP32D/ILP32F/ILP32S case, this causes the caller
+  // to allocate stack space for the reserved argument area for the callee, then
+  // recursively again for the memcpy call. In the NEWABI case, this doesn't
+  // occur as those ABIs mandate that the callee allocates the reserved argument
+  // area. We do still produce nested CALLSEQ_START..CALLSEQ_END with zero space
+  // though.
   //
   // If the callee has a byval argument and memcpy is used, we are mandated
-  // to already have produced a reserved argument area for the callee for LP32.
-  // Therefore, the reserved argument area can be reused for both calls.
+  // to already have produced a reserved argument area for the callee for
+  // ILP32D/ILP32F/ILP32S. Therefore, the reserved argument area can be reused
+  // for both calls.
   //
   // Other cases of calling memcpy cannot have a chain with a CALLSEQ_START
   // present, as we have yet to hook that node onto the chain.
@@ -2616,17 +2621,13 @@ LoongArchTargetLowering::LowerCall(TargetLowering::CallLoweringInfo &CLI,
 
   // Check if it's really possible to do a tail call. Restrict it to functions
   // that are part of this compilation unit.
-  bool InternalLinkage = false;
   if (IsTailCall) {
     IsTailCall = isEligibleForTailCallOptimization(
-        CCInfo, NextStackOffset, *MF.getInfo<LoongArchFunctionInfo>());
-     if (GlobalAddressSDNode *G = dyn_cast<GlobalAddressSDNode>(Callee)) {
-      InternalLinkage = G->getGlobal()->hasInternalLinkage();
-      IsTailCall &= (InternalLinkage || G->getGlobal()->hasLocalLinkage() ||
-                     G->getGlobal()->hasPrivateLinkage() ||
-                     G->getGlobal()->hasHiddenVisibility() ||
-                     G->getGlobal()->hasProtectedVisibility());
-     }
+        CCInfo, CLI, MF, NextStackOffset, *MF.getInfo<LoongArchFunctionInfo>());
+    if (GlobalAddressSDNode *G = dyn_cast<GlobalAddressSDNode>(Callee)) {
+      if (G->getGlobal()->hasExternalWeakLinkage())
+        IsTailCall = false;
+    }
   }
   if (!IsTailCall && CLI.CB && CLI.CB->isMustTailCall())
     report_fatal_error("failed to perform tail call elimination on a call "
@@ -2645,9 +2646,9 @@ LoongArchTargetLowering::LowerCall(TargetLowering::CallLoweringInfo &CLI,
   if (!(IsTailCall || MemcpyInByVal))
     Chain = DAG.getCALLSEQ_START(Chain, NextStackOffset, 0, DL);
 
-  SDValue StackPtr =
-      DAG.getCopyFromReg(Chain, DL, ABI.IsLP64D() ? LoongArch::SP_64 : LoongArch::SP,
-                         getPointerTy(DAG.getDataLayout()));
+  SDValue StackPtr = DAG.getCopyFromReg(
+      Chain, DL, Subtarget.is64Bit() ? LoongArch::SP_64 : LoongArch::SP,
+      getPointerTy(DAG.getDataLayout()));
 
   std::deque<std::pair<unsigned, SDValue>> RegsToPass;
   SmallVector<SDValue, 8> MemOpChains;
@@ -2767,8 +2768,8 @@ LoongArchTargetLowering::LowerCall(TargetLowering::CallLoweringInfo &CLI,
   SmallVector<SDValue, 8> Ops(1, Chain);
   SDVTList NodeTys = DAG.getVTList(MVT::Other, MVT::Glue);
 
-  getOpndList(Ops, RegsToPass, IsPIC, GlobalOrExternal, InternalLinkage,
-              IsCallReloc, CLI, Callee, Chain);
+  getOpndList(Ops, RegsToPass, IsPIC, GlobalOrExternal, IsCallReloc, CLI,
+              Callee, Chain, IsTailCall);
 
   if (IsTailCall) {
     MF.getFrameInfo().setHasTailCall();
@@ -2885,10 +2886,10 @@ static SDValue UnpackFromArgumentSlot(SDValue Val, const CCValAssign &VA,
   }
   }
 
-  // If this is an value smaller than the argument slot size (32-bit for LP32,
-  // 64-bit for LPX32/LP64D), it has been promoted in some way to the argument slot
-  // size. Extract the value and insert any appropriate assertions regarding
-  // sign/zero extension.
+  // If this is an value smaller than the argument slot size (32-bit for
+  // ILP32D/ILP32F/ILP32S, 64-bit for LP64D/LP64S/LP64F), it has been promoted
+  // in some way to the argument slot size. Extract the value and insert any
+  // appropriate assertions regarding sign/zero extension.
   switch (VA.getLocInfo()) {
   default:
     llvm_unreachable("Unknown loc info!");
@@ -2899,10 +2900,19 @@ static SDValue UnpackFromArgumentSlot(SDValue Val, const CCValAssign &VA,
     Val = DAG.getNode(ISD::TRUNCATE, DL, ValVT, Val);
     break;
   case CCValAssign::SExtUpper:
-  case CCValAssign::SExt:
-    Val = DAG.getNode(ISD::AssertSext, DL, LocVT, Val, DAG.getValueType(ValVT));
-    Val = DAG.getNode(ISD::TRUNCATE, DL, ValVT, Val);
+  case CCValAssign::SExt: {
+    if ((ArgVT == MVT::i1) || (ArgVT == MVT::i8) || (ArgVT == MVT::i16)) {
+      SDValue SubReg = DAG.getTargetConstant(LoongArch::sub_32, DL, MVT::i32);
+      Val = SDValue(DAG.getMachineNode(TargetOpcode::EXTRACT_SUBREG, DL, ValVT,
+                                       Val, SubReg),
+                    0);
+    } else {
+      Val =
+          DAG.getNode(ISD::AssertSext, DL, LocVT, Val, DAG.getValueType(ValVT));
+      Val = DAG.getNode(ISD::TRUNCATE, DL, ValVT, Val);
+    }
     break;
+  }
   case CCValAssign::ZExtUpper:
   case CCValAssign::ZExt:
     Val = DAG.getNode(ISD::AssertZext, DL, LocVT, Val, DAG.getValueType(ValVT));
@@ -2993,22 +3003,19 @@ SDValue LoongArchTargetLowering::LowerFormalArguments(
           (RegVT == MVT::i64 && ValVT == MVT::f64) ||
           (RegVT == MVT::f64 && ValVT == MVT::i64))
         ArgValue = DAG.getNode(ISD::BITCAST, DL, ValVT, ArgValue);
-      else if (ABI.IsLP32() && RegVT == MVT::i32 &&
-               ValVT == MVT::f64) {
-        // TODO: lp32
+      else if ((ABI.IsILP32D() || ABI.IsILP32F() || ABI.IsILP32S()) &&
+               RegVT == MVT::i32 && ValVT == MVT::f64) {
+        // TODO
+        llvm_unreachable("Unimplemented ABI");
       }
 
       InVals.push_back(ArgValue);
     } else { // VA.isRegLoc()
       MVT LocVT = VA.getLocVT();
 
-      if (ABI.IsLP32()) {
-        // We ought to be able to use LocVT directly but LP32 sets it to i32
-        // when allocating floating point values to integer registers.
-        // This shouldn't influence how we load the value into registers unless
-        // we are targeting softfloat.
-        if (VA.getValVT().isFloatingPoint() && !Subtarget.useSoftFloat())
-          LocVT = VA.getValVT();
+      if (ABI.IsILP32D() || ABI.IsILP32F() || ABI.IsILP32S()) {
+        // TODO
+        llvm_unreachable("Unimplemented ABI");
       }
 
       // sanity check
@@ -3039,7 +3046,7 @@ SDValue LoongArchTargetLowering::LowerFormalArguments(
       unsigned Reg = LoongArchFI->getSRetReturnReg();
       if (!Reg) {
         Reg = MF.getRegInfo().createVirtualRegister(
-            getRegClassFor(ABI.IsLP64D() ? MVT::i64 : MVT::i32));
+            getRegClassFor(Subtarget.is64Bit() ? MVT::i64 : MVT::i32));
         LoongArchFI->setSRetReturnReg(Reg);
       }
       SDValue Copy = DAG.getCopyToReg(DAG.getEntryNode(), DL, Reg, InVals[i]);
@@ -3077,8 +3084,8 @@ LoongArchTargetLowering::CanLowerReturn(CallingConv::ID CallConv,
 
 bool
 LoongArchTargetLowering::shouldSignExtendTypeInLibCall(EVT Type, bool IsSigned) const {
-  if ((ABI.IsLPX32() || ABI.IsLP64D()) && Type == MVT::i32)
-      return true;
+  if (Subtarget.is64Bit() && Type == MVT::i32)
+    return true;
 
   return IsSigned;
 }
@@ -3165,7 +3172,7 @@ LoongArchTargetLowering::LowerReturn(SDValue Chain, CallingConv::ID CallConv,
       llvm_unreachable("sret virtual register not created in the entry block");
     SDValue Val =
         DAG.getCopyFromReg(Chain, DL, Reg, getPointerTy(DAG.getDataLayout()));
-    unsigned A0 = ABI.IsLP64D() ? LoongArch::A0_64 : LoongArch::A0;
+    unsigned A0 = Subtarget.is64Bit() ? LoongArch::A0_64 : LoongArch::A0;
 
     Chain = DAG.getCopyToReg(Chain, DL, A0, Val, Flag);
     Flag = Chain.getValue(1);
@@ -3283,7 +3290,7 @@ static std::pair<bool, bool> parsePhysicalReg(StringRef C, StringRef &Prefix,
 
 EVT LoongArchTargetLowering::getTypeForExtReturn(LLVMContext &Context, EVT VT,
                                             ISD::NodeType) const {
-  bool Cond = !Subtarget.isABI_LP32() && VT.getSizeInBits() == 32;
+  bool Cond = Subtarget.is64Bit() && VT.getSizeInBits() == 32;
   EVT MinVT = getRegisterType(Context, Cond ? MVT::i64 : MVT::i32);
   return VT.bitsLT(MinVT) ? MinVT : VT;
 }
@@ -3305,10 +3312,10 @@ parseRegForInlineAsmConstraint(StringRef C, MVT VT) const {
     return std::make_pair(0U, nullptr);
 
   if (Prefix == "$f") { // Parse $f0-$f31.
-    // If the size of FP registers is 64-bit or Reg is an even number, select
-    // the 64-bit register class. Otherwise, select the 32-bit register class.
+    // If the size of FP registers is 64-bit, select the 64-bit register class.
+    // Otherwise, select the 32-bit register class.
     if (VT == MVT::Other)
-      VT = (Subtarget.isFP64bit() || !(Reg % 2)) ? MVT::f64 : MVT::f32;
+      VT = Subtarget.hasBasicD() ? MVT::f64 : MVT::f32;
 
     RC = getRegClassFor(VT);
   }
@@ -3708,9 +3715,9 @@ void LoongArchTargetLowering::writeVarArgRegs(std::vector<SDValue> &OutChains,
   LoongArchFI->setVarArgsFrameIndex(FI);
 
   // Copy the integer registers that have not been used for argument passing
-  // to the argument register save area. For LP32, the save area is allocated
-  // in the caller's stack frame, while for LPX32/LP64D, it is allocated in the
-  // callee's stack frame.
+  // to the argument register save area. For ILP32D/ILP32F/ILP32S, the save area
+  // is allocated in the caller's stack frame, while for LP64D/LP64S/LP64F, it
+  // is allocated in the callee's stack frame.
   for (unsigned I = Idx; I < ArgRegs.size();
        ++I, VaArgOffset += RegSizeInBytes) {
     unsigned Reg = addLiveIn(MF, ArgRegs[I], RC);
@@ -3739,9 +3746,12 @@ void LoongArchTargetLowering::HandleByVal(CCState *State, unsigned &Size,
   if (State->getCallingConv() != CallingConv::Fast) {
     unsigned RegSizeInBytes = Subtarget.getGPRSizeInBytes();
     ArrayRef<MCPhysReg> IntArgRegs = ABI.GetByValArgRegs();
-    // FIXME: The LP32 case actually describes no shadow registers.
+    // FIXME: The ILP32D/ILP32F/ILP32S case actually describes no shadow
+    // registers.
     const MCPhysReg *ShadowRegs =
-        ABI.IsLP32() ? IntArgRegs.data() : LoongArch64DPRegs;
+        (ABI.IsILP32D() || ABI.IsILP32F() || ABI.IsILP32S())
+            ? IntArgRegs.data()
+            : LoongArch64DPRegs;
 
     // We used to check the size as well but we can't do that anymore since
     // CCState::HandleByVal() rounds up the size after calling this function.
diff --git a/llvm/lib/Target/LoongArch/LoongArchISelLowering.h b/llvm/lib/Target/LoongArch/LoongArchISelLowering.h
index edcf95c539b26..5b49adccb53db 100644
--- a/llvm/lib/Target/LoongArch/LoongArchISelLowering.h
+++ b/llvm/lib/Target/LoongArch/LoongArchISelLowering.h
@@ -218,12 +218,11 @@ class TargetRegisterClass;
     /// This function fills Ops, which is the list of operands that will later
     /// be used when a function call node is created. It also generates
     /// copyToReg nodes to set up argument registers.
-    void
-    getOpndList(SmallVectorImpl<SDValue> &Ops,
-                std::deque<std::pair<unsigned, SDValue>> &RegsToPass,
-                bool IsPICCall, bool GlobalOrExternal, bool InternalLinkage,
-                bool IsCallReloc, CallLoweringInfo &CLI, SDValue Callee,
-                SDValue Chain) const;
+    void getOpndList(SmallVectorImpl<SDValue> &Ops,
+                     std::deque<std::pair<unsigned, SDValue>> &RegsToPass,
+                     bool IsPICCall, bool GlobalOrExternal, bool IsCallReloc,
+                     CallLoweringInfo &CLI, SDValue Callee, SDValue Chain,
+                     bool IsTailCall) const;
 
     SDValue lowerLOAD(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerSTORE(SDValue Op, SelectionDAG &DAG) const;
@@ -279,7 +278,6 @@ class TargetRegisterClass;
     SDValue lowerSETCC(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerVASTART(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerVAARG(SDValue Op, SelectionDAG &DAG) const;
-    SDValue lowerFCOPYSIGN(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerFABS(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerFRAMEADDR(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerRETURNADDR(SDValue Op, SelectionDAG &DAG) const;
@@ -294,6 +292,7 @@ class TargetRegisterClass;
     /// for tail call optimization.
     bool
     isEligibleForTailCallOptimization(const CCState &CCInfo,
+                                      CallLoweringInfo &CLI, MachineFunction &MF,
                                       unsigned NextStackOffset,
                                       const LoongArchFunctionInfo &FI) const;
 
diff --git a/llvm/lib/Target/LoongArch/LoongArchInstrInfo.td b/llvm/lib/Target/LoongArch/LoongArchInstrInfo.td
index 7fc098e6b2d31..50ba8c91dcb0a 100644
--- a/llvm/lib/Target/LoongArch/LoongArchInstrInfo.td
+++ b/llvm/lib/Target/LoongArch/LoongArchInstrInfo.td
@@ -1305,6 +1305,31 @@ def PseudoCall : LoongArchPseudo<(outs), (ins calltarget:$target),
                                         []>;
 }
 
+let isCall = 1, isTerminator = 1, isReturn = 1, isBarrier = 1, Uses = [SP] in
+def PseudoTailCall : LoongArchPseudo<(outs), (ins calltarget:$target),
+                                     []>;
+
+class PseudoTailBase<DAGOperand opnd> : LoongArchPseudo<(outs), (ins opnd:$offset26),
+                                                        []> {
+  let isTerminator = 1;
+  let isBarrier = 1;
+  let isReturn = 1;
+  let isCodeGenOnly = 1;
+}
+def PseudoTailReturn : PseudoTailBase<calltarget>;
+
+
+def : LoongArchPat<(LoongArchTailCall tglobaladdr:$dst),
+                   (PseudoTailCall tglobaladdr:$dst)>;
+
+def : LoongArchPat<(LoongArchTailCall texternalsym:$dst),
+                   (PseudoTailCall texternalsym:$dst)>;
+
+let isCall = 1, isTerminator = 1, isReturn = 1, isBarrier = 1,  isIndirectBranch = 1, Uses = [SP] in
+def PseudoTAILIndirect : LoongArchPseudo<(outs), (ins GPRTC64Opnd:$rj), [(LoongArchTailCall GPRTC64Opnd:$rj)]>,
+                         PseudoInstExpansion<(JIRL ZERO_64, GPR64Opnd:$rj, 0)>;
+
+
 def : LoongArchPat<(LoongArchJmpLink tglobaladdr:$dst),
               (PseudoCall tglobaladdr:$dst)>;
 
@@ -1558,6 +1583,12 @@ let isCodeGenOnly = 1 in {
       let InOperandList = (ins GPR64:$rj) in
         def SLLI_W_64_64 : Shift_Imm32<"", GPR32Opnd>, R2_IMM5<0b00>, GPR_64;
   }
+
+  let AsmString = "sltui\t$rd, $rj, $imm12",
+      OutOperandList = (outs GPR64:$rd) in {
+      let InOperandList = (ins GPR64:$rj, simm12:$imm12) in
+        def SLTUI_64 : SetCC_I<"", GPR64Opnd, simm12>, R2_IMM12<0b001>, GPR_64;
+  }
 }
 
 // 32-to-64-bit extension
@@ -1780,3 +1811,16 @@ def : LoongArchPat<(i64 (sext (i32 (sra GPR32:$src, immZExt5:$imm5)))),
 def : LoongArchPat<(i64 (sext (i32 (sra GPR32:$src, GPR32:$src2)))),
               (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
               (SRA_W GPR32:$src, GPR32:$src2), sub_32)>;
+
+
+def : LoongArchPat<(i64 (xor GPR64:$rj, (i64 -1))),
+                   (NOR ZERO_64, GPR64:$rj)>;
+
+def : LoongArchPat<(and GPR64:$rj, (i64 (xor GPR64:$rk, (i64 -1)))),
+                   (ANDN GPR64:$rj, GPR64:$rk)>;
+
+def : LoongArchPat<(i64 (or GPR64:$rj, (xor GPR64:$rk, (i64 -1)))),
+                   (ORN GPR64:$rj, GPR64:$rk)>;
+
+def : LoongArchPat<(i64 (zext (i32 (seteq GPR64:$rj, (i64 0))))),
+                   (SLTUI_64 GPR64:$rj, (i64 1))>;
diff --git a/llvm/lib/Target/LoongArch/LoongArchInstrInfoF.td b/llvm/lib/Target/LoongArch/LoongArchInstrInfoF.td
index a0d14000d331c..2524db7dae131 100644
--- a/llvm/lib/Target/LoongArch/LoongArchInstrInfoF.td
+++ b/llvm/lib/Target/LoongArch/LoongArchInstrInfoF.td
@@ -22,8 +22,7 @@ def fpimm1 : PatLeaf<(fpimm), [{
   return N->isExactlyValue(+1.0);
 }]>;
 
-def IsNotSoftFloat   : Predicate<"!Subtarget->useSoftFloat()">,
-                       AssemblerPredicate<(all_of FeatureSoftFloat)>;
+def IsNotSoftFloat   : Predicate<"!Subtarget->useSoftFloat()">;
 
 class HARDFLOAT { list<Predicate> HardFloatPredicate = [IsNotSoftFloat]; }
 
diff --git a/llvm/lib/Target/LoongArch/LoongArchMachineFunction.cpp b/llvm/lib/Target/LoongArch/LoongArchMachineFunction.cpp
index 501766609e850..c7f95b4555d21 100644
--- a/llvm/lib/Target/LoongArch/LoongArchMachineFunction.cpp
+++ b/llvm/lib/Target/LoongArch/LoongArchMachineFunction.cpp
@@ -46,13 +46,4 @@ MachinePointerInfo LoongArchFunctionInfo::callPtrInfo(const GlobalValue *GV) {
   return MachinePointerInfo(MF.getPSVManager().getGlobalValueCallEntry(GV));
 }
 
-int LoongArchFunctionInfo::getMoveF64ViaSpillFI(const TargetRegisterClass *RC) {
-  const TargetRegisterInfo &TRI = *MF.getSubtarget().getRegisterInfo();
-  if (MoveF64ViaSpillFI == -1) {
-    MoveF64ViaSpillFI = MF.getFrameInfo().CreateStackObject(
-        TRI.getSpillSize(*RC), TRI.getSpillAlign(*RC), false);
-  }
-  return MoveF64ViaSpillFI;
-}
-
 void LoongArchFunctionInfo::anchor() {}
diff --git a/llvm/lib/Target/LoongArch/LoongArchMachineFunction.h b/llvm/lib/Target/LoongArch/LoongArchMachineFunction.h
index 46847c45947ef..549e7b4053d4c 100644
--- a/llvm/lib/Target/LoongArch/LoongArchMachineFunction.h
+++ b/llvm/lib/Target/LoongArch/LoongArchMachineFunction.h
@@ -59,8 +59,6 @@ class LoongArchFunctionInfo : public MachineFunctionInfo {
   void setSaveS2() { SaveS2 = true; }
   bool hasSaveS2() const { return SaveS2; }
 
-  int getMoveF64ViaSpillFI(const TargetRegisterClass *RC);
-
 private:
   virtual void anchor();
 
@@ -89,9 +87,6 @@ class LoongArchFunctionInfo : public MachineFunctionInfo {
   // saveS2
   bool SaveS2 = false;
 
-  /// FrameIndex for expanding BuildPairF64 nodes to spill and reload when the
-  /// LP32 FPXX ABI is enabled. -1 is used to denote invalid index.
-  int MoveF64ViaSpillFI = -1;
 };
 
 } // end namespace llvm
diff --git a/llvm/lib/Target/LoongArch/LoongArchRegisterInfo.cpp b/llvm/lib/Target/LoongArch/LoongArchRegisterInfo.cpp
index e08236335f665..f7e4b57761872 100644
--- a/llvm/lib/Target/LoongArch/LoongArchRegisterInfo.cpp
+++ b/llvm/lib/Target/LoongArch/LoongArchRegisterInfo.cpp
@@ -87,16 +87,13 @@ const MCPhysReg *
 LoongArchRegisterInfo::getCalleeSavedRegs(const MachineFunction *MF) const {
   const LoongArchSubtarget &Subtarget = MF->getSubtarget<LoongArchSubtarget>();
 
-  if (Subtarget.isSingleFloat())
+  if ((Subtarget.hasBasicF() && !Subtarget.hasBasicD()))
     return CSR_SingleFloatOnly_SaveList;
 
   if (Subtarget.isABI_LP64D())
     return CSR_LP64_SaveList;
 
-  if (Subtarget.isABI_LPX32())
-    return CSR_LPX32_SaveList;
-
-  return CSR_LP32_SaveList;
+  return CSR_ILP32_SaveList;
 }
 
 const uint32_t *
@@ -104,13 +101,13 @@ LoongArchRegisterInfo::getCallPreservedMask(const MachineFunction &MF,
                                        CallingConv::ID) const {
   const LoongArchSubtarget &Subtarget = MF.getSubtarget<LoongArchSubtarget>();
 
-  if (Subtarget.isSingleFloat())
+  if ((Subtarget.hasBasicF() && !Subtarget.hasBasicD()))
     return CSR_SingleFloatOnly_RegMask;
 
   if (Subtarget.isABI_LP64D())
     return CSR_LP64_RegMask;
 
-  return CSR_LP32_RegMask;
+  return CSR_ILP32_RegMask;
 }
 
 BitVector LoongArchRegisterInfo::
diff --git a/llvm/lib/Target/LoongArch/LoongArchRegisterInfo.td b/llvm/lib/Target/LoongArch/LoongArchRegisterInfo.td
index b5341c5d64315..ba72cfa4019ca 100644
--- a/llvm/lib/Target/LoongArch/LoongArchRegisterInfo.td
+++ b/llvm/lib/Target/LoongArch/LoongArchRegisterInfo.td
@@ -197,6 +197,12 @@ def GPR64 : RegisterClass<"LoongArch", [i64], 64, (add
   // Reserved
   T9_64, FP_64)>;
 
+def GPRTC64 : RegisterClass<"LoongArch", [i64], 64, (add
+  // Return Values and Arguments
+  A0_64, A1_64, A2_64, A3_64, A4_64, A5_64, A6_64, A7_64,
+  // Not preserved across procedure calls
+  T0_64, T1_64, T2_64, T3_64, T4_64, T5_64, T6_64, T7_64, T8_64)>;
+
 /// FP Registers.
 def FGR64 : RegisterClass<"LoongArch", [f64], 64, (sequence "F%u_64", 0, 31)>;
 def FGR32 : RegisterClass<"LoongArch", [f32], 64, (sequence "F%u", 0, 31)>;
@@ -258,6 +264,10 @@ def GPR64Opnd : RegisterOperand<GPR64> {
   let ParserMatchClass = GPR64AsmOperand;
 }
 
+def GPRTC64Opnd : RegisterOperand<GPRTC64> {
+  let ParserMatchClass = GPR64AsmOperand;
+}
+
 def FGR32Opnd : RegisterOperand<FGR32> {
   let ParserMatchClass = FGR32AsmOperand;
 }
diff --git a/llvm/lib/Target/LoongArch/LoongArchSubtarget.cpp b/llvm/lib/Target/LoongArch/LoongArchSubtarget.cpp
index 74036c7c3e931..8b78034b8eb80 100644
--- a/llvm/lib/Target/LoongArch/LoongArchSubtarget.cpp
+++ b/llvm/lib/Target/LoongArch/LoongArchSubtarget.cpp
@@ -32,25 +32,19 @@ using namespace llvm;
 
 void LoongArchSubtarget::anchor() {}
 
-LoongArchSubtarget::LoongArchSubtarget(const Triple &TT, StringRef CPU, StringRef FS,
-                             const LoongArchTargetMachine &TM,
-                             MaybeAlign StackAlignOverride)
-    : LoongArchGenSubtargetInfo(TT, CPU, FS),
-      HasLA64(false),
-      IsSoftFloat(false), IsSingleFloat(false),
-      IsFP64bit(false),
-      StackAlignOverride(StackAlignOverride),
-      TM(TM), TargetTriple(TT), TSInfo(),
-      InstrInfo(initializeSubtargetDependencies(CPU, FS, TM)),
-      FrameLowering(*this),
-      TLInfo(TM, *this) {
+LoongArchSubtarget::LoongArchSubtarget(const Triple &TT, StringRef CPU,
+                                       StringRef FS,
+                                       const LoongArchTargetMachine &TM,
+                                       MaybeAlign StackAlignOverride)
+    : LoongArchGenSubtargetInfo(TT, CPU, FS), HasLA64(false),
+      HasBasicF(false), HasBasicD(false),
+      StackAlignOverride(StackAlignOverride), TM(TM), TargetTriple(TT),
+      TSInfo(), InstrInfo(initializeSubtargetDependencies(CPU, FS, TM)),
+      FrameLowering(*this), TLInfo(TM, *this) {
 
   // Check if Architecture and ABI are compatible.
-  assert(((!is64Bit() && isABI_LP32()) ||
-          (is64Bit() && (isABI_LPX32() || isABI_LP64D()))) &&
+  assert(((!is64Bit() && isABI_ILP32()) || (is64Bit() && isABI_LP64())) &&
          "Invalid  Arch & ABI pair.");
-
-  assert(isFP64bit());
 }
 
 bool LoongArchSubtarget::isPositionIndependent() const {
@@ -82,10 +76,10 @@ LoongArchSubtarget::initializeSubtargetDependencies(StringRef CPU, StringRef FS,
 
   if (StackAlignOverride)
     stackAlignment = *StackAlignOverride;
-  else if (isABI_LPX32() || isABI_LP64D())
+  else if (isABI_LP64())
     stackAlignment = Align(16);
   else {
-    assert(isABI_LP32() && "Unknown ABI for stack alignment!");
+    assert(isABI_ILP32() && "Unknown ABI for stack alignment!");
     stackAlignment = Align(8);
   }
 
@@ -97,6 +91,15 @@ Reloc::Model LoongArchSubtarget::getRelocationModel() const {
 }
 
 bool LoongArchSubtarget::isABI_LP64D() const { return getABI().IsLP64D(); }
-bool LoongArchSubtarget::isABI_LPX32() const { return getABI().IsLPX32(); }
-bool LoongArchSubtarget::isABI_LP32() const { return getABI().IsLP32(); }
+bool LoongArchSubtarget::isABI_LP64S() const { return getABI().IsLP64S(); }
+bool LoongArchSubtarget::isABI_LP64F() const { return getABI().IsLP64F(); }
+bool LoongArchSubtarget::isABI_LP64() const {
+  return isABI_LP64D() || isABI_LP64S() || isABI_LP64F();
+}
+bool LoongArchSubtarget::isABI_ILP32D() const { return getABI().IsILP32D(); }
+bool LoongArchSubtarget::isABI_ILP32F() const { return getABI().IsILP32F(); }
+bool LoongArchSubtarget::isABI_ILP32S() const { return getABI().IsILP32S(); }
+bool LoongArchSubtarget::isABI_ILP32() const {
+  return isABI_ILP32D() || isABI_ILP32F() || isABI_ILP32S();
+}
 const LoongArchABIInfo &LoongArchSubtarget::getABI() const { return TM.getABI(); }
diff --git a/llvm/lib/Target/LoongArch/LoongArchSubtarget.h b/llvm/lib/Target/LoongArch/LoongArchSubtarget.h
index 467ebb9c14f29..134eb658b4b4f 100644
--- a/llvm/lib/Target/LoongArch/LoongArchSubtarget.h
+++ b/llvm/lib/Target/LoongArch/LoongArchSubtarget.h
@@ -38,16 +38,13 @@ class LoongArchSubtarget : public LoongArchGenSubtargetInfo {
   // HasLA64 - The target processor has LA64 ISA support.
   bool HasLA64;
 
-  // IsSoftFloat - The target does not support any floating point instructions.
-  bool IsSoftFloat;
+  // HasBasicF - The target restricts the use of hardware floating-point
+  // instructions to 32-bit operations.
+  bool HasBasicF;
 
-  // IsSingleFloat - The target only supports single precision float
-  // point operations. This enable the target to use all 32 32-bit
-  // floating point registers instead of only using even ones.
-  bool IsSingleFloat;
-
-  // IsFP64bit - The target processor has 64-bit floating point registers.
-  bool IsFP64bit;
+  // HasBasicD - The target allows hardware floating-point instructions to
+  // cover both 32-bit and 64-bit operations.
+  bool HasBasicD;
 
   /// The minimum alignment known to hold of the stack frame on
   /// entry to the function and which must be maintained by every function.
@@ -74,9 +71,14 @@ class LoongArchSubtarget : public LoongArchGenSubtargetInfo {
   void getCriticalPathRCs(RegClassVector &CriticalPathRCs) const override;
   CodeGenOpt::Level getOptLevelToEnablePostRAScheduler() const override;
 
+  bool isABI_LP64() const;
   bool isABI_LP64D() const;
-  bool isABI_LPX32() const;
-  bool isABI_LP32() const;
+  bool isABI_LP64S() const;
+  bool isABI_LP64F() const;
+  bool isABI_ILP32() const;
+  bool isABI_ILP32D() const;
+  bool isABI_ILP32F() const;
+  bool isABI_ILP32S() const;
   const LoongArchABIInfo &getABI() const;
 
   /// This constructor initializes the data members to match that
@@ -89,10 +91,10 @@ class LoongArchSubtarget : public LoongArchGenSubtargetInfo {
   void ParseSubtargetFeatures(StringRef CPU, StringRef FS);
 
   bool is64Bit() const { return HasLA64; }
-  bool isFP64bit() const { return IsFP64bit; }
+  bool hasBasicD() const { return HasBasicD; }
   unsigned getGPRSizeInBytes() const { return is64Bit() ? 8 : 4; }
-  bool isSingleFloat() const { return IsSingleFloat; }
-  bool useSoftFloat() const { return IsSoftFloat; }
+  bool hasBasicF() const { return HasBasicF; }
+  bool useSoftFloat() const { return (!HasBasicD && !HasBasicF); }
 
   // After compiler-rt is supported in LA, this returns true.
   bool isXRaySupported() const override { return false; }
diff --git a/llvm/lib/Target/LoongArch/LoongArchTargetMachine.cpp b/llvm/lib/Target/LoongArch/LoongArchTargetMachine.cpp
index 051bf84b89701..0ace2ce23ca59 100644
--- a/llvm/lib/Target/LoongArch/LoongArchTargetMachine.cpp
+++ b/llvm/lib/Target/LoongArch/LoongArchTargetMachine.cpp
@@ -52,13 +52,14 @@ static std::string computeDataLayout(const Triple &TT, StringRef CPU,
 
   Ret += "e";
 
-  if (ABI.IsLP32())
-    Ret += "-m:m";
+  if (ABI.IsILP32D() || ABI.IsILP32F() || ABI.IsILP32S())
+    // TODO
+    llvm_unreachable("Unimplemented ABI");
   else
     Ret += "-m:e";
 
   // Pointers are 32 bit on some ABIs.
-  if (!(ABI.IsLP64D()))
+  if (!(ABI.IsLP64D() || ABI.IsLP64S() || ABI.IsLP64F()))
     Ret += "-p:32:32";
 
   // 8 and 16 bit integers only need to have natural alignment, but try to
@@ -68,7 +69,7 @@ static std::string computeDataLayout(const Triple &TT, StringRef CPU,
   // 32 bit registers are always available and the stack is at least 64 bit
   // aligned. On LP64 64 bit registers are also available and the stack is
   // 128 bit aligned.
-  if (ABI.IsLP64D() || ABI.IsLPX32())
+  if (ABI.IsLP64D() || ABI.IsLP64S() || ABI.IsLP64F())
     Ret += "-n32:64-S128";
   else
     Ret += "-n32-S64";
@@ -116,16 +117,6 @@ LoongArchTargetMachine::getSubtargetImpl(const Function &F) const {
                        ? FSAttr.getValueAsString().str()
                        : TargetFS;
 
-  // FIXME: This is related to the code below to reset the target options,
-  // we need to know whether or not the soft float flag is set on the
-  // function, so we can enable it as a subtarget feature.
-  bool softFloat =
-      F.hasFnAttribute("use-soft-float") &&
-      F.getFnAttribute("use-soft-float").getValueAsString() == "true";
-
-  if (softFloat)
-    FS += FS.empty() ? "+soft-float" : ",+soft-float";
-
   auto &I = SubtargetMap[CPU + FS];
   if (!I) {
     // This needs to be done before we create a new subtarget since any
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchABIInfo.cpp b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchABIInfo.cpp
index 133f389819541..5e70e64e59036 100644
--- a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchABIInfo.cpp
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchABIInfo.cpp
@@ -15,7 +15,6 @@
 using namespace llvm;
 
 namespace {
-static const MCPhysReg LP32IntRegs[4] = {LoongArch::A0, LoongArch::A1, LoongArch::A2, LoongArch::A3};
 
 static const MCPhysReg LoongArch64IntRegs[8] = {
     LoongArch::A0_64, LoongArch::A1_64, LoongArch::A2_64, LoongArch::A3_64,
@@ -23,42 +22,51 @@ static const MCPhysReg LoongArch64IntRegs[8] = {
 }
 
 ArrayRef<MCPhysReg> LoongArchABIInfo::GetByValArgRegs() const {
-  if (IsLP32())
-    return makeArrayRef(LP32IntRegs);
-  if (IsLPX32() || IsLP64D())
+  if (IsILP32D() || IsILP32F() || IsILP32S())
+    // TODO
+    llvm_unreachable("Unimplemented ABI");
+  if (IsLP64D() || IsLP64S() || IsLP64F())
     return makeArrayRef(LoongArch64IntRegs);
   llvm_unreachable("Unhandled ABI");
 }
 
 ArrayRef<MCPhysReg> LoongArchABIInfo::GetVarArgRegs() const {
-  if (IsLP32())
-    return makeArrayRef(LP32IntRegs);
-  if (IsLPX32() || IsLP64D())
+  if (IsILP32D() || IsILP32F() || IsILP32S())
+    // TODO
+    llvm_unreachable("Unimplemented ABI");
+  if (IsLP64D() || IsLP64S() || IsLP64F())
     return makeArrayRef(LoongArch64IntRegs);
   llvm_unreachable("Unhandled ABI");
 }
 
 unsigned LoongArchABIInfo::GetCalleeAllocdArgSizeInBytes(CallingConv::ID CC) const {
-  if (IsLP32())
-    return CC != CallingConv::Fast ? 16 : 0;
-  if (IsLPX32() || IsLP64D())
+  if (IsILP32D() || IsILP32F() || IsILP32S())
+    // TODO
+    llvm_unreachable("Unimplemented ABI");
+  if (IsLP64D() || IsLP64S() || IsLP64F())
     return 0;
   llvm_unreachable("Unhandled ABI");
 }
 
 LoongArchABIInfo LoongArchABIInfo::computeTargetABI(const Triple &TT, StringRef CPU,
                                           const MCTargetOptions &Options) {
-  if (Options.getABIName().startswith("lp32"))
-    return LoongArchABIInfo::LP32();
-  if (Options.getABIName().startswith("lpx32"))
-    return LoongArchABIInfo::LPX32();
+  if (Options.getABIName().startswith("ilp32d"))
+    return LoongArchABIInfo::ILP32D();
+  if (Options.getABIName().startswith("ilp32f"))
+    return LoongArchABIInfo::ILP32F();
+  if (Options.getABIName().startswith("ilp32s"))
+    return LoongArchABIInfo::ILP32S();
   if (Options.getABIName().startswith("lp64d"))
     return LoongArchABIInfo::LP64D();
+  if (Options.getABIName().startswith("lp64s"))
+    return LoongArchABIInfo::LP64S();
+  if (Options.getABIName().startswith("lp64f"))
+    return LoongArchABIInfo::LP64F();
   assert(Options.getABIName().empty() && "Unknown ABI option for LoongArch");
 
   if (TT.isLoongArch64())
     return LoongArchABIInfo::LP64D();
-  return LoongArchABIInfo::LP32();
+  return LoongArchABIInfo::ILP32D();
 }
 
 unsigned LoongArchABIInfo::GetStackPtr() const {
@@ -109,6 +117,6 @@ unsigned LoongArchABIInfo::GetEhDataReg(unsigned I) const {
     LoongArch::A0_64, LoongArch::A1_64, LoongArch::A2_64, LoongArch::A3_64
   };
 
-  return IsLP64D() ? EhDataReg64[I] : EhDataReg[I];
+  return (IsLP64D() || IsLP64S() || IsLP64F()) ? EhDataReg64[I] : EhDataReg[I];
 }
 
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchABIInfo.h b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchABIInfo.h
index b3fea26a29572..74c30732e3e2f 100644
--- a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchABIInfo.h
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchABIInfo.h
@@ -22,7 +22,7 @@ class TargetRegisterClass;
 
 class LoongArchABIInfo {
 public:
-  enum class ABI { Unknown, LP32, LPX32, LP64D };
+  enum class ABI { Unknown, ILP32D, ILP32F, ILP32S, LP64D, LP64F, LP64S };
 
 protected:
   ABI ThisABI;
@@ -31,16 +31,22 @@ class LoongArchABIInfo {
   LoongArchABIInfo(ABI ThisABI) : ThisABI(ThisABI) {}
 
   static LoongArchABIInfo Unknown() { return LoongArchABIInfo(ABI::Unknown); }
-  static LoongArchABIInfo LP32() { return LoongArchABIInfo(ABI::LP32); }
-  static LoongArchABIInfo LPX32() { return LoongArchABIInfo(ABI::LPX32); }
+  static LoongArchABIInfo ILP32D() { return LoongArchABIInfo(ABI::ILP32D); }
+  static LoongArchABIInfo ILP32F() { return LoongArchABIInfo(ABI::ILP32F); }
+  static LoongArchABIInfo ILP32S() { return LoongArchABIInfo(ABI::ILP32S); }
   static LoongArchABIInfo LP64D() { return LoongArchABIInfo(ABI::LP64D); }
+  static LoongArchABIInfo LP64S() { return LoongArchABIInfo(ABI::LP64S); }
+  static LoongArchABIInfo LP64F() { return LoongArchABIInfo(ABI::LP64F); }
   static LoongArchABIInfo computeTargetABI(const Triple &TT, StringRef CPU,
                                       const MCTargetOptions &Options);
 
   bool IsKnown() const { return ThisABI != ABI::Unknown; }
-  bool IsLP32() const { return ThisABI == ABI::LP32; }
-  bool IsLPX32() const { return ThisABI == ABI::LPX32; }
+  bool IsILP32D() const { return ThisABI == ABI::ILP32D; }
+  bool IsILP32F() const { return ThisABI == ABI::ILP32F; }
+  bool IsILP32S() const { return ThisABI == ABI::ILP32S; }
   bool IsLP64D() const { return ThisABI == ABI::LP64D; }
+  bool IsLP64S() const { return ThisABI == ABI::LP64S; }
+  bool IsLP64F() const { return ThisABI == ABI::LP64F; }
   ABI GetEnumValue() const { return ThisABI; }
 
   /// The registers to use for byval arguments.
@@ -50,7 +56,7 @@ class LoongArchABIInfo {
   ArrayRef<MCPhysReg> GetVarArgRegs() const;
 
   /// Obtain the size of the area allocated by the callee for arguments.
-  /// CallingConv::FastCall affects the value for LP32.
+  /// CallingConv::FastCall affects the value for 32-bit ABI.
   unsigned GetCalleeAllocdArgSizeInBytes(CallingConv::ID CC) const;
 
   /// Ordering of ABI's
@@ -70,8 +76,12 @@ class LoongArchABIInfo {
   unsigned GetPtrSubOp() const;
   unsigned GetPtrAndOp() const;
   unsigned GetGPRMoveOp() const;
-  inline bool ArePtrs64bit() const { return IsLP64D(); }
-  inline bool AreGprs64bit() const { return IsLPX32() || IsLP64D(); }
+  inline bool ArePtrs64bit() const {
+    return IsLP64D() || IsLP64S() || IsLP64F();
+  }
+  inline bool AreGprs64bit() const {
+    return IsLP64D() || IsLP64S() || IsLP64F();
+  }
 
   unsigned GetEhDataReg(unsigned I) const;
 };
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchAsmBackend.cpp b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchAsmBackend.cpp
index e2c642832c146..50fdc0e4cc778 100644
--- a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchAsmBackend.cpp
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchAsmBackend.cpp
@@ -58,7 +58,7 @@ static unsigned adjustFixupValue(const MCFixup &Fixup, uint64_t Value,
 
 std::unique_ptr<MCObjectTargetWriter>
 LoongArchAsmBackend::createObjectTargetWriter() const {
-  return createLoongArchELFObjectWriter(TheTriple, IsLPX32);
+  return createLoongArchELFObjectWriter(TheTriple);
 }
 
 /// ApplyFixup - Apply the \p Value for given \p Fixup into the provided
@@ -229,6 +229,5 @@ MCAsmBackend *llvm::createLoongArchAsmBackend(const Target &T,
                                               const MCTargetOptions &Options) {
   LoongArchABIInfo ABI = LoongArchABIInfo::computeTargetABI(
                            STI.getTargetTriple(), STI.getCPU(), Options);
-  return new LoongArchAsmBackend(T, MRI, STI.getTargetTriple(), STI.getCPU(),
-                                 ABI.IsLPX32());
+  return new LoongArchAsmBackend(T, MRI, STI.getTargetTriple(), STI.getCPU());
 }
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchAsmBackend.h b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchAsmBackend.h
index 575dd915e9dd5..c6c28237eb5da 100644
--- a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchAsmBackend.h
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchAsmBackend.h
@@ -29,13 +29,11 @@ class Target;
 
 class LoongArchAsmBackend : public MCAsmBackend {
   Triple TheTriple;
-  bool IsLPX32;
 
 public:
-  LoongArchAsmBackend(const Target &T, const MCRegisterInfo &MRI, const Triple &TT,
-                 StringRef CPU, bool LPX32)
-      : MCAsmBackend(support::little),
-        TheTriple(TT), IsLPX32(LPX32) {
+  LoongArchAsmBackend(const Target &T, const MCRegisterInfo &MRI,
+                      const Triple &TT, StringRef CPU)
+      : MCAsmBackend(support::little), TheTriple(TT) {
     assert(TT.isLittleEndian());
   }
 
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchELFObjectWriter.cpp b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchELFObjectWriter.cpp
index e00b9af9d49e9..4e043035b5587 100644
--- a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchELFObjectWriter.cpp
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchELFObjectWriter.cpp
@@ -177,9 +177,9 @@ unsigned LoongArchELFObjectWriter::getRelocType(MCContext &Ctx,
 }
 
 std::unique_ptr<MCObjectTargetWriter>
-llvm::createLoongArchELFObjectWriter(const Triple &TT, bool IsLPX32) {
+llvm::createLoongArchELFObjectWriter(const Triple &TT) {
   uint8_t OSABI = MCELFObjectTargetWriter::getOSABI(TT.getOS());
-  bool IsLP64 = TT.isArch64Bit() && !IsLPX32;
+  bool IsLP64 = TT.isArch64Bit();
   bool HasRelocationAddend = TT.isArch64Bit();
   return std::make_unique<LoongArchELFObjectWriter>(OSABI, HasRelocationAddend,
                                                 IsLP64);
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCTargetDesc.h b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCTargetDesc.h
index c348a10a06178..56949ef1f7b0e 100644
--- a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCTargetDesc.h
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCTargetDesc.h
@@ -45,7 +45,7 @@ MCAsmBackend *createLoongArchAsmBackend(const Target &T,
                                         const MCTargetOptions &Options);
 
 std::unique_ptr<MCObjectTargetWriter>
-createLoongArchELFObjectWriter(const Triple &TT, bool IsLPX32);
+createLoongArchELFObjectWriter(const Triple &TT);
 
 namespace LoongArch_MC {
 StringRef selectLoongArchCPU(const Triple &TT, StringRef CPU);
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchTargetStreamer.cpp b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchTargetStreamer.cpp
index 30d255f47f396..87710e136fd71 100644
--- a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchTargetStreamer.cpp
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchTargetStreamer.cpp
@@ -212,19 +212,6 @@ LoongArchTargetELFStreamer::LoongArchTargetELFStreamer(MCStreamer &S,
   // cases we don't handle here are covered by LoongArchAsmPrinter.
   Pic = MCA.getContext().getObjectFileInfo()->isPositionIndependent();
 
-  // Set the header flags that we can in the constructor.
-  // FIXME: This is a fairly terrible hack. We set the rest
-  // of these in the destructor. The problem here is two-fold:
-  //
-  // a: Some of the eflags can be set/reset by directives.
-  // b: There aren't any usage paths that initialize the ABI
-  //    pointer until after we initialize either an assembler
-  //    or the target machine.
-  // We can fix this by making the target streamer construct
-  // the ABI, but this is fraught with wide ranging dependency
-  // issues as well.
-  unsigned EFlags = MCA.getELFHeaderEFlags();
-
   // FIXME: Fix a dependency issue by instantiating the ABI object to some
   // default based off the triple. The triple doesn't describe the target
   // fully, but any external user of the API that uses the MCTargetStreamer
@@ -232,11 +219,9 @@ LoongArchTargetELFStreamer::LoongArchTargetELFStreamer(MCStreamer &S,
 
   ABI = LoongArchABIInfo(
       STI.getTargetTriple().getArch() == Triple::ArchType::loongarch32
-          ? LoongArchABIInfo::LP32()
+          ? LoongArchABIInfo::ILP32D()
           : LoongArchABIInfo::LP64D());
 
-  EFlags |= ELF::EF_LARCH_ABI;
-  MCA.setELFHeaderEFlags(EFlags);
 }
 
 void LoongArchTargetELFStreamer::emitLabel(MCSymbol *S) {
@@ -290,12 +275,18 @@ void LoongArchTargetELFStreamer::finish() {
 
   // ABI
   // LP64D does not require any ABI bits.
-  if (getABI().IsLP32())
-    EFlags |= ELF::EF_LARCH_ABI_LP32;
-  else if (getABI().IsLPX32())
-    EFlags |= ELF::EF_LARCH_ABI_XLP32;
-  else
-    EFlags |= ELF::EF_LARCH_ABI_LP64D;
+  if (getABI().IsILP32S())
+    EFlags |= ELF::EF_LARCH_BASE_ABI_ILP32S;
+  else if (getABI().IsILP32F())
+    EFlags |= ELF::EF_LARCH_BASE_ABI_ILP32F;
+  else if (getABI().IsILP32D())
+    EFlags |= ELF::EF_LARCH_BASE_ABI_ILP32D;
+  else if (getABI().IsLP64S())
+    EFlags |= ELF::EF_LARCH_BASE_ABI_LP64S;
+  else if (getABI().IsLP64F())
+    EFlags |= ELF::EF_LARCH_BASE_ABI_LP64F;
+  else if (getABI().IsLP64D())
+    EFlags |= ELF::EF_LARCH_BASE_ABI_LP64D;
 
   MCA.setELFHeaderEFlags(EFlags);
 }
diff --git a/llvm/lib/Transforms/Instrumentation/MemorySanitizer.cpp b/llvm/lib/Transforms/Instrumentation/MemorySanitizer.cpp
index c30c1938c381e..6091e21f1e7bc 100644
--- a/llvm/lib/Transforms/Instrumentation/MemorySanitizer.cpp
+++ b/llvm/lib/Transforms/Instrumentation/MemorySanitizer.cpp
@@ -4229,12 +4229,6 @@ struct VarArgLoongArch64Helper : public VarArgHelper {
       Value *A = *ArgIt;
       Value *Base;
       uint64_t ArgSize = DL.getTypeAllocSize(A->getType());
-      if (TargetTriple.getArch() == Triple::loongarch64) {
-        // Adjusting the shadow for argument with size < 8 to match the placement
-        // of bits in big endian system
-        if (ArgSize < 8)
-          VAArgOffset += (8 - ArgSize);
-      }
       Base = getShadowPtrForVAArgument(A->getType(), IRB, VAArgOffset, ArgSize);
       VAArgOffset += ArgSize;
       VAArgOffset = alignTo(VAArgOffset, 8);
diff --git a/llvm/test/CodeGen/LoongArch/atomic-operand-imm0.ll b/llvm/test/CodeGen/LoongArch/atomic-operand-imm0.ll
new file mode 100644
index 0000000000000..d1d0c0bc42f80
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/atomic-operand-imm0.ll
@@ -0,0 +1,17 @@
+; Test that the last immediate 0 operand of amtomic instruction is printed
+
+; RUN: llc -march=loongarch64 -o - %s | FileCheck %s
+
+define void @test_i32(i32* %dst, i32 %val) {
+; CHECK: ammax_db.wu $r[[REG1:[0-9]+]], $r[[REG2:[0-9]+]], $r[[REG3:[0-9]+]], 0
+entry:
+  %a = atomicrmw umax i32* %dst, i32 %val monotonic
+  ret void
+}
+
+define void @test_i64(i64* %dst, i64 %val) {
+; CHECK: ammax_db.du $r[[REG1:[0-9]+]], $r[[REG2:[0-9]+]], $r[[REG3:[0-9]+]], 0
+entry:
+  %a = atomicrmw umax i64* %dst, i64 %val monotonic
+  ret void
+}
diff --git a/llvm/test/CodeGen/LoongArch/atomic_16_8.ll b/llvm/test/CodeGen/LoongArch/atomic_16_8.ll
new file mode 100644
index 0000000000000..cd09bbef3b0de
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/atomic_16_8.ll
@@ -0,0 +1,833 @@
+; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
+; RUN: llc -march=loongarch64 -o - %s | FileCheck %s
+
+
+define void @umax_8(i8* %ptr) {
+; CHECK-LABEL: umax_8:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    ori $r7, $zero, 255
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB0_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    and $r14, $r10, $r7
+; CHECK-NEXT:    and $r5, $r5, $r7
+; CHECK-NEXT:    sltu $r13, $r14, $r5
+; CHECK-NEXT:    masknez $r11, $r14, $r13
+; CHECK-NEXT:    maskeqz $r13, $r5, $r13
+; CHECK-NEXT:    or $r11, $r11, $r13
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB0_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.b $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw umax i8* %ptr, i8 100 seq_cst
+    ret void
+}
+
+define void @umax_16(i16* %ptr) {
+; CHECK-LABEL: umax_16:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    lu12i.w $r7, 15
+; CHECK-NEXT:    ori $r7, $r7, 4095
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB1_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    and $r14, $r10, $r7
+; CHECK-NEXT:    and $r5, $r5, $r7
+; CHECK-NEXT:    sltu $r13, $r14, $r5
+; CHECK-NEXT:    masknez $r11, $r14, $r13
+; CHECK-NEXT:    maskeqz $r13, $r5, $r13
+; CHECK-NEXT:    or $r11, $r11, $r13
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB1_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.h $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw umax i16* %ptr, i16 100 seq_cst
+    ret void
+}
+
+define void @max_8(i8* %ptr) {
+; CHECK-LABEL: max_8:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    ori $r7, $zero, 255
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB2_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    and $r14, $r10, $r7
+; CHECK-NEXT:    and $r5, $r5, $r7
+; CHECK-NEXT:    slt $r13, $r14, $r5
+; CHECK-NEXT:    masknez $r11, $r14, $r13
+; CHECK-NEXT:    maskeqz $r13, $r5, $r13
+; CHECK-NEXT:    or $r11, $r11, $r13
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB2_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.b $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw max i8* %ptr, i8 100 seq_cst
+    ret void
+}
+
+define void @max_16(i16* %ptr) {
+; CHECK-LABEL: max_16:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    lu12i.w $r7, 15
+; CHECK-NEXT:    ori $r7, $r7, 4095
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB3_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    and $r14, $r10, $r7
+; CHECK-NEXT:    and $r5, $r5, $r7
+; CHECK-NEXT:    slt $r13, $r14, $r5
+; CHECK-NEXT:    masknez $r11, $r14, $r13
+; CHECK-NEXT:    maskeqz $r13, $r5, $r13
+; CHECK-NEXT:    or $r11, $r11, $r13
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB3_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.h $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw max i16* %ptr, i16 100 seq_cst
+    ret void
+}
+
+
+define void @umin_8(i8* %ptr) {
+; CHECK-LABEL: umin_8:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    ori $r7, $zero, 255
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB4_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    and $r14, $r10, $r7
+; CHECK-NEXT:    and $r5, $r5, $r7
+; CHECK-NEXT:    sltu $r13, $r14, $r5
+; CHECK-NEXT:    maskeqz $r11, $r14, $r13
+; CHECK-NEXT:    masknez $r13, $r5, $r13
+; CHECK-NEXT:    or $r11, $r11, $r13
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB4_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.b $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw umin i8* %ptr, i8 100 seq_cst
+    ret void
+}
+
+define void @umin_16(i16* %ptr) {
+; CHECK-LABEL: umin_16:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    lu12i.w $r7, 15
+; CHECK-NEXT:    ori $r7, $r7, 4095
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB5_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    and $r14, $r10, $r7
+; CHECK-NEXT:    and $r5, $r5, $r7
+; CHECK-NEXT:    sltu $r13, $r14, $r5
+; CHECK-NEXT:    maskeqz $r11, $r14, $r13
+; CHECK-NEXT:    masknez $r13, $r5, $r13
+; CHECK-NEXT:    or $r11, $r11, $r13
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB5_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.h $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw umin i16* %ptr, i16 100 seq_cst
+    ret void
+}
+
+define void @min_8(i8* %ptr) {
+; CHECK-LABEL: min_8:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    ori $r7, $zero, 255
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB6_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    and $r14, $r10, $r7
+; CHECK-NEXT:    and $r5, $r5, $r7
+; CHECK-NEXT:    slt $r13, $r14, $r5
+; CHECK-NEXT:    maskeqz $r11, $r14, $r13
+; CHECK-NEXT:    masknez $r13, $r5, $r13
+; CHECK-NEXT:    or $r11, $r11, $r13
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB6_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.b $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw min i8* %ptr, i8 100 seq_cst
+    ret void
+}
+
+define void @min_16(i16* %ptr) {
+; CHECK-LABEL: min_16:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    lu12i.w $r7, 15
+; CHECK-NEXT:    ori $r7, $r7, 4095
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB7_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    and $r14, $r10, $r7
+; CHECK-NEXT:    and $r5, $r5, $r7
+; CHECK-NEXT:    slt $r13, $r14, $r5
+; CHECK-NEXT:    maskeqz $r11, $r14, $r13
+; CHECK-NEXT:    masknez $r13, $r5, $r13
+; CHECK-NEXT:    or $r11, $r11, $r13
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB7_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.h $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw min i16* %ptr, i16 100 seq_cst
+    ret void
+}
+
+
+define void @or_8(i8* %ptr) {
+; CHECK-LABEL: or_8:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    ori $r7, $zero, 255
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB8_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    or $r11, $r10, $r5
+; CHECK-NEXT:    and $r11, $r11, $r7
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB8_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.b $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw or i8* %ptr, i8 100 seq_cst
+    ret void
+}
+
+define void @or_16(i16* %ptr) {
+; CHECK-LABEL: or_16:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    lu12i.w $r7, 15
+; CHECK-NEXT:    ori $r7, $r7, 4095
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB9_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    or $r11, $r10, $r5
+; CHECK-NEXT:    and $r11, $r11, $r7
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB9_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.h $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw or i16* %ptr, i16 100 seq_cst
+    ret void
+}
+
+
+define void @add_8(i8* %ptr) {
+; CHECK-LABEL: add_8:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    ori $r7, $zero, 255
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB10_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    add.w $r11, $r10, $r5
+; CHECK-NEXT:    and $r11, $r11, $r7
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB10_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.b $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw add i8* %ptr, i8 100 seq_cst
+    ret void
+}
+
+define void @add_16(i16* %ptr) {
+; CHECK-LABEL: add_16:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    lu12i.w $r7, 15
+; CHECK-NEXT:    ori $r7, $r7, 4095
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB11_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    add.w $r11, $r10, $r5
+; CHECK-NEXT:    and $r11, $r11, $r7
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB11_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.h $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw add i16* %ptr, i16 100 seq_cst
+    ret void
+}
+
+
+define void @sub_8(i8* %ptr) {
+; CHECK-LABEL: sub_8:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    ori $r7, $zero, 255
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB12_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    sub.w $r11, $r10, $r5
+; CHECK-NEXT:    and $r11, $r11, $r7
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB12_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.b $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw sub i8* %ptr, i8 100 seq_cst
+    ret void
+}
+
+define void @sub_16(i16* %ptr) {
+; CHECK-LABEL: sub_16:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    lu12i.w $r7, 15
+; CHECK-NEXT:    ori $r7, $r7, 4095
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB13_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    sub.w $r11, $r10, $r5
+; CHECK-NEXT:    and $r11, $r11, $r7
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB13_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.h $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw sub i16* %ptr, i16 100 seq_cst
+    ret void
+}
+
+
+define void @and_8(i8* %ptr) {
+; CHECK-LABEL: and_8:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    ori $r7, $zero, 255
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB14_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    and $r11, $r10, $r5
+; CHECK-NEXT:    and $r11, $r11, $r7
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB14_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.b $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw and i8* %ptr, i8 100 seq_cst
+    ret void
+}
+
+define void @and_16(i16* %ptr) {
+; CHECK-LABEL: and_16:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    lu12i.w $r7, 15
+; CHECK-NEXT:    ori $r7, $r7, 4095
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB15_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    and $r11, $r10, $r5
+; CHECK-NEXT:    and $r11, $r11, $r7
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB15_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.h $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw and i16* %ptr, i16 100 seq_cst
+    ret void
+}
+
+
+define void @nand_8(i8* %ptr) {
+; CHECK-LABEL: nand_8:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    ori $r7, $zero, 255
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB16_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    and $r11, $r10, $r5
+; CHECK-NEXT:    nor $r11, $zero, $r11
+; CHECK-NEXT:    and $r11, $r11, $r7
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB16_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.b $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw nand i8* %ptr, i8 100 seq_cst
+    ret void
+}
+
+define void @nand_16(i16* %ptr) {
+; CHECK-LABEL: nand_16:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    lu12i.w $r7, 15
+; CHECK-NEXT:    ori $r7, $r7, 4095
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB17_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    and $r11, $r10, $r5
+; CHECK-NEXT:    nor $r11, $zero, $r11
+; CHECK-NEXT:    and $r11, $r11, $r7
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB17_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.h $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw nand i16* %ptr, i16 100 seq_cst
+    ret void
+}
+
+
+define void @xor_8(i8* %ptr) {
+; CHECK-LABEL: xor_8:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    ori $r7, $zero, 255
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB18_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    xor $r11, $r10, $r5
+; CHECK-NEXT:    and $r11, $r11, $r7
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB18_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.b $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw xor i8* %ptr, i8 100 seq_cst
+    ret void
+}
+
+define void @xor_16(i16* %ptr) {
+; CHECK-LABEL: xor_16:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    lu12i.w $r7, 15
+; CHECK-NEXT:    ori $r7, $r7, 4095
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB19_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    xor $r11, $r10, $r5
+; CHECK-NEXT:    and $r11, $r11, $r7
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB19_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.h $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw xor i16* %ptr, i16 100 seq_cst
+    ret void
+}
+
+
+define void @xchg_8(i8* %ptr) {
+; CHECK-LABEL: xchg_8:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    ori $r7, $zero, 255
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB20_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    and $r11, $r5, $r7
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB20_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.b $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw xchg i8* %ptr, i8 100 seq_cst
+    ret void
+}
+
+define void @xchg_16(i16* %ptr) {
+; CHECK-LABEL: xchg_16:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r6, $zero, -4
+; CHECK-NEXT:    and $r6, $r4, $r6
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    lu12i.w $r7, 15
+; CHECK-NEXT:    ori $r7, $r7, 4095
+; CHECK-NEXT:    sll.w $r7, $r7, $r4
+; CHECK-NEXT:    nor $r8, $zero, $r7
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB21_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r10, $r6, 0
+; CHECK-NEXT:    and $r11, $r5, $r7
+; CHECK-NEXT:    and $r12, $r10, $r8
+; CHECK-NEXT:    or $r12, $r12, $r11
+; CHECK-NEXT:    sc.w $r12, $r6, 0
+; CHECK-NEXT:    beq $r12, $zero, .LBB21_1
+; CHECK-NEXT:  # %bb.2:
+; CHECK-NEXT:    and $r9, $r10, $r7
+; CHECK-NEXT:    srl.w $r9, $r9, $r4
+; CHECK-NEXT:    ext.w.h $r9, $r9
+; CHECK-NEXT:  # %bb.3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = atomicrmw xchg i16* %ptr, i16 100 seq_cst
+    ret void
+}
+
+define void @cmpxchg_8(i8* %ptr) {
+; CHECK-LABEL: cmpxchg_8:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 1
+; CHECK-NEXT:    addi.w $r6, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r7, $zero, -4
+; CHECK-NEXT:    and $r7, $r4, $r7
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    ori $r8, $zero, 255
+; CHECK-NEXT:    sll.w $r8, $r8, $r4
+; CHECK-NEXT:    nor $r9, $zero, $r8
+; CHECK-NEXT:    andi $r6, $r6, 255
+; CHECK-NEXT:    sll.w $r6, $r6, $r4
+; CHECK-NEXT:    andi $r5, $r5, 255
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB22_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r11, $r7, 0
+; CHECK-NEXT:    and $r12, $r11, $r8
+; CHECK-NEXT:    bne $r12, $r6, .LBB22_3
+; CHECK-NEXT:  # %bb.2: # in Loop: Header=BB22_1 Depth=1
+; CHECK-NEXT:    and $r11, $r11, $r9
+; CHECK-NEXT:    or $r11, $r11, $r5
+; CHECK-NEXT:    sc.w $r11, $r7, 0
+; CHECK-NEXT:    beq $r11, $zero, .LBB22_1
+; CHECK-NEXT:  .LBB22_3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    srl.w $r10, $r12, $r4
+; CHECK-NEXT:    ext.w.b $r10, $r10
+; CHECK-NEXT:  # %bb.4:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = cmpxchg i8* %ptr, i8 100, i8 1 seq_cst seq_cst
+    ret void
+}
+
+define void @cmpxchg_16(i16* %ptr) {
+; CHECK-LABEL: cmpxchg_16:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    addi.w $r5, $zero, 1
+; CHECK-NEXT:    addi.w $r6, $zero, 100
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    addi.d $r7, $zero, -4
+; CHECK-NEXT:    and $r7, $r4, $r7
+; CHECK-NEXT:    andi $r4, $r4, 3
+; CHECK-NEXT:    slli.w $r4, $r4, 3
+; CHECK-NEXT:    lu12i.w $r8, 15
+; CHECK-NEXT:    ori $r8, $r8, 4095
+; CHECK-NEXT:    sll.w $r9, $r8, $r4
+; CHECK-NEXT:    nor $r10, $zero, $r9
+; CHECK-NEXT:    and $r6, $r6, $r8
+; CHECK-NEXT:    sll.w $r6, $r6, $r4
+; CHECK-NEXT:    and $r5, $r5, $r8
+; CHECK-NEXT:    sll.w $r5, $r5, $r4
+; CHECK-NEXT:  .LBB23_1: # =>This Inner Loop Header: Depth=1
+; CHECK-NEXT:    ll.w $r11, $r7, 0
+; CHECK-NEXT:    and $r12, $r11, $r9
+; CHECK-NEXT:    bne $r12, $r6, .LBB23_3
+; CHECK-NEXT:  # %bb.2: # in Loop: Header=BB23_1 Depth=1
+; CHECK-NEXT:    and $r11, $r11, $r10
+; CHECK-NEXT:    or $r11, $r11, $r5
+; CHECK-NEXT:    sc.w $r11, $r7, 0
+; CHECK-NEXT:    beq $r11, $zero, .LBB23_1
+; CHECK-NEXT:  .LBB23_3:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    srl.w $r8, $r12, $r4
+; CHECK-NEXT:    ext.w.h $r8, $r8
+; CHECK-NEXT:  # %bb.4:
+; CHECK-NEXT:    dbar 0
+; CHECK-NEXT:    jr $ra
+    %ret = cmpxchg i16* %ptr, i16 100, i16 1 seq_cst seq_cst
+    ret void
+}
diff --git a/llvm/test/CodeGen/LoongArch/disable-tail-calls.ll b/llvm/test/CodeGen/LoongArch/disable-tail-calls.ll
new file mode 100644
index 0000000000000..586daca23c939
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/disable-tail-calls.ll
@@ -0,0 +1,94 @@
+; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
+; RUN: llc -march=loongarch64 -relocation-model=pic  < %s \
+; RUN: | FileCheck %s --check-prefixes=CHECK1
+; RUN: llc -march=loongarch64 -relocation-model=pic -disable-tail-calls  < %s \
+; RUN: | FileCheck %s --check-prefixes=CHECK2
+; RUN: llc -march=loongarch64 -relocation-model=pic -disable-tail-calls=false  < %s \
+; RUN: | FileCheck %s --check-prefixes=CHECK3
+
+; Function with attribute #0 = { "disable-tail-calls"="true" }
+define i32 @caller1(i32 %a) #0 {
+; CHECK1-LABEL: caller1:
+; CHECK1:       # %bb.0: # %entry
+; CHECK1-NEXT:    addi.d $sp, $sp, -16
+; CHECK1-NEXT:    .cfi_def_cfa_offset 16
+; CHECK1-NEXT:    st.d $ra, $sp, 8 # 8-byte Folded Spill
+; CHECK1-NEXT:    .cfi_offset 1, -8
+; CHECK1-NEXT:    bl callee
+; CHECK1-NEXT:    ld.d $ra, $sp, 8 # 8-byte Folded Reload
+; CHECK1-NEXT:    addi.d $sp, $sp, 16
+; CHECK1-NEXT:    jr $ra
+;
+; CHECK2-LABEL: caller1:
+; CHECK2:       # %bb.0: # %entry
+; CHECK2-NEXT:    addi.d $sp, $sp, -16
+; CHECK2-NEXT:    .cfi_def_cfa_offset 16
+; CHECK2-NEXT:    st.d $ra, $sp, 8 # 8-byte Folded Spill
+; CHECK2-NEXT:    .cfi_offset 1, -8
+; CHECK2-NEXT:    bl callee
+; CHECK2-NEXT:    ld.d $ra, $sp, 8 # 8-byte Folded Reload
+; CHECK2-NEXT:    addi.d $sp, $sp, 16
+; CHECK2-NEXT:    jr $ra
+;
+; CHECK3-LABEL: caller1:
+; CHECK3:       # %bb.0: # %entry
+; CHECK3-NEXT:    b callee
+entry:
+  %call = tail call i32 @callee(i32 %a)
+  ret i32 %call
+}
+
+
+; Function with attribute #1 = { "disable-tail-calls"="false" }
+define i32 @caller2(i32 %a) #1 {
+; CHECK1-LABEL: caller2:
+; CHECK1:       # %bb.0: # %entry
+; CHECK1-NEXT:    b callee
+;
+; CHECK2-LABEL: caller2:
+; CHECK2:       # %bb.0: # %entry
+; CHECK2-NEXT:    addi.d $sp, $sp, -16
+; CHECK2-NEXT:    .cfi_def_cfa_offset 16
+; CHECK2-NEXT:    st.d $ra, $sp, 8 # 8-byte Folded Spill
+; CHECK2-NEXT:    .cfi_offset 1, -8
+; CHECK2-NEXT:    bl callee
+; CHECK2-NEXT:    ld.d $ra, $sp, 8 # 8-byte Folded Reload
+; CHECK2-NEXT:    addi.d $sp, $sp, 16
+; CHECK2-NEXT:    jr $ra
+;
+; CHECK3-LABEL: caller2:
+; CHECK3:       # %bb.0: # %entry
+; CHECK3-NEXT:    b callee
+entry:
+  %call = tail call i32 @callee(i32 %a)
+  ret i32 %call
+}
+
+define i32 @caller3(i32 %a) {
+; CHECK1-LABEL: caller3:
+; CHECK1:       # %bb.0: # %entry
+; CHECK1-NEXT:    b callee
+;
+; CHECK2-LABEL: caller3:
+; CHECK2:       # %bb.0: # %entry
+; CHECK2-NEXT:    addi.d $sp, $sp, -16
+; CHECK2-NEXT:    .cfi_def_cfa_offset 16
+; CHECK2-NEXT:    st.d $ra, $sp, 8 # 8-byte Folded Spill
+; CHECK2-NEXT:    .cfi_offset 1, -8
+; CHECK2-NEXT:    bl callee
+; CHECK2-NEXT:    ld.d $ra, $sp, 8 # 8-byte Folded Reload
+; CHECK2-NEXT:    addi.d $sp, $sp, 16
+; CHECK2-NEXT:    jr $ra
+;
+; CHECK3-LABEL: caller3:
+; CHECK3:       # %bb.0: # %entry
+; CHECK3-NEXT:    b callee
+entry:
+  %call = tail call i32 @callee(i32 %a)
+  ret i32 %call
+}
+
+declare i32 @callee(i32)
+
+attributes #0 = { "disable-tail-calls"="true" }
+attributes #1 = { "disable-tail-calls"="false" }
diff --git a/llvm/test/CodeGen/LoongArch/fcopysign.ll b/llvm/test/CodeGen/LoongArch/fcopysign.ll
new file mode 100644
index 0000000000000..7d8d6a9bf3dfa
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/fcopysign.ll
@@ -0,0 +1,17 @@
+; RUN: llc -march=loongarch64 -mattr=+d -o - %s | FileCheck %s
+
+define float @fcopysign_s(float %a, float %b) {
+; CHECK-LABEL: fcopysign_s:
+; CHECK:    fcopysign.s $f0, $f0, $f1
+  %ret = call float @llvm.copysign.f32(float %a, float %b)
+  ret float %ret
+}
+declare float @llvm.copysign.f32(float %a, float %b)
+
+define double @fcopysign_d(double %a, double %b) {
+; CHECK-LABEL: fcopysign_d:
+; CHECK:    fcopysign.d $f0, $f0, $f1
+  %ret = call double @llvm.copysign.f64(double %a, double %b)
+  ret double %ret
+}
+declare double @llvm.copysign.f64(double %a, double %b)
diff --git a/llvm/test/CodeGen/LoongArch/logic-op.ll b/llvm/test/CodeGen/LoongArch/logic-op.ll
new file mode 100644
index 0000000000000..c1029c1ff2469
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/logic-op.ll
@@ -0,0 +1,171 @@
+; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
+; RUN: llc -march=loongarch64  < %s | FileCheck %s
+
+define signext i32 @foo32(i32 signext %a) {
+; CHECK-LABEL: foo32:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    sltui $r4, $r4, 1
+; CHECK-NEXT:    jr $ra
+entry:
+  %tobool = icmp eq i32 %a, 0
+  %conv = zext i1 %tobool to i32
+  ret i32 %conv
+}
+
+define i64 @foo(i64 %a) {
+; CHECK-LABEL: foo:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    sltui $r4, $r4, 1
+; CHECK-NEXT:    jr $ra
+entry:
+  %tobool = icmp eq i64 %a, 0
+  %conv = zext i1 %tobool to i64
+  ret i64 %conv
+}
+
+define i64 @not(i64 %a) {
+; CHECK-LABEL: not:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    nor $r4, $zero, $r4
+; CHECK-NEXT:    jr $ra
+entry:
+  %not = xor i64 %a, -1
+  ret i64 %not
+}
+
+define i64 @and(i64 %a, i64 %b) {
+; CHECK-LABEL: and:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    and $r4, $r5, $r4
+; CHECK-NEXT:    jr $ra
+entry:
+  %and = and i64 %b, %a
+  ret i64 %and
+}
+
+define i64 @or(i64 %a, i64 %b) {
+; CHECK-LABEL: or:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    or $r4, $r5, $r4
+; CHECK-NEXT:    jr $ra
+entry:
+  %or = or i64 %b, %a
+  ret i64 %or
+}
+
+define i64 @xor(i64 %a, i64 %b) {
+; CHECK-LABEL: xor:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    xor $r4, $r5, $r4
+; CHECK-NEXT:    jr $ra
+entry:
+  %xor = xor i64 %b, %a
+  ret i64 %xor
+}
+
+define i64 @nor(i64 %a, i64 %b) {
+; CHECK-LABEL: nor:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    nor $r4, $r5, $r4
+; CHECK-NEXT:    jr $ra
+entry:
+  %or = or i64 %b, %a
+  %not = xor i64 %or, -1
+  ret i64 %not
+}
+
+define i64 @andn(i64 %a, i64 %b) {
+; CHECK-LABEL: andn:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    andn $r4, $r4, $r5
+; CHECK-NEXT:    jr $ra
+entry:
+  %not = xor i64 %b, -1
+  %and = and i64 %not, %a
+  ret i64 %and
+}
+
+define signext i32 @andn32(i32 signext %a, i32 signext %b) {
+; CHECK-LABEL: andn32:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    andn $r4, $r4, $r5
+; CHECK-NEXT:    jr $ra
+entry:
+  %not = xor i32 %b, -1
+  %and = and i32 %not, %a
+  ret i32 %and
+}
+
+define i64 @orn(i64 %a, i64 %b) {
+; CHECK-LABEL: orn:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    orn $r4, $r4, $r5
+; CHECK-NEXT:    jr $ra
+entry:
+  %not = xor i64 %b, -1
+  %or = or i64 %not, %a
+  ret i64 %or
+}
+
+define signext i32 @orn32(i32 signext %a, i32 signext %b) {
+; CHECK-LABEL: orn32:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    orn $r4, $r4, $r5
+; CHECK-NEXT:    jr $ra
+entry:
+  %not = xor i32 %b, -1
+  %or = or i32 %not, %a
+  ret i32 %or
+}
+
+define signext i32 @and32(i32 signext %a, i32 signext %b) {
+; CHECK-LABEL: and32:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    and $r4, $r5, $r4
+; CHECK-NEXT:    jr $ra
+entry:
+  %and = and i32 %b, %a
+  ret i32 %and
+}
+
+define signext i32 @or32(i32 signext %a, i32 signext %b) {
+; CHECK-LABEL: or32:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    or $r4, $r5, $r4
+; CHECK-NEXT:    jr $ra
+entry:
+  %or = or i32 %b, %a
+  ret i32 %or
+}
+
+define signext i32 @xor32(i32 signext %a, i32 signext %b) {
+; CHECK-LABEL: xor32:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    xor $r4, $r5, $r4
+; CHECK-NEXT:    jr $ra
+entry:
+  %xor = xor i32 %b, %a
+  ret i32 %xor
+}
+
+define signext i32 @nor32(i32 signext %a, i32 signext %b) {
+; CHECK-LABEL: nor32:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    nor $r4, $r4, $r5
+; CHECK-NEXT:    jr $ra
+entry:
+  %or = or i32 %b, %a
+  %not = xor i32 %or, -1
+  ret i32 %not
+}
+
+define signext i32 @not32(i32 signext %a) {
+; CHECK-LABEL: not32:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    nor $r4, $zero, $r4
+; CHECK-NEXT:    jr $ra
+entry:
+  %not = xor i32 %a, -1
+  ret i32 %not
+}
+
diff --git a/llvm/test/CodeGen/LoongArch/signext.ll b/llvm/test/CodeGen/LoongArch/signext.ll
new file mode 100644
index 0000000000000..057be466ed1e7
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/signext.ll
@@ -0,0 +1,9 @@
+; RUN: llc -march=loongarch64 < %s | FileCheck %s
+
+define i32 @foo(i32 signext %a) {
+; CHECK-LABEL: foo:
+; CHECK:       # %bb.0:
+; CHECK-NEXT:    slli.w $r4, $r4, 0
+; CHECK-NEXT:    jr $ra
+  ret i32 %a
+}
diff --git a/llvm/test/CodeGen/LoongArch/tailcall-R.ll b/llvm/test/CodeGen/LoongArch/tailcall-R.ll
new file mode 100644
index 0000000000000..688d818ad18ab
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/tailcall-R.ll
@@ -0,0 +1,62 @@
+; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
+; RUN: llc -march=loongarch64 -relocation-model=pic < %s | FileCheck %s
+
+@errors = external local_unnamed_addr global i32, align 4
+
+define signext i32 @compare(i8* %x, i8* %y) {
+; CHECK-LABEL: compare:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    addi.d $sp, $sp, -32
+; CHECK-NEXT:    .cfi_def_cfa_offset 32
+; CHECK-NEXT:    st.d $ra, $sp, 24 # 8-byte Folded Spill
+; CHECK-NEXT:    st.d $r23, $sp, 16 # 8-byte Folded Spill
+; CHECK-NEXT:    .cfi_offset 1, -8
+; CHECK-NEXT:    .cfi_offset 23, -16
+; CHECK-NEXT:    ld.w $r23, $r5, 0
+; CHECK-NEXT:    ld.d $r6, $r4, 8
+; CHECK-NEXT:    beqz $r23, .LBB0_3
+; CHECK-NEXT:  # %bb.1: # %land.lhs.true
+; CHECK-NEXT:    ld.w $r4, $r4, 0
+; CHECK-NEXT:    st.d $r6, $sp, 8 # 8-byte Folded Spill
+; CHECK-NEXT:    ld.d $r5, $sp, 8 # 8-byte Folded Reload
+; CHECK-NEXT:    jirl $ra, $r5, 0
+; CHECK-NEXT:    ld.d $r6, $sp, 8 # 8-byte Folded Reload
+; CHECK-NEXT:    beqz $r4, .LBB0_3
+; CHECK-NEXT:  # %bb.2: # %if.then
+; CHECK-NEXT:    la.got $r4, errors
+; CHECK-NEXT:    # la expanded slot
+; CHECK-NEXT:    ld.w $r5, $r4, 0
+; CHECK-NEXT:    addi.w $r5, $r5, 1
+; CHECK-NEXT:    st.w $r5, $r4, 0
+; CHECK-NEXT:  .LBB0_3: # %if.end
+; CHECK-NEXT:    slli.w $r4, $r23, 0
+; CHECK-NEXT:    ld.d $r23, $sp, 16 # 8-byte Folded Reload
+; CHECK-NEXT:    ld.d $ra, $sp, 24 # 8-byte Folded Reload
+; CHECK-NEXT:    addi.d $sp, $sp, 32
+; CHECK-NEXT:    jr $r6
+entry:
+  %compare = getelementptr inbounds i8, i8* %x, i64 8
+  %0 = bitcast i8* %compare to i32 (i32)**
+  %1 = load i32 (i32)*, i32 (i32)** %0, align 8
+  %elt = bitcast i8* %y to i32*
+  %2 = load i32, i32* %elt, align 8
+  %cmp = icmp eq i32 %2, 0
+  br i1 %cmp, label %if.end, label %land.lhs.true
+
+land.lhs.true:                                    ; preds = %entry
+  %elt3 = bitcast i8* %x to i32*
+  %3 = load i32, i32* %elt3, align 8
+  %call4 = tail call signext i32 %1(i32 signext %3)
+  %cmp5 = icmp eq i32 %call4, 0
+  br i1 %cmp5, label %if.end, label %if.then
+
+if.then:                                          ; preds = %land.lhs.true
+  %4 = load i32, i32* @errors, align 4
+  %inc = add nsw i32 %4, 1
+  store i32 %inc, i32* @errors, align 4
+  br label %if.end
+
+if.end:                                           ; preds = %if.then, %land.lhs.true, %entry
+  %call6 = tail call signext i32 %1(i32 signext %2)
+  ret i32 %call6
+}
diff --git a/llvm/test/CodeGen/LoongArch/tailcall-check.ll b/llvm/test/CodeGen/LoongArch/tailcall-check.ll
new file mode 100644
index 0000000000000..8cf2f9f915d68
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/tailcall-check.ll
@@ -0,0 +1,155 @@
+; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
+; RUN: llc -march=loongarch64 -relocation-model=pic < %s | FileCheck %s
+
+; Perform tail call optimization for global address.
+declare i32 @callee_tail(i32 %i)
+define i32 @caller_tail(i32 %i) {
+; CHECK-LABEL: caller_tail:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    b callee_tail
+entry:
+  %r = tail call i32 @callee_tail(i32 %i)
+  ret i32 %r
+}
+
+
+; Do not tail call optimize functions with varargs.
+declare i32 @callee_varargs(i32, ...)
+define void @caller_varargs(i32 %a, i32 %b) {
+; CHECK-LABEL: caller_varargs:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    addi.d $sp, $sp, -16
+; CHECK-NEXT:    .cfi_def_cfa_offset 16
+; CHECK-NEXT:    st.d $ra, $sp, 8 # 8-byte Folded Spill
+; CHECK-NEXT:    .cfi_offset 1, -8
+; CHECK-NEXT:    move $r6, $r5
+; CHECK-NEXT:    move $r7, $r4
+; CHECK-NEXT:    bl callee_varargs
+; CHECK-NEXT:    ld.d $ra, $sp, 8 # 8-byte Folded Reload
+; CHECK-NEXT:    addi.d $sp, $sp, 16
+; CHECK-NEXT:    jr $ra
+entry:
+  %call = tail call i32 (i32, ...) @callee_varargs(i32 %a, i32 %b, i32 %b, i32 %a)
+  ret void
+}
+
+
+; Do not tail call optimize if stack is used to pass parameters.
+declare i32 @callee_args(i32 %a, i32 %b, i32 %c, i32 %dd, i32 %e, i32 %ff, i32 %g, i32 %h, i32 %i, i32 %j, i32 %k, i32 %l, i32 %m, i32 %n)
+define i32 @caller_args(i32 %a, i32 %b, i32 %c, i32 %dd, i32 %e, i32 %ff, i32 %g, i32 %h, i32 %i, i32 %j, i32 %k, i32 %l, i32 %m, i32 %n) {
+; CHECK-LABEL: caller_args:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    addi.d $sp, $sp, -64
+; CHECK-NEXT:    .cfi_def_cfa_offset 64
+; CHECK-NEXT:    st.d $ra, $sp, 56 # 8-byte Folded Spill
+; CHECK-NEXT:    .cfi_offset 1, -8
+; CHECK-NEXT:    ld.d $r12, $sp, 64
+; CHECK-NEXT:    ld.d $r13, $sp, 72
+; CHECK-NEXT:    ld.d $r14, $sp, 80
+; CHECK-NEXT:    ld.d $r15, $sp, 88
+; CHECK-NEXT:    ld.d $r16, $sp, 96
+; CHECK-NEXT:    ld.d $r17, $sp, 104
+; CHECK-NEXT:    st.d $r17, $sp, 40
+; CHECK-NEXT:    st.d $r16, $sp, 32
+; CHECK-NEXT:    st.d $r15, $sp, 24
+; CHECK-NEXT:    st.d $r14, $sp, 16
+; CHECK-NEXT:    st.d $r13, $sp, 8
+; CHECK-NEXT:    st.d $r12, $sp, 0
+; CHECK-NEXT:    bl callee_args
+; CHECK-NEXT:    ld.d $ra, $sp, 56 # 8-byte Folded Reload
+; CHECK-NEXT:    addi.d $sp, $sp, 64
+; CHECK-NEXT:    jr $ra
+entry:
+  %r = tail call i32 @callee_args(i32 %a, i32 %b, i32 %c, i32 %dd, i32 %e, i32 %ff, i32 %g, i32 %h, i32 %i, i32 %j, i32 %k, i32 %l, i32 %m, i32 %n)
+  ret i32 %r
+}
+
+
+; Do not tail call optimize for exception-handling functions.
+declare void @callee_interrupt()
+define void @caller_interrupt() #0 {
+; CHECK-LABEL: caller_interrupt:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    addi.d $sp, $sp, -16
+; CHECK-NEXT:    .cfi_def_cfa_offset 16
+; CHECK-NEXT:    st.d $ra, $sp, 8 # 8-byte Folded Spill
+; CHECK-NEXT:    .cfi_offset 1, -8
+; CHECK-NEXT:    bl callee_interrupt
+; CHECK-NEXT:    ld.d $ra, $sp, 8 # 8-byte Folded Reload
+; CHECK-NEXT:    addi.d $sp, $sp, 16
+; CHECK-NEXT:    jr $ra
+entry:
+  tail call void @callee_interrupt()
+  ret void
+}
+attributes #0 = { "interrupt"="machine" }
+
+
+; Do not tail call optimize functions with byval parameters.
+declare i32 @callee_byval(i32** byval %a)
+define i32 @caller_byval() {
+; CHECK-LABEL: caller_byval:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    addi.d $sp, $sp, -32
+; CHECK-NEXT:    .cfi_def_cfa_offset 32
+; CHECK-NEXT:    st.d $ra, $sp, 24 # 8-byte Folded Spill
+; CHECK-NEXT:    .cfi_offset 1, -8
+; CHECK-NEXT:    ld.d $r4, $sp, 16
+; CHECK-NEXT:    st.d $r4, $sp, 0
+; CHECK-NEXT:    bl callee_byval
+; CHECK-NEXT:    ld.d $ra, $sp, 24 # 8-byte Folded Reload
+; CHECK-NEXT:    addi.d $sp, $sp, 32
+; CHECK-NEXT:    jr $ra
+entry:
+  %a = alloca i32*
+  %r = tail call i32 @callee_byval(i32** byval %a)
+  ret i32 %r
+}
+
+
+; Do not tail call optimize if callee uses structret semantics.
+%struct.A = type { i32 }
+@a = global %struct.A zeroinitializer
+
+declare void @callee_struct(%struct.A* sret %a)
+define void @caller_nostruct() {
+; CHECK-LABEL: caller_nostruct:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    addi.d $sp, $sp, -16
+; CHECK-NEXT:    .cfi_def_cfa_offset 16
+; CHECK-NEXT:    st.d $ra, $sp, 8 # 8-byte Folded Spill
+; CHECK-NEXT:    .cfi_offset 1, -8
+; CHECK-NEXT:    la.got $r4, a
+; CHECK-NEXT:    # la expanded slot
+; CHECK-NEXT:    bl callee_struct
+; CHECK-NEXT:    ld.d $ra, $sp, 8 # 8-byte Folded Reload
+; CHECK-NEXT:    addi.d $sp, $sp, 16
+; CHECK-NEXT:    jr $ra
+entry:
+  tail call void @callee_struct(%struct.A* sret @a)
+  ret void
+}
+
+
+; Do not tail call optimize if caller uses structret semantics.
+declare void @callee_nostruct()
+define void @caller_struct(%struct.A* sret %a) {
+; CHECK-LABEL: caller_struct:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    addi.d $sp, $sp, -16
+; CHECK-NEXT:    .cfi_def_cfa_offset 16
+; CHECK-NEXT:    st.d $ra, $sp, 8 # 8-byte Folded Spill
+; CHECK-NEXT:    st.d $r23, $sp, 0 # 8-byte Folded Spill
+; CHECK-NEXT:    .cfi_offset 1, -8
+; CHECK-NEXT:    .cfi_offset 23, -16
+; CHECK-NEXT:    move $r23, $r4
+; CHECK-NEXT:    bl callee_nostruct
+; CHECK-NEXT:    move $r4, $r23
+; CHECK-NEXT:    ld.d $r23, $sp, 0 # 8-byte Folded Reload
+; CHECK-NEXT:    ld.d $ra, $sp, 8 # 8-byte Folded Reload
+; CHECK-NEXT:    addi.d $sp, $sp, 16
+; CHECK-NEXT:    jr $ra
+entry:
+  tail call void @callee_nostruct()
+  ret void
+}
diff --git a/llvm/test/CodeGen/LoongArch/tailcall-mem.ll b/llvm/test/CodeGen/LoongArch/tailcall-mem.ll
new file mode 100644
index 0000000000000..68ddaa8997b0a
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/tailcall-mem.ll
@@ -0,0 +1,35 @@
+; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
+; RUN: llc -march=loongarch64 -relocation-model=pic < %s | FileCheck %s
+
+
+define void @tail_memcpy(i8* %p, i8* %q, i32 %n) {
+; CHECK-LABEL: tail_memcpy:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    b memcpy
+entry:
+  tail call void @llvm.memcpy.p0i8.p0i8.i32(i8* %p, i8* %q, i32 %n, i1 false)
+  ret void
+}
+
+define void @tail_memmove(i8* %p, i8* %q, i32 %n) {
+; CHECK-LABEL: tail_memmove:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    b memmove
+entry:
+  tail call void @llvm.memmove.p0i8.p0i8.i32(i8* %p, i8* %q, i32 %n, i1 false)
+  ret void
+}
+
+define void @tail_memset(i8* %p, i8 %c, i32 %n) {
+; CHECK-LABEL: tail_memset:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    b memset
+entry:
+  tail call void @llvm.memset.p0i8.i32(i8* %p, i8 %c, i32 %n, i1 false)
+  ret void
+}
+
+declare void @llvm.memcpy.p0i8.p0i8.i32(i8* nocapture, i8* nocapture readonly, i32, i1)
+declare void @llvm.memmove.p0i8.p0i8.i32(i8* nocapture, i8* nocapture readonly, i32, i1)
+declare void @llvm.memset.p0i8.i32(i8* nocapture, i8, i32, i1)
+
diff --git a/llvm/test/CodeGen/LoongArch/tailcall.ll b/llvm/test/CodeGen/LoongArch/tailcall.ll
new file mode 100644
index 0000000000000..984df2cb63d15
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/tailcall.ll
@@ -0,0 +1,13 @@
+; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
+; RUN: llc -march=loongarch64 -relocation-model=pic < %s | FileCheck %s
+
+define void @f() {
+; CHECK-LABEL: f:
+; CHECK:       # %bb.0: # %entry
+; CHECK-NEXT:    b foo
+entry:
+  tail call void bitcast (void (...)* @foo to void ()*)()
+  ret void
+}
+
+declare void @foo(...)
diff --git a/llvm/test/CodeGen/LoongArch/target-feature-double.ll b/llvm/test/CodeGen/LoongArch/target-feature-double.ll
new file mode 100644
index 0000000000000..814d130723b00
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/target-feature-double.ll
@@ -0,0 +1,31 @@
+; RUN: llc -march=loongarch64 -target-abi lp64d --relocation-model=pic < %s \
+; RUN:   | FileCheck -check-prefix=ATTRN-F-DNF %s
+; RUN: llc -march=loongarch64 -target-abi lp64d -mattr=+d < %s \
+; RUN:   | FileCheck -check-prefix=ATTRD-DF-FD-NFD %s
+; RUN: llc -march=loongarch64 -target-abi lp64d -mattr=+f --relocation-model=pic < %s \
+; RUN:   | FileCheck -check-prefix=ATTRN-F-DNF %s
+; RUN: llc -march=loongarch64 -target-abi lp64d -mattr=+d,+f < %s \
+; RUN:   | FileCheck -check-prefix=ATTRD-DF-FD-NFD %s
+; RUN: llc -march=loongarch64 -target-abi lp64d -mattr=+f,+d < %s \
+; RUN:   | FileCheck -check-prefix=ATTRD-DF-FD-NFD %s
+; RUN: llc -march=loongarch64 -target-abi lp64d -mattr=+d,-f --relocation-model=pic < %s \
+; RUN:   | FileCheck -check-prefix=ATTRN-F-DNF %s
+; RUN: llc -march=loongarch64 -target-abi lp64d -mattr=-f,+d < %s \
+; RUN:   | FileCheck -check-prefix=ATTRD-DF-FD-NFD %s
+
+define double @test(double %a, double %b) {
+; ATTRN-F-DNF-LABEL: test:
+; ATTRN-F-DNF:       # %bb.0: # %entry
+; ATTRN-F-DNF:    addi.d $sp, $sp, -16
+; ATTRN-F-DNF:    bl __adddf3
+; ATTRN-F-DNF:    addi.d $sp, $sp, 16
+; ATTRN-F-DNF:    jr $ra
+;
+; ATTRD-DF-FD-NFD-LABEL: test:
+; ATTRD-DF-FD-NFD:       # %bb.0: # %entry
+; ATTRD-DF-FD-NFD:    fadd.d $f0, $f0, $f1
+; ATTRD-DF-FD-NFD:    jr $ra
+entry:
+  %add = fadd double %a, %b
+  ret double %add
+}
diff --git a/llvm/test/CodeGen/LoongArch/target-feature-float.ll b/llvm/test/CodeGen/LoongArch/target-feature-float.ll
new file mode 100644
index 0000000000000..fd0ba21c6371b
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/target-feature-float.ll
@@ -0,0 +1,35 @@
+; RUN: llc -march=loongarch64 -target-abi lp64d --relocation-model=pic < %s \
+; RUN:   | FileCheck -check-prefix=ATTRN-DNF %s
+; RUN: llc -march=loongarch64 -target-abi lp64d -mattr=+d < %s \
+; RUN:   | FileCheck -check-prefix=ATTRD-F-DF-FD-NFD %s
+; RUN: llc -march=loongarch64 -target-abi lp64d -mattr=+f < %s \
+; RUN:   | FileCheck -check-prefix=ATTRD-F-DF-FD-NFD %s
+; RUN: llc -march=loongarch64 -target-abi lp64d -mattr=+d,+f < %s \
+; RUN:   | FileCheck -check-prefix=ATTRD-F-DF-FD-NFD %s
+; RUN: llc -march=loongarch64 -target-abi lp64d -mattr=+f,+d < %s \
+; RUN:   | FileCheck -check-prefix=ATTRD-F-DF-FD-NFD %s
+; RUN: llc -march=loongarch64 -target-abi lp64d -mattr=+d,-f --relocation-model=pic < %s \
+; RUN:   | FileCheck -check-prefix=ATTRN-DNF %s
+; RUN: llc -march=loongarch64 -target-abi lp64d -mattr=-f,+d < %s \
+; RUN:   | FileCheck -check-prefix=ATTRD-F-DF-FD-NFD %s
+
+define float @test(float %a, float %b) {
+; ATTRN-DNF-LABEL: test:
+; ATTRN-DNF:       # %bb.0: # %entry
+; ATTRN-DNF:    addi.d $sp, $sp, -16
+; ATTRN-DNF:    st.d $ra, $sp, 8 # 8-byte Folded Spill
+; ATTRN-DNF:    slli.w $r4, $r4, 0
+; ATTRN-DNF:    slli.w $r5, $r5, 0
+; ATTRN-DNF:    bl __addsf3
+; ATTRN-DNF:    ld.d $ra, $sp, 8 # 8-byte Folded Reload
+; ATTRN-DNF:    addi.d $sp, $sp, 16
+; ATTRN-DNF:    jr $ra
+;
+; ATTRD-F-DF-FD-NFD-LABEL: test:
+; ATTRD-F-DF-FD-NFD:       # %bb.0: # %entry
+; ATTRD-F-DF-FD-NFD:    fadd.s $f0, $f0, $f1
+; ATTRD-F-DF-FD-NFD:    jr $ra
+entry:
+  %add = fadd float %a, %b
+  ret float %add
+}
diff --git a/llvm/test/MC/LoongArch/invalid.s b/llvm/test/MC/LoongArch/invalid.s
index eea01f6707d36..e0fc7ce4b2021 100644
--- a/llvm/test/MC/LoongArch/invalid.s
+++ b/llvm/test/MC/LoongArch/invalid.s
@@ -2,3 +2,49 @@
 .text
 csrxchg        $r6, $r0, 214                # CHECK: :[[@LINE]]:1: error: invalid operand ($zero) for instruction
 csrxchg        $r6, $r1, 214                # CHECK: :[[@LINE]]:1: error: invalid operand ($r1) for instruction
+
+## out-of-bound immediate
+### simm16 << 2
+beq            $r10, $r7, -0x20000-4           # CHECK: :[[@LINE]]:1: error: branch target out of range
+beq            $r10, $r7,  0x1FFFC+4           # CHECK: :[[@LINE]]:1: error: branch target out of range
+bne            $r10, $r7, -0x20000-4           # CHECK: :[[@LINE]]:1: error: branch target out of range
+bne            $r10, $r7,  0x1FFFC+4           # CHECK: :[[@LINE]]:1: error: branch target out of range
+blt            $r10, $r7, -0x20000-4           # CHECK: :[[@LINE]]:1: error: branch target out of range
+blt            $r10, $r7,  0x1FFFC+4           # CHECK: :[[@LINE]]:1: error: branch target out of range
+bge            $r10, $r7, -0x20000-4           # CHECK: :[[@LINE]]:1: error: branch target out of range
+bge            $r10, $r7,  0x1FFFC+4           # CHECK: :[[@LINE]]:1: error: branch target out of range
+bltu           $r10, $r7, -0x20000-4           # CHECK: :[[@LINE]]:1: error: branch target out of range
+bltu           $r10, $r7,  0x1FFFC+4           # CHECK: :[[@LINE]]:1: error: branch target out of range
+bgeu           $r10, $r7, -0x20000-4           # CHECK: :[[@LINE]]:1: error: branch target out of range
+bgeu           $r10, $r7,  0x1FFFC+4           # CHECK: :[[@LINE]]:1: error: branch target out of range
+### simm21 << 2
+beqz           $r9, -0x400000-4                # CHECK: :[[@LINE]]:1: error: branch target out of range
+beqz           $r9,  0x3FFFFC+4                # CHECK: :[[@LINE]]:1: error: branch target out of range
+bnez           $r9, -0x400000-4                # CHECK: :[[@LINE]]:1: error: branch target out of range
+bnez           $r9,  0x3FFFFC+4                # CHECK: :[[@LINE]]:1: error: branch target out of range
+bceqz          $fcc6, -0x400000-4              # CHECK: :[[@LINE]]:1: error: branch target out of range
+bceqz          $fcc6,  0x3FFFFC+4              # CHECK: :[[@LINE]]:1: error: branch target out of range
+bcnez          $fcc6, -0x400000-4              # CHECK: :[[@LINE]]:1: error: branch target out of range
+bcnez          $fcc6,  0x3FFFFC+4              # CHECK: :[[@LINE]]:1: error: branch target out of range
+### simm26 << 2
+b              -0x8000000-4                    # CHECK: :[[@LINE]]:1: error: branch target out of range
+b               0x7FFFFFC+4                    # CHECK: :[[@LINE]]:1: error: branch target out of range
+bl             -0x8000000-4                    # CHECK: :[[@LINE]]:1: error: branch target out of range
+bl              0x7FFFFFC+4                    # CHECK: :[[@LINE]]:1: error: branch target out of range
+
+## unaligned immediate
+### simm16 << 2
+beq            $r10, $r7,  0x1FFFC+1           # CHECK: :[[@LINE]]:1: error: branch to misaligned address
+bne            $r10, $r7,  0x1FFFC+1           # CHECK: :[[@LINE]]:1: error: branch to misaligned address
+blt            $r10, $r7,  0x1FFFC+1           # CHECK: :[[@LINE]]:1: error: branch to misaligned address
+bge            $r10, $r7,  0x1FFFC+1           # CHECK: :[[@LINE]]:1: error: branch to misaligned address
+bltu           $r10, $r7,  0x1FFFC+1           # CHECK: :[[@LINE]]:1: error: branch to misaligned address
+bgeu           $r10, $r7,  0x1FFFC+1           # CHECK: :[[@LINE]]:1: error: branch to misaligned address
+### simm21 << 2
+beqz           $r9,  0x3FFFFC+1                # CHECK: :[[@LINE]]:1: error: branch to misaligned address
+bnez           $r9,  0x3FFFFC+1                # CHECK: :[[@LINE]]:1: error: branch to misaligned address
+bceqz          $fcc6,  0x3FFFFC+1              # CHECK: :[[@LINE]]:1: error: branch to misaligned address
+bcnez          $fcc6,  0x3FFFFC+1              # CHECK: :[[@LINE]]:1: error: branch to misaligned address
+### simm26 << 2
+b               0x7FFFFFC+1                    # CHECK: :[[@LINE]]:1: error: branch to misaligned address
+bl              0x7FFFFFC+1                    # CHECK: :[[@LINE]]:1: error: branch to misaligned address
diff --git a/llvm/test/MC/LoongArch/lit.local.cfg b/llvm/test/MC/LoongArch/lit.local.cfg
new file mode 100644
index 0000000000000..6223fc691edc4
--- /dev/null
+++ b/llvm/test/MC/LoongArch/lit.local.cfg
@@ -0,0 +1,3 @@
+if not 'LoongArch' in config.root.targets:
+    config.unsupported = True
+
diff --git a/llvm/test/MC/LoongArch/target-abi-valid.s b/llvm/test/MC/LoongArch/target-abi-valid.s
new file mode 100644
index 0000000000000..eb8fc09935a57
--- /dev/null
+++ b/llvm/test/MC/LoongArch/target-abi-valid.s
@@ -0,0 +1,23 @@
+# RUN: llvm-mc -triple loongarch64 -filetype=obj < %s \
+# RUN:   | llvm-readelf -h - \
+# RUN:   | FileCheck -check-prefix=CHECK-NONE %s
+
+# RUN: llvm-mc -triple loongarch64 -target-abi lp64s -filetype=obj < %s \
+# RUN:   | llvm-readelf -h - \
+# RUN:   | FileCheck -check-prefix=CHECK-LP64S %s
+
+# RUN: llvm-mc -triple loongarch64 -target-abi lp64f -filetype=obj < %s \
+# RUN:   | llvm-readelf -h - \
+# RUN:   | FileCheck -check-prefix=CHECK-LP64F %s
+
+# RUN: llvm-mc -triple loongarch64 -target-abi lp64d -filetype=obj < %s \
+# RUN:   | llvm-readelf -h - \
+# RUN:   | FileCheck -check-prefix=CHECK-LP64D %s
+
+# CHECK-NONE:        Flags:  0x3, LP64D
+
+# CHECK-LP64S:       Flags:  0x1, LP64S
+
+# CHECK-LP64F:       Flags:  0x2, LP64F
+
+# CHECK-LP64D:       Flags:  0x3, LP64D
diff --git a/llvm/test/MC/LoongArch/valid_branch.s b/llvm/test/MC/LoongArch/valid_branch.s
index 0e0df3d94e0b4..21e506a012f40 100644
--- a/llvm/test/MC/LoongArch/valid_branch.s
+++ b/llvm/test/MC/LoongArch/valid_branch.s
@@ -1,4 +1,6 @@
 # RUN: llvm-mc %s -triple=loongarch64-unknown-linux-gnu -show-encoding | FileCheck %s
+
+## random operands
 beqz           $r9, 96                       # CHECK: beqz       $r9, 96                       # encoding: [0x20,0x61,0x00,0x40]
 bnez           $sp, 212                      # CHECK: bnez       $sp, 212                      # encoding: [0x60,0xd4,0x00,0x44]
 bceqz          $fcc6, 12                     # CHECK: bceqz      $fcc6, 12                     # encoding: [0xc0,0x0c,0x00,0x48]
@@ -11,3 +13,32 @@ blt            $r15, $r30, 168               # CHECK: blt        $r15, $r30, 168
 bge            $r12, $r15, 148               # CHECK: bge        $r12, $r15, 148               # encoding: [0x8f,0x95,0x00,0x64]
 bltu           $r17, $r5, 4                  # CHECK: bltu       $r17, $r5, 4                  # encoding: [0x25,0x06,0x00,0x68]
 bgeu           $r6, $r23, 140                # CHECK: bgeu       $r6, $r23, 140                # encoding: [0xd7,0x8c,0x00,0x6c]
+
+## immediate lower/upper boundary
+### simm16 << 2
+beq            $r10, $r7, -0x20000           # CHECK: beq        $r10, $r7, -131072            # encoding: [0x47,0x01,0x00,0x5a]
+beq            $r10, $r7,  0x1FFFC           # CHECK: beq        $r10, $r7,  131068            # encoding: [0x47,0xfd,0xff,0x59]
+bne            $r10, $r7, -0x20000           # CHECK: bne        $r10, $r7, -131072            # encoding: [0x47,0x01,0x00,0x5e]
+bne            $r10, $r7,  0x1FFFC           # CHECK: bne        $r10, $r7,  131068            # encoding: [0x47,0xfd,0xff,0x5d]
+blt            $r10, $r7, -0x20000           # CHECK: blt        $r10, $r7, -131072            # encoding: [0x47,0x01,0x00,0x62]
+blt            $r10, $r7,  0x1FFFC           # CHECK: blt        $r10, $r7,  131068            # encoding: [0x47,0xfd,0xff,0x61]
+bge            $r10, $r7, -0x20000           # CHECK: bge        $r10, $r7, -131072            # encoding: [0x47,0x01,0x00,0x66]
+bge            $r10, $r7,  0x1FFFC           # CHECK: bge        $r10, $r7,  131068            # encoding: [0x47,0xfd,0xff,0x65]
+bltu           $r10, $r7, -0x20000           # CHECK: bltu       $r10, $r7, -131072            # encoding: [0x47,0x01,0x00,0x6a]
+bltu           $r10, $r7,  0x1FFFC           # CHECK: bltu       $r10, $r7,  131068            # encoding: [0x47,0xfd,0xff,0x69]
+bgeu           $r10, $r7, -0x20000           # CHECK: bgeu       $r10, $r7, -131072            # encoding: [0x47,0x01,0x00,0x6e]
+bgeu           $r10, $r7,  0x1FFFC           # CHECK: bgeu       $r10, $r7,  131068            # encoding: [0x47,0xfd,0xff,0x6d]
+### simm21 << 2
+beqz           $r9, -0x400000                # CHECK: beqz       $r9, -4194304                 # encoding: [0x30,0x01,0x00,0x40]
+beqz           $r9,  0x3FFFFC                # CHECK: beqz       $r9,  4194300                 # encoding: [0x2f,0xfd,0xff,0x43]
+bnez           $r9, -0x400000                # CHECK: bnez       $r9, -4194304                 # encoding: [0x30,0x01,0x00,0x44]
+bnez           $r9,  0x3FFFFC                # CHECK: bnez       $r9,  4194300                 # encoding: [0x2f,0xfd,0xff,0x47]
+bceqz          $fcc6, -0x400000              # CHECK: bceqz      $fcc6, -4194304               # encoding: [0xd0,0x00,0x00,0x48]
+bceqz          $fcc6,  0x3FFFFC              # CHECK: bceqz      $fcc6,  4194300               # encoding: [0xcf,0xfc,0xff,0x4b]
+bcnez          $fcc6, -0x400000              # CHECK: bcnez      $fcc6, -4194304               # encoding: [0xd0,0x01,0x00,0x48]
+bcnez          $fcc6,  0x3FFFFC              # CHECK: bcnez      $fcc6,  4194300               # encoding: [0xcf,0xfd,0xff,0x4b]
+### simm26 << 2
+b              -0x8000000                    # CHECK: b          -134217728                    # encoding: [0x00,0x02,0x00,0x50]
+b               0x7FFFFFC                    # CHECK: b           134217724                    # encoding: [0xff,0xfd,0xff,0x53]
+bl             -0x8000000                    # CHECK: bl         -134217728                    # encoding: [0x00,0x02,0x00,0x54]
+bl              0x7FFFFFC                    # CHECK: bl          134217724                    # encoding: [0xff,0xfd,0xff,0x57]
diff --git a/llvm/tools/llvm-readobj/ELFDumper.cpp b/llvm/tools/llvm-readobj/ELFDumper.cpp
index 91f6d85b66e02..0ddef6aa661b8 100644
--- a/llvm/tools/llvm-readobj/ELFDumper.cpp
+++ b/llvm/tools/llvm-readobj/ELFDumper.cpp
@@ -1860,10 +1860,11 @@ static const EnumEntry<unsigned> ElfHeaderRISCVFlags[] = {
 };
 
 static const EnumEntry<unsigned> ElfHeaderLoongArchFlags[] = {
-  ENUM_ENT(EF_LARCH_ABI_LP64D, "LP64D")
-  // FIXME: Change these and add more flags in future when all ABIs definition were finalized.
-  // See current definitions:
-  // https://loongson.github.io/LoongArch-Documentation/LoongArch-ELF-ABI-EN.html#_e_flags_identifies_abi_type_and_version
+    ENUM_ENT(EF_LARCH_BASE_ABI_LP64D, "LP64D"),
+    ENUM_ENT(EF_LARCH_BASE_ABI_LP64S, "LP64S"), ENUM_ENT(EF_LARCH_BASE_ABI_LP64F, "LP64F")
+    // FIXME: Change these and add more flags in future when all ABIs definition
+    // were finalized. See current definitions:
+    // https://loongson.github.io/LoongArch-Documentation/LoongArch-ELF-ABI-EN.html#_e_flags_identifies_abi_type_and_version
 };
 
 static const EnumEntry<unsigned> ElfSymOtherFlags[] = {
@@ -3495,7 +3496,8 @@ template <class ELFT> void GNUStyle<ELFT>::printFileHeaders(const ELFO *Obj) {
   else if (e->e_machine == EM_RISCV)
     ElfFlags = printFlags(e->e_flags, makeArrayRef(ElfHeaderRISCVFlags));
   else if (e->e_machine == EM_LOONGARCH)
-    ElfFlags = printFlags(e->e_flags, makeArrayRef(ElfHeaderLoongArchFlags));
+    ElfFlags = printFlags(e->e_flags, makeArrayRef(ElfHeaderLoongArchFlags),
+                          unsigned(ELF::EF_LARCH_BASE_ABI));
   Str = "0x" + to_hexString(e->e_flags);
   if (!ElfFlags.empty())
     Str = Str + ", " + ElfFlags;
@@ -6148,7 +6150,8 @@ template <class ELFT> void LLVMStyle<ELFT>::printFileHeaders(const ELFO *Obj) {
     else if (E->e_machine == EM_RISCV)
       W.printFlags("Flags", E->e_flags, makeArrayRef(ElfHeaderRISCVFlags));
     else if (E->e_machine == EM_LOONGARCH)
-      W.printFlags("Flags", E->e_flags, makeArrayRef(ElfHeaderLoongArchFlags));
+      W.printFlags("Flags", E->e_flags, makeArrayRef(ElfHeaderLoongArchFlags),
+                   unsigned(ELF::EF_LARCH_BASE_ABI));
     else
       W.printFlags("Flags", E->e_flags);
     W.printNumber("HeaderSize", E->e_ehsize);
